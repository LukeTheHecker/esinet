{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.0s remaining:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = create_forward_model(info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate Source\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ca1aed83f642948567948613e15fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Source Data to mne.SourceEstimate object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7ee938d2814fcc9e1816c21757869c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project sources to EEG...\n",
      "\ttook 0.79 seconds\n",
      "\n",
      "Create EEG trials with noise...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d62539be14fa9a6cb87fa64ea66d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert EEG matrices to a single instance of mne.Epochs...\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "settings = dict(duration_of_trial=1.0)\n",
    "\n",
    "sim_lstm = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap true sources with eloreta predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.util import wrap_mne_inverse\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_lstm, method='eLORETA')\n",
    "\n",
    "# Scale\n",
    "for i, src in enumerate(pred_elor):\n",
    "    pred_elor[i].data = src.data / np.abs(src.data).max(axis=0)\n",
    "\n",
    "sim_lstm.source_data = pred_elor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & train LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FC_0 (TimeDistributed)       (None, None, 400)         24800     \n",
      "_________________________________________________________________\n",
      "Drop_0 (Dropout)             (None, None, 400)         0         \n",
      "_________________________________________________________________\n",
      "FC_1 (TimeDistributed)       (None, None, 400)         160400    \n",
      "_________________________________________________________________\n",
      "Drop_1 (Dropout)             (None, None, 400)         0         \n",
      "_________________________________________________________________\n",
      "FC_Out (TimeDistributed)     (None, None, 1284)        514884    \n",
      "=================================================================\n",
      "Total params: 700,084\n",
      "Trainable params: 700,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "113/113 [==============================] - 5s 42ms/step - loss: 0.2381 - mae: 0.5486 - val_loss: 0.0687 - val_mae: 0.2770\n",
      "Epoch 2/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0936 - mae: 0.3375 - val_loss: 0.0330 - val_mae: 0.1923\n",
      "Epoch 3/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0733 - mae: 0.2988 - val_loss: 0.0243 - val_mae: 0.1635\n",
      "Epoch 4/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0662 - mae: 0.2839 - val_loss: 0.0210 - val_mae: 0.1523\n",
      "Epoch 5/150\n",
      "113/113 [==============================] - 4s 39ms/step - loss: 0.0630 - mae: 0.2770 - val_loss: 0.0195 - val_mae: 0.1468\n",
      "Epoch 6/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0609 - mae: 0.2725 - val_loss: 0.0192 - val_mae: 0.1460\n",
      "Epoch 7/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0596 - mae: 0.2696 - val_loss: 0.0189 - val_mae: 0.1459\n",
      "Epoch 8/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0588 - mae: 0.2678 - val_loss: 0.0165 - val_mae: 0.1353\n",
      "Epoch 9/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0581 - mae: 0.2662 - val_loss: 0.0164 - val_mae: 0.1351\n",
      "Epoch 10/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0576 - mae: 0.2651 - val_loss: 0.0165 - val_mae: 0.1357\n",
      "Epoch 11/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0572 - mae: 0.2642 - val_loss: 0.0166 - val_mae: 0.1366\n",
      "Epoch 12/150\n",
      "113/113 [==============================] - 4s 40ms/step - loss: 0.0566 - mae: 0.2629 - val_loss: 0.0165 - val_mae: 0.1362\n",
      "Epoch 13/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0565 - mae: 0.2626 - val_loss: 0.0167 - val_mae: 0.1372\n",
      "Epoch 14/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0562 - mae: 0.2619 - val_loss: 0.0155 - val_mae: 0.1316\n",
      "Epoch 15/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0559 - mae: 0.2612 - val_loss: 0.0153 - val_mae: 0.1307\n",
      "Epoch 16/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0556 - mae: 0.2604 - val_loss: 0.0153 - val_mae: 0.1312\n",
      "Epoch 17/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0554 - mae: 0.2601 - val_loss: 0.0158 - val_mae: 0.1336\n",
      "Epoch 18/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0553 - mae: 0.2599 - val_loss: 0.0163 - val_mae: 0.1358\n",
      "Epoch 19/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0552 - mae: 0.2596 - val_loss: 0.0153 - val_mae: 0.1316\n",
      "Epoch 20/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0548 - mae: 0.2587 - val_loss: 0.0153 - val_mae: 0.1313\n",
      "Epoch 21/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0547 - mae: 0.2585 - val_loss: 0.0148 - val_mae: 0.1294\n",
      "Epoch 22/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0546 - mae: 0.2582 - val_loss: 0.0147 - val_mae: 0.1284\n",
      "Epoch 23/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0544 - mae: 0.2576 - val_loss: 0.0154 - val_mae: 0.1322\n",
      "Epoch 24/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0544 - mae: 0.2577 - val_loss: 0.0150 - val_mae: 0.1305\n",
      "Epoch 25/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0543 - mae: 0.2576 - val_loss: 0.0152 - val_mae: 0.1314\n",
      "Epoch 26/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0542 - mae: 0.2571 - val_loss: 0.0153 - val_mae: 0.1319\n",
      "Epoch 27/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0540 - mae: 0.2567 - val_loss: 0.0150 - val_mae: 0.1306\n",
      "Epoch 28/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0539 - mae: 0.2566 - val_loss: 0.0145 - val_mae: 0.1284\n",
      "Epoch 29/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0538 - mae: 0.2563 - val_loss: 0.0151 - val_mae: 0.1308\n",
      "Epoch 30/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0537 - mae: 0.2561 - val_loss: 0.0150 - val_mae: 0.1308\n",
      "Epoch 31/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0538 - mae: 0.2562 - val_loss: 0.0156 - val_mae: 0.1336\n",
      "Epoch 32/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0535 - mae: 0.2556 - val_loss: 0.0146 - val_mae: 0.1283\n",
      "Epoch 33/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0534 - mae: 0.2553 - val_loss: 0.0144 - val_mae: 0.1273\n",
      "Epoch 34/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0533 - mae: 0.2552 - val_loss: 0.0155 - val_mae: 0.1328\n",
      "Epoch 35/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0532 - mae: 0.2549 - val_loss: 0.0147 - val_mae: 0.1287\n",
      "Epoch 36/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0531 - mae: 0.2546 - val_loss: 0.0146 - val_mae: 0.1287\n",
      "Epoch 37/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0531 - mae: 0.2546 - val_loss: 0.0144 - val_mae: 0.1280\n",
      "Epoch 38/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0530 - mae: 0.2543 - val_loss: 0.0139 - val_mae: 0.1252\n",
      "Epoch 39/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0530 - mae: 0.2544 - val_loss: 0.0142 - val_mae: 0.1271\n",
      "Epoch 40/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0528 - mae: 0.2540 - val_loss: 0.0147 - val_mae: 0.1296\n",
      "Epoch 41/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0527 - mae: 0.2535 - val_loss: 0.0145 - val_mae: 0.1287\n",
      "Epoch 42/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0526 - mae: 0.2532 - val_loss: 0.0144 - val_mae: 0.1280\n",
      "Epoch 43/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0526 - mae: 0.2533 - val_loss: 0.0148 - val_mae: 0.1301\n",
      "Epoch 44/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0525 - mae: 0.2532 - val_loss: 0.0148 - val_mae: 0.1299\n",
      "Epoch 45/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0525 - mae: 0.2530 - val_loss: 0.0143 - val_mae: 0.1270\n",
      "Epoch 46/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0523 - mae: 0.2527 - val_loss: 0.0138 - val_mae: 0.1252\n",
      "Epoch 47/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0524 - mae: 0.2527 - val_loss: 0.0138 - val_mae: 0.1252\n",
      "Epoch 48/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0522 - mae: 0.2524 - val_loss: 0.0146 - val_mae: 0.1286\n",
      "Epoch 49/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0523 - mae: 0.2524 - val_loss: 0.0145 - val_mae: 0.1289\n",
      "Epoch 50/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0521 - mae: 0.2519 - val_loss: 0.0143 - val_mae: 0.1278\n",
      "Epoch 51/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0520 - mae: 0.2518 - val_loss: 0.0143 - val_mae: 0.1276\n",
      "Epoch 52/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0520 - mae: 0.2516 - val_loss: 0.0142 - val_mae: 0.1275\n",
      "Epoch 53/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0519 - mae: 0.2515 - val_loss: 0.0138 - val_mae: 0.1248\n",
      "Epoch 54/150\n",
      "113/113 [==============================] - 5s 42ms/step - loss: 0.0518 - mae: 0.2513 - val_loss: 0.0139 - val_mae: 0.1254\n",
      "Epoch 55/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0517 - mae: 0.2511 - val_loss: 0.0138 - val_mae: 0.1252\n",
      "Epoch 56/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0517 - mae: 0.2510 - val_loss: 0.0135 - val_mae: 0.1236\n",
      "Epoch 57/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0516 - mae: 0.2506 - val_loss: 0.0138 - val_mae: 0.1249\n",
      "Epoch 58/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0516 - mae: 0.2507 - val_loss: 0.0141 - val_mae: 0.1269\n",
      "Epoch 59/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0516 - mae: 0.2508 - val_loss: 0.0140 - val_mae: 0.1266\n",
      "Epoch 60/150\n",
      "113/113 [==============================] - 5s 40ms/step - loss: 0.0514 - mae: 0.2503 - val_loss: 0.0142 - val_mae: 0.1274\n",
      "Epoch 61/150\n",
      "113/113 [==============================] - 4s 40ms/step - loss: 0.0514 - mae: 0.2503 - val_loss: 0.0137 - val_mae: 0.1249\n",
      "Epoch 62/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0513 - mae: 0.2499 - val_loss: 0.0142 - val_mae: 0.1273\n",
      "Epoch 63/150\n",
      "113/113 [==============================] - 5s 42ms/step - loss: 0.0512 - mae: 0.2497 - val_loss: 0.0144 - val_mae: 0.1284\n",
      "Epoch 64/150\n",
      "113/113 [==============================] - 5s 44ms/step - loss: 0.0512 - mae: 0.2496 - val_loss: 0.0133 - val_mae: 0.1233\n",
      "Epoch 65/150\n",
      "113/113 [==============================] - 5s 41ms/step - loss: 0.0511 - mae: 0.2494 - val_loss: 0.0142 - val_mae: 0.1270\n",
      "Epoch 66/150\n",
      "113/113 [==============================] - 5s 42ms/step - loss: 0.0511 - mae: 0.2494 - val_loss: 0.0137 - val_mae: 0.1249\n",
      "Epoch 67/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0510 - mae: 0.2491 - val_loss: 0.0141 - val_mae: 0.1266\n",
      "Epoch 68/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0509 - mae: 0.2489 - val_loss: 0.0143 - val_mae: 0.1278\n",
      "Epoch 69/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0509 - mae: 0.2490 - val_loss: 0.0142 - val_mae: 0.1270\n",
      "Epoch 70/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0508 - mae: 0.2486 - val_loss: 0.0138 - val_mae: 0.1251\n",
      "Epoch 71/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0508 - mae: 0.2488 - val_loss: 0.0139 - val_mae: 0.1261\n",
      "Epoch 72/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0507 - mae: 0.2485 - val_loss: 0.0135 - val_mae: 0.1239\n",
      "Epoch 73/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0507 - mae: 0.2484 - val_loss: 0.0136 - val_mae: 0.1242\n",
      "Epoch 74/150\n",
      "113/113 [==============================] - 4s 38ms/step - loss: 0.0506 - mae: 0.2483 - val_loss: 0.0136 - val_mae: 0.1239\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00074: early stopping\n",
      "Model: \"LSTM_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RNN_0 (Bidirectional)        (None, None, 400)         419200    \n",
      "_________________________________________________________________\n",
      "FC_Out (TimeDistributed)     (None, None, 1284)        514884    \n",
      "=================================================================\n",
      "Total params: 934,084\n",
      "Trainable params: 934,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "113/113 [==============================] - 16s 104ms/step - loss: 0.3772 - mae: 0.7302 - val_loss: 0.3232 - val_mae: 0.6642\n",
      "Epoch 2/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.2618 - mae: 0.5842 - val_loss: 0.2113 - val_mae: 0.5181\n",
      "Epoch 3/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.1626 - mae: 0.4435 - val_loss: 0.1306 - val_mae: 0.3928\n",
      "Epoch 4/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.1004 - mae: 0.3417 - val_loss: 0.0711 - val_mae: 0.2832\n",
      "Epoch 5/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0680 - mae: 0.2795 - val_loss: 0.0513 - val_mae: 0.2417\n",
      "Epoch 6/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0501 - mae: 0.2401 - val_loss: 0.0415 - val_mae: 0.2158\n",
      "Epoch 7/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0402 - mae: 0.2145 - val_loss: 0.0484 - val_mae: 0.2407\n",
      "Epoch 8/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0343 - mae: 0.1977 - val_loss: 0.0326 - val_mae: 0.1924\n",
      "Epoch 9/150\n",
      "113/113 [==============================] - 11s 100ms/step - loss: 0.0303 - mae: 0.1852 - val_loss: 0.0275 - val_mae: 0.1740\n",
      "Epoch 10/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0277 - mae: 0.1766 - val_loss: 0.0268 - val_mae: 0.1741\n",
      "Epoch 11/150\n",
      "113/113 [==============================] - 12s 102ms/step - loss: 0.0256 - mae: 0.1696 - val_loss: 0.0265 - val_mae: 0.1745\n",
      "Epoch 12/150\n",
      "113/113 [==============================] - 12s 102ms/step - loss: 0.0240 - mae: 0.1637 - val_loss: 0.0231 - val_mae: 0.1615\n",
      "Epoch 13/150\n",
      "113/113 [==============================] - 12s 102ms/step - loss: 0.0228 - mae: 0.1596 - val_loss: 0.0248 - val_mae: 0.1687\n",
      "Epoch 14/150\n",
      "113/113 [==============================] - 12s 102ms/step - loss: 0.0218 - mae: 0.1558 - val_loss: 0.0216 - val_mae: 0.1547\n",
      "Epoch 15/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0210 - mae: 0.1528 - val_loss: 0.0195 - val_mae: 0.1455\n",
      "Epoch 16/150\n",
      "113/113 [==============================] - 12s 102ms/step - loss: 0.0201 - mae: 0.1490 - val_loss: 0.0202 - val_mae: 0.1504\n",
      "Epoch 17/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0196 - mae: 0.1472 - val_loss: 0.0190 - val_mae: 0.1445\n",
      "Epoch 18/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0190 - mae: 0.1447 - val_loss: 0.0172 - val_mae: 0.1360\n",
      "Epoch 19/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0186 - mae: 0.1430 - val_loss: 0.0177 - val_mae: 0.1390\n",
      "Epoch 20/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0180 - mae: 0.1409 - val_loss: 0.0186 - val_mae: 0.1438\n",
      "Epoch 21/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0176 - mae: 0.1393 - val_loss: 0.0190 - val_mae: 0.1459\n",
      "Epoch 22/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0173 - mae: 0.1379 - val_loss: 0.0163 - val_mae: 0.1329\n",
      "Epoch 23/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0167 - mae: 0.1353 - val_loss: 0.0176 - val_mae: 0.1403\n",
      "Epoch 24/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0164 - mae: 0.1342 - val_loss: 0.0159 - val_mae: 0.1313\n",
      "Epoch 25/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0161 - mae: 0.1325 - val_loss: 0.0162 - val_mae: 0.1323\n",
      "Epoch 26/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0159 - mae: 0.1316 - val_loss: 0.0159 - val_mae: 0.1314\n",
      "Epoch 27/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0155 - mae: 0.1301 - val_loss: 0.0145 - val_mae: 0.1244\n",
      "Epoch 28/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0153 - mae: 0.1290 - val_loss: 0.0166 - val_mae: 0.1367\n",
      "Epoch 29/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0151 - mae: 0.1280 - val_loss: 0.0157 - val_mae: 0.1302\n",
      "Epoch 30/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0147 - mae: 0.1264 - val_loss: 0.0145 - val_mae: 0.1252\n",
      "Epoch 31/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0145 - mae: 0.1253 - val_loss: 0.0145 - val_mae: 0.1259\n",
      "Epoch 32/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0143 - mae: 0.1246 - val_loss: 0.0157 - val_mae: 0.1328\n",
      "Epoch 33/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0140 - mae: 0.1233 - val_loss: 0.0141 - val_mae: 0.1240\n",
      "Epoch 34/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0138 - mae: 0.1219 - val_loss: 0.0169 - val_mae: 0.1389\n",
      "Epoch 35/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0136 - mae: 0.1214 - val_loss: 0.0142 - val_mae: 0.1250\n",
      "Epoch 36/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0134 - mae: 0.1204 - val_loss: 0.0131 - val_mae: 0.1188\n",
      "Epoch 37/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0132 - mae: 0.1195 - val_loss: 0.0167 - val_mae: 0.1389\n",
      "Epoch 38/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0130 - mae: 0.1185 - val_loss: 0.0126 - val_mae: 0.1154\n",
      "Epoch 39/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0128 - mae: 0.1175 - val_loss: 0.0126 - val_mae: 0.1165\n",
      "Epoch 40/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0126 - mae: 0.1164 - val_loss: 0.0135 - val_mae: 0.1216\n",
      "Epoch 41/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0124 - mae: 0.1157 - val_loss: 0.0163 - val_mae: 0.1368\n",
      "Epoch 42/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0123 - mae: 0.1150 - val_loss: 0.0122 - val_mae: 0.1129\n",
      "Epoch 43/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0121 - mae: 0.1138 - val_loss: 0.0121 - val_mae: 0.1133\n",
      "Epoch 44/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0120 - mae: 0.1136 - val_loss: 0.0120 - val_mae: 0.1136\n",
      "Epoch 45/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0118 - mae: 0.1125 - val_loss: 0.0117 - val_mae: 0.1114\n",
      "Epoch 46/150\n",
      "113/113 [==============================] - 11s 100ms/step - loss: 0.0117 - mae: 0.1118 - val_loss: 0.0113 - val_mae: 0.1093\n",
      "Epoch 47/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0116 - mae: 0.1113 - val_loss: 0.0123 - val_mae: 0.1156\n",
      "Epoch 48/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0113 - mae: 0.1101 - val_loss: 0.0117 - val_mae: 0.1120\n",
      "Epoch 49/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0113 - mae: 0.1099 - val_loss: 0.0116 - val_mae: 0.1123\n",
      "Epoch 50/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0110 - mae: 0.1084 - val_loss: 0.0127 - val_mae: 0.1187\n",
      "Epoch 51/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0110 - mae: 0.1084 - val_loss: 0.0126 - val_mae: 0.1190\n",
      "Epoch 52/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0109 - mae: 0.1078 - val_loss: 0.0146 - val_mae: 0.1279\n",
      "Epoch 53/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0108 - mae: 0.1070 - val_loss: 0.0107 - val_mae: 0.1058\n",
      "Epoch 54/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0106 - mae: 0.1063 - val_loss: 0.0122 - val_mae: 0.1144\n",
      "Epoch 55/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0106 - mae: 0.1062 - val_loss: 0.0104 - val_mae: 0.1048\n",
      "Epoch 56/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0104 - mae: 0.1048 - val_loss: 0.0113 - val_mae: 0.1101\n",
      "Epoch 57/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0103 - mae: 0.1046 - val_loss: 0.0111 - val_mae: 0.1100\n",
      "Epoch 58/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0102 - mae: 0.1042 - val_loss: 0.0125 - val_mae: 0.1186\n",
      "Epoch 59/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0101 - mae: 0.1033 - val_loss: 0.0100 - val_mae: 0.1020\n",
      "Epoch 60/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0100 - mae: 0.1032 - val_loss: 0.0115 - val_mae: 0.1126\n",
      "Epoch 61/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0099 - mae: 0.1025 - val_loss: 0.0108 - val_mae: 0.1086\n",
      "Epoch 62/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0098 - mae: 0.1018 - val_loss: 0.0102 - val_mae: 0.1036\n",
      "Epoch 63/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0097 - mae: 0.1013 - val_loss: 0.0107 - val_mae: 0.1075\n",
      "Epoch 64/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0096 - mae: 0.1009 - val_loss: 0.0107 - val_mae: 0.1076\n",
      "Epoch 65/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0096 - mae: 0.1005 - val_loss: 0.0098 - val_mae: 0.1023\n",
      "Epoch 66/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0094 - mae: 0.0997 - val_loss: 0.0095 - val_mae: 0.0993\n",
      "Epoch 67/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0094 - mae: 0.0995 - val_loss: 0.0096 - val_mae: 0.1009\n",
      "Epoch 68/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0093 - mae: 0.0991 - val_loss: 0.0102 - val_mae: 0.1049\n",
      "Epoch 69/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0092 - mae: 0.0985 - val_loss: 0.0096 - val_mae: 0.1006\n",
      "Epoch 70/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0092 - mae: 0.0984 - val_loss: 0.0093 - val_mae: 0.0989\n",
      "Epoch 71/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0091 - mae: 0.0976 - val_loss: 0.0091 - val_mae: 0.0976\n",
      "Epoch 72/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0090 - mae: 0.0973 - val_loss: 0.0094 - val_mae: 0.1000\n",
      "Epoch 73/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0090 - mae: 0.0971 - val_loss: 0.0091 - val_mae: 0.0974\n",
      "Epoch 74/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0089 - mae: 0.0966 - val_loss: 0.0093 - val_mae: 0.0989\n",
      "Epoch 75/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0088 - mae: 0.0961 - val_loss: 0.0093 - val_mae: 0.0995\n",
      "Epoch 76/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0087 - mae: 0.0956 - val_loss: 0.0093 - val_mae: 0.0989\n",
      "Epoch 77/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0087 - mae: 0.0954 - val_loss: 0.0092 - val_mae: 0.0986\n",
      "Epoch 78/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0086 - mae: 0.0950 - val_loss: 0.0094 - val_mae: 0.1010\n",
      "Epoch 79/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0085 - mae: 0.0944 - val_loss: 0.0095 - val_mae: 0.1020\n",
      "Epoch 80/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0086 - mae: 0.0947 - val_loss: 0.0091 - val_mae: 0.0976\n",
      "Epoch 81/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0085 - mae: 0.0942 - val_loss: 0.0092 - val_mae: 0.0978\n",
      "Epoch 82/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0084 - mae: 0.0939 - val_loss: 0.0088 - val_mae: 0.0968\n",
      "Epoch 83/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0084 - mae: 0.0935 - val_loss: 0.0092 - val_mae: 0.0995\n",
      "Epoch 84/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0083 - mae: 0.0930 - val_loss: 0.0087 - val_mae: 0.0959\n",
      "Epoch 85/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0082 - mae: 0.0928 - val_loss: 0.0087 - val_mae: 0.0951\n",
      "Epoch 86/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0082 - mae: 0.0924 - val_loss: 0.0086 - val_mae: 0.0953\n",
      "Epoch 87/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0081 - mae: 0.0920 - val_loss: 0.0083 - val_mae: 0.0929\n",
      "Epoch 88/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0081 - mae: 0.0920 - val_loss: 0.0086 - val_mae: 0.0949\n",
      "Epoch 89/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0080 - mae: 0.0913 - val_loss: 0.0083 - val_mae: 0.0934\n",
      "Epoch 90/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0080 - mae: 0.0915 - val_loss: 0.0083 - val_mae: 0.0931\n",
      "Epoch 91/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0079 - mae: 0.0908 - val_loss: 0.0083 - val_mae: 0.0934\n",
      "Epoch 92/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0079 - mae: 0.0909 - val_loss: 0.0084 - val_mae: 0.0944\n",
      "Epoch 93/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0078 - mae: 0.0905 - val_loss: 0.0083 - val_mae: 0.0927\n",
      "Epoch 94/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0078 - mae: 0.0900 - val_loss: 0.0089 - val_mae: 0.0972\n",
      "Epoch 95/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0078 - mae: 0.0899 - val_loss: 0.0083 - val_mae: 0.0938\n",
      "Epoch 96/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0077 - mae: 0.0897 - val_loss: 0.0088 - val_mae: 0.0975\n",
      "Epoch 97/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0077 - mae: 0.0896 - val_loss: 0.0082 - val_mae: 0.0932\n",
      "Epoch 98/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0076 - mae: 0.0888 - val_loss: 0.0081 - val_mae: 0.0921\n",
      "Epoch 99/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0076 - mae: 0.0890 - val_loss: 0.0084 - val_mae: 0.0939\n",
      "Epoch 100/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0076 - mae: 0.0887 - val_loss: 0.0079 - val_mae: 0.0909\n",
      "Epoch 101/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0075 - mae: 0.0883 - val_loss: 0.0078 - val_mae: 0.0897\n",
      "Epoch 102/150\n",
      "113/113 [==============================] - 12s 109ms/step - loss: 0.0075 - mae: 0.0880 - val_loss: 0.0083 - val_mae: 0.0942\n",
      "Epoch 103/150\n",
      "113/113 [==============================] - 12s 108ms/step - loss: 0.0074 - mae: 0.0878 - val_loss: 0.0076 - val_mae: 0.0885\n",
      "Epoch 104/150\n",
      "113/113 [==============================] - 12s 108ms/step - loss: 0.0074 - mae: 0.0878 - val_loss: 0.0078 - val_mae: 0.0900\n",
      "Epoch 105/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0074 - mae: 0.0876 - val_loss: 0.0081 - val_mae: 0.0933\n",
      "Epoch 106/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0073 - mae: 0.0872 - val_loss: 0.0077 - val_mae: 0.0903\n",
      "Epoch 107/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0073 - mae: 0.0869 - val_loss: 0.0095 - val_mae: 0.1033\n",
      "Epoch 108/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0073 - mae: 0.0869 - val_loss: 0.0080 - val_mae: 0.0928\n",
      "Epoch 109/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0072 - mae: 0.0864 - val_loss: 0.0100 - val_mae: 0.1054\n",
      "Epoch 110/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0072 - mae: 0.0865 - val_loss: 0.0076 - val_mae: 0.0888\n",
      "Epoch 111/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0071 - mae: 0.0862 - val_loss: 0.0078 - val_mae: 0.0908\n",
      "Epoch 112/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0071 - mae: 0.0858 - val_loss: 0.0082 - val_mae: 0.0939\n",
      "Epoch 113/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0071 - mae: 0.0859 - val_loss: 0.0077 - val_mae: 0.0908\n",
      "Epoch 114/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0070 - mae: 0.0856 - val_loss: 0.0076 - val_mae: 0.0894\n",
      "Epoch 115/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0070 - mae: 0.0852 - val_loss: 0.0074 - val_mae: 0.0879\n",
      "Epoch 116/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0070 - mae: 0.0852 - val_loss: 0.0076 - val_mae: 0.0892\n",
      "Epoch 117/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0069 - mae: 0.0848 - val_loss: 0.0072 - val_mae: 0.0868\n",
      "Epoch 118/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0069 - mae: 0.0849 - val_loss: 0.0079 - val_mae: 0.0921\n",
      "Epoch 119/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0069 - mae: 0.0846 - val_loss: 0.0072 - val_mae: 0.0865\n",
      "Epoch 120/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0068 - mae: 0.0842 - val_loss: 0.0078 - val_mae: 0.0915\n",
      "Epoch 121/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0068 - mae: 0.0843 - val_loss: 0.0077 - val_mae: 0.0912\n",
      "Epoch 122/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0068 - mae: 0.0839 - val_loss: 0.0072 - val_mae: 0.0869\n",
      "Epoch 123/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0068 - mae: 0.0838 - val_loss: 0.0074 - val_mae: 0.0888\n",
      "Epoch 124/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0067 - mae: 0.0837 - val_loss: 0.0070 - val_mae: 0.0853\n",
      "Epoch 125/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0067 - mae: 0.0836 - val_loss: 0.0071 - val_mae: 0.0858\n",
      "Epoch 126/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0067 - mae: 0.0831 - val_loss: 0.0072 - val_mae: 0.0874\n",
      "Epoch 127/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0067 - mae: 0.0832 - val_loss: 0.0079 - val_mae: 0.0922\n",
      "Epoch 128/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0067 - mae: 0.0834 - val_loss: 0.0073 - val_mae: 0.0882\n",
      "Epoch 129/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0066 - mae: 0.0827 - val_loss: 0.0072 - val_mae: 0.0873\n",
      "Epoch 130/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0066 - mae: 0.0827 - val_loss: 0.0075 - val_mae: 0.0898\n",
      "Epoch 131/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0065 - mae: 0.0824 - val_loss: 0.0070 - val_mae: 0.0855\n",
      "Epoch 132/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0066 - mae: 0.0825 - val_loss: 0.0072 - val_mae: 0.0880\n",
      "Epoch 133/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0065 - mae: 0.0820 - val_loss: 0.0070 - val_mae: 0.0856\n",
      "Epoch 134/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0065 - mae: 0.0820 - val_loss: 0.0071 - val_mae: 0.0867\n",
      "Epoch 135/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0064 - mae: 0.0818 - val_loss: 0.0069 - val_mae: 0.0846\n",
      "Epoch 136/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0064 - mae: 0.0815 - val_loss: 0.0067 - val_mae: 0.0836\n",
      "Epoch 137/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0064 - mae: 0.0816 - val_loss: 0.0068 - val_mae: 0.0843\n",
      "Epoch 138/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0064 - mae: 0.0812 - val_loss: 0.0069 - val_mae: 0.0855\n",
      "Epoch 139/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0063 - mae: 0.0810 - val_loss: 0.0069 - val_mae: 0.0847\n",
      "Epoch 140/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0063 - mae: 0.0810 - val_loss: 0.0067 - val_mae: 0.0836\n",
      "Epoch 141/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0063 - mae: 0.0808 - val_loss: 0.0068 - val_mae: 0.0847\n",
      "Epoch 142/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0063 - mae: 0.0808 - val_loss: 0.0067 - val_mae: 0.0838\n",
      "Epoch 143/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0062 - mae: 0.0802 - val_loss: 0.0066 - val_mae: 0.0827\n",
      "Epoch 144/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0062 - mae: 0.0804 - val_loss: 0.0069 - val_mae: 0.0858\n",
      "Epoch 145/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0062 - mae: 0.0803 - val_loss: 0.0068 - val_mae: 0.0852\n",
      "Epoch 146/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0062 - mae: 0.0803 - val_loss: 0.0068 - val_mae: 0.0856\n",
      "Epoch 147/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0061 - mae: 0.0798 - val_loss: 0.0070 - val_mae: 0.0864\n",
      "Epoch 148/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0061 - mae: 0.0800 - val_loss: 0.0065 - val_mae: 0.0816\n",
      "Epoch 149/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0061 - mae: 0.0798 - val_loss: 0.0065 - val_mae: 0.0827\n",
      "Epoch 150/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0061 - mae: 0.0794 - val_loss: 0.0069 - val_mae: 0.0861\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "patience = 10\n",
    "activation_funcion = 'relu'\n",
    "loss = 'huber'\n",
    "dropout = 0.2\n",
    "\n",
    "# Dense net\n",
    "model_params = dict(activation_function=activation_funcion, n_dense_layers=2, \n",
    "    n_dense_units=400, n_lstm_layers=0, model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=dropout, loss=loss, optimizer='adam', return_history=True,\n",
    "    batch_size=8)\n",
    "net_dense = Net(fwd, **model_params)\n",
    "_, history_dense = net_dense.fit(sim_lstm, **train_params)\n",
    "\n",
    "# LSTM v2\n",
    "model_params = dict(activation_function=activation_funcion, n_lstm_layers=1, \n",
    "    n_lstm_units=200, n_dense_layers=0, \n",
    "    model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=0, loss=loss, return_history=True, learning_rate=0.001,\n",
    "    batch_size=8)\n",
    "\n",
    "net_lstm = Net(fwd, **model_params)\n",
    "_, history_lstm = net_lstm.fit(sim_lstm, **train_params)\n",
    "\n",
    "\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20769521474838257\n"
     ]
    }
   ],
   "source": [
    "epp = mne.EpochsArray(np.random.randn(1,61,100), info, verbose=0)\n",
    "from time import time\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    net_lstm.predict(epp)\n",
    "end = time()\n",
    "print((end-start)/100)\n",
    "\n",
    "# sim_lstm.eeg_data.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830ddb30632e4c119d08d29e77bb38e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bd9be0f98541259d9635d2071abbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttook 0.16 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6e6ecb25244c3997f1ecf8c69b3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered issue in callback (most recent call last):\n",
      "  File \"C:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1348, in _on_button_release\n",
      "    self.picked_renderer = self.plotter.iren.FindPokedRenderer(x, y)\n",
      "AttributeError: 'RenderWindowInteractor' object has no attribute 'FindPokedRenderer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.38828332 0.43662978 0.66681116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered issue in callback (most recent call last):\n",
      "  File \"C:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1348, in _on_button_release\n",
      "    self.picked_renderer = self.plotter.iren.FindPokedRenderer(x, y)\n",
      "AttributeError: 'RenderWindowInteractor' object has no attribute 'FindPokedRenderer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [1.82880090e-11 2.08840266e-11 2.97204482e-11]\n",
      "Using control points [0.38828332 0.43662978 0.66681116]\n",
      "Using control points [1.82880090e-11 2.08840266e-11 2.97204482e-11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered issue in callback (most recent call last):\n",
      "  File \"C:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1348, in _on_button_release\n",
      "    self.picked_renderer = self.plotter.iren.FindPokedRenderer(x, y)\n",
      "AttributeError: 'RenderWindowInteractor' object has no attribute 'FindPokedRenderer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [9.85546530e-12 3.93145593e-11 9.82169077e-10]\n"
     ]
    }
   ],
   "source": [
    "from esinet.util import wrap_mne_inverse\n",
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "# settings_eval = dict(duration_of_trial=1, target_snr=(0.5, 10), extents=(15, 35), number_of_sources=(1, 20))\n",
    "settings_eval = dict(duration_of_trial=1, target_snr=9999, extents=(15, 35), number_of_sources=2)\n",
    "\n",
    "# Simulate new data\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(1)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "# Predict sources with classical methods\n",
    "prediction_elor_data = wrap_mne_inverse(fwd, sim_test)[idx].data.astype(np.float32)\n",
    "prediction_elor = deepcopy(predictions[0])\n",
    "prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "prediction_mne_data = wrap_mne_inverse(fwd, sim_test, method='MNE')[idx].data.astype(np.float32)\n",
    "prediction_mne = deepcopy(predictions[0])\n",
    "prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "# Get predictions and names in order\n",
    "predictions.append(prediction_elor)\n",
    "predictions.append(prediction_mne)\n",
    "model_names.append('eLORETA')\n",
    "model_names.append('MNE')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    brain = prediction.plot(**plot_params)\n",
    "    brain.add_text(0.1, 0.9, model_name, 'title')\n",
    "    # Plot predicted EEG\n",
    "    # evoked_esi = util.get_eeg_from_source(prediction, fwd, info, tmin=0.)\n",
    "    # evoked_esi.plot()\n",
    "    # evoked_esi.plot_topomap(title=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate Source\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11caabde4d848e684950594fdcb078b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Source Data to mne.SourceEstimate object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db04a040b08041929334b7f1d951d993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project sources to EEG...\n",
      "\n",
      "Create EEG trials with noise...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d3a3154fef4245847ec723804ca724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert EEG matrices to a single instance of mne.Epochs...\n",
      "predict esinets...\n",
      "predict elor & MNE\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2a1512a84a406cb394824f664fdd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783834e52e364748a28206198ce53e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4db6d95c604d7aba684e0c9fdaf01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e03880130634fe184fc0d6294af18d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64240e45263c4888af5cb830a9eae43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c407452fa8043e58ed572862045b77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf035a3b5bf4d2a86724c3638e25534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aced10583034be4ba07c0a8fcedfaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0988ad508e8d494dacb0b33d8d00e186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04623a6ab3942a3aaf0d89fe3be8bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cea89c55d547dda96cc501b88374ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e0a3d7231c4cbfb82e4b2887063198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e197814b1c4c5db29ed6f55d19cd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191082afe55c45208bb3cd4e637c32d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9244f65121604577a121f0368bcfdd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bf475b04724c2bb5078f1b75bca754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from esinet.util import wrap_mne_inverse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Simulate\n",
    "settings = dict(duration_of_trial=0.2, target_snr=(0.5, 10), number_of_sources=1)\n",
    "\n",
    "sim_lstm_test = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=5000)\n",
    "\n",
    "# Predict\n",
    "# models = [net_lstm, net_lstm_big]\n",
    "print('predict esinets...')\n",
    "# model_names = [model.model.name for model in models]\n",
    "predictions = [model.predict(sim_lstm_test) for model in models]\n",
    "\n",
    "print('predict elor & MNE')\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_lstm_test, method='eLORETA')\n",
    "# model_names.append('eLORETA')\n",
    "predictions.append(pred_elor)\n",
    "\n",
    "pred_mne = wrap_mne_inverse(fwd, sim_lstm_test, method='MNE')\n",
    "# model_names.append('MNE')\n",
    "predictions.append(pred_mne)\n",
    "\n",
    "size = 500\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "distance_matrix = cdist(pos, pos)\n",
    "\n",
    "mean_localization_errors = []\n",
    "aucs = []\n",
    "nmses = []\n",
    "mses = []\n",
    "true_sources = np.stack([src.data for src in sim_lstm_test.source_data], axis=0)\n",
    "true_sources = util.collapse(true_sources)\n",
    "choice = np.random.choice(np.arange(true_sources.shape[0]), size=size, replace=False)\n",
    "true_sources = true_sources[choice]\n",
    "# true_sources = true_sources[:size]\n",
    "\n",
    "for prediction in predictions:\n",
    "    predicted_sources = util.collapse(np.stack([src.data for src in prediction], axis=0))\n",
    "    \n",
    "    predicted_sources = predicted_sources[choice]\n",
    "    # predicted_sources = predicted_sources[:size]\n",
    "    \n",
    "    print('mle calculation....')\n",
    "    mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, distance_matrix=distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    auc = [eval_auc(true_source, predicted_source, pos, epsilon=0.05,n_redraw=25) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    \n",
    "    mean_localization_errors.append(mean_localization_error)\n",
    "    aucs.append(auc)\n",
    "    nmses.append(nmse)\n",
    "    mses.append(mse)\n",
    "\n",
    "aucs_far = [auc[:, 1] for auc in np.array(aucs)]\n",
    "aucs_close = [auc[:, 0] for auc in np.array(aucs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid')\n",
    "%matplotlib qt\n",
    "xticks = dict(ticks=np.arange(len(model_names)), labels=model_names)\n",
    "plot = sns.boxplot  # violinplot\n",
    "variables = [nmses, mses, aucs_far, aucs_close, mean_localization_errors]\n",
    "names = ['Normalized Mean Squared Errors', 'Mean Squared Errors', 'Far area under the curve', 'Close area under the curve', 'Mean Localization Errors']\n",
    "plt.figure(figsize=(12, 10))\n",
    "    \n",
    "subplot_nums = np.arange(321, 326)\n",
    "for variable, name, num in zip(variables, names, subplot_nums):\n",
    "    plt.subplot(num)\n",
    "    plot(data=variable)\n",
    "    plt.title(name)\n",
    "    plt.xticks(**xticks)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), 20)\n",
    "param = \"target_snr\"  # \"number_of_sources\"\n",
    "names = ['Normalized Mean Squared Errors', 'Mean Squared Errors', 'Far area under the curve', 'Close area under the curve', 'Mean Localization Errors']\n",
    "variables = [nmses, mses, aucs_far, aucs_close, mean_localization_errors]\n",
    "subplot_nums = np.arange(321, 326)\n",
    "model_names = ['Dense', 'LSTM', 'eLORETA', 'MNE']\n",
    "n_sources = sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param].values\n",
    "\n",
    "plt.figure()\n",
    "for variable, name, num in zip(variables, names, subplot_nums):\n",
    "    plt.subplot(num)\n",
    "    for i, model_name in enumerate(model_names[:4]):\n",
    "        try:\n",
    "            plt.scatter(n_sources, np.array(variable)[i, :], label=model_name + f' m={np.array(variable)[i,:].mean():.2f}, r={pearsonr(n_sources, np.array(variable)[i, :])[0]:.2f}')\n",
    "        except:\n",
    "            plt.scatter(n_sources, np.array(variable)[i, :], label=model_name + f' m={np.nanmean(np.array(variable)[i,:]):.2f}')\n",
    "            \n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "model_names[0] = 'Dense'\n",
    "\n",
    "data = {model_name: nmse for model_name, nmse in zip(model_names, nmses)}\n",
    "plt.figure()\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mse for model_name, mse in zip(model_names, mses)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_far)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_close)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mle for model_name, mle in zip(model_names, mean_localization_errors)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "plt.title('Mean Localization Errors')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance per Difficulty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [nmses, mean_localization_errors, aucs_far, aucs_close]\n",
    "metric_names = ['nmses', 'mean_localization_errors', 'aucs_far', 'aucs_close']\n",
    "\n",
    "covariates = ['target_snr', 'number_of_sources']\n",
    "\n",
    "n_samples = int(np.array(nmses).shape[1] / 20)\n",
    "for covariate in covariates:\n",
    "    sim_params = sim_lstm_test.simulation_info[covariate].values[:n_samples]\n",
    "    sim_params = np.repeat(sim_params, 20)\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        plt.figure()\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            plt.scatter(sim_params, metric[i], label=model_name, s=.7)\n",
    "        plt.title(f'{covariate}, {metric_name}')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "var = nmses\n",
    "plt.plot([0,np.nanmax(var)], [0,np.nanmax(var)], 'orange')\n",
    "plt.scatter(np.array(var)[1], np.array(var)[2], s=0.5, color='darkblue')\n",
    "plt.title('nMSE')\n",
    "plt.xlabel('LSTM')\n",
    "plt.ylabel('eLORETA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idc = [2,3]\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.scatter(nmses[idc[0]], nmses[idc[1]], s=0.5)\n",
    "ax.plot([0, 1], [0, 1], linewidth=2, color='black')\n",
    "# ax.set_xlim([-np.percentile(nmses[0], 5), np.percentile(nmses[0], 99)])\n",
    "# ax.set_ylim([-np.percentile(nmses[0], 5), np.percentile(nmses[0], 99)])\n",
    "ax.set_xlabel(model_names[idc[0]])\n",
    "ax.set_ylabel(model_names[idc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "print('AUC_far:', ttest_rel(aucs_far[0], aucs_far[1]))\n",
    "print('AUC_close:', ttest_rel(aucs_close[0], aucs_close[1]))\n",
    "print('MLE:', wilcoxon(mean_localization_errors[0], mean_localization_errors[1]))\n",
    "print('nMSE:', ttest_rel(nmses[0], nmses[1]))\n",
    "print('MSE:', ttest_rel(mses[0], mses[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
