{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = create_forward_model(info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "settings = dict(duration_of_trial=1, target_snr=(0.5, 10))\n",
    "\n",
    "sim_lstm = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=n_samples)\n",
    "sim_dense = util.convert_simulation_temporal_to_single(sim_lstm)\n",
    "\n",
    "sim_lstm_test = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=1000)\n",
    "sim_dense_test = util.convert_simulation_temporal_to_single(sim_lstm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras-Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 31m 16s]\n",
      "val_loss: 0.17756995558738708\n",
      "\n",
      "Best val_loss So Far: 0.17470848560333252\n",
      "Total elapsed time: 02h 33m 00s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "lstm_layers       |3                 |1                 \n",
      "dense_layers      |3                 |3                 \n",
      "activation_out    |sigmoid           |tanh              \n",
      "actvation_all     |sigmoid           |elu               \n",
      "learning_rate     |0.017656          |0.015366          \n",
      "Clip Value        |0.39941           |0.85608           \n",
      "lstm_units_l-0    |167               |101               \n",
      "dropout_lstm_l-0  |0.31976           |0.62957           \n",
      "dense_units_l-0   |129               |143               \n",
      "dropout_dense_l-0 |0.83085           |0.089334          \n",
      "dense_units_l-1   |239               |94                \n",
      "dropout_dense_l-1 |0.40118           |0.015374          \n",
      "lstm_units_l-1    |74                |168               \n",
      "dropout_lstm_l-1  |0.04274           |0.47418           \n",
      "lstm_units_l-2    |180               |99                \n",
      "dropout_lstm_l-2  |0.81685           |0.80903           \n",
      "dense_units_l-2   |289               |25                \n",
      "dropout_dense_l-2 |0.42477           |0                 \n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "625/625 [==============================] - 118s 185ms/step - loss: 0.1790 - mse: 1.0188 - mae: 0.2324 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 2/150\n",
      "625/625 [==============================] - 115s 184ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 3/150\n",
      "625/625 [==============================] - 116s 185ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 4/150\n",
      "625/625 [==============================] - 116s 186ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 5/150\n",
      "625/625 [==============================] - 116s 185ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 6/150\n",
      "625/625 [==============================] - 116s 185ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 7/150\n",
      "625/625 [==============================] - 117s 187ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 8/150\n",
      "625/625 [==============================] - 116s 185ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 9/150\n",
      "625/625 [==============================] - 114s 183ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 10/150\n",
      "625/625 [==============================] - 115s 184ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 11/150\n",
      "625/625 [==============================] - 118s 189ms/step - loss: 0.1787 - mse: 1.0184 - mae: 0.2315 - val_loss: 0.1776 - val_mse: 1.0185 - val_mae: 0.2296\n",
      "Epoch 12/150\n",
      "496/625 [======================>.......] - ETA: 23s - loss: 0.1787 - mse: 1.0184 - mae: 0.2315"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from esinet.net import build_nas_lstm\n",
    "\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_nas_lstm,\n",
    "#     objective='val_loss',\n",
    "#     max_trials=450, \n",
    "#     directory='keras-tuner',\n",
    "#     project_name=f'holiday_run',\n",
    "#     # max_model_size=int(2e6),  # Maximum 700k parameters per model\n",
    "#     beta=2.6*2,  # exploration parameter\n",
    "#     )    \n",
    "tuner = kt.RandomSearch(\n",
    "    build_nas_lstm,\n",
    "    objective='val_loss',\n",
    "    max_trials=450, \n",
    "    directory='keras-tuner',\n",
    "    project_name=f'holiday_run',\n",
    "    )\n",
    "callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=15, min_delta=0.0001)]\n",
    "net = Net(fwd)\n",
    "x_train, y_train = net.prep_data(sim_lstm)\n",
    "x_val, y_val = net.prep_data(sim_lstm_test)\n",
    "tuner.search(x_train, y_train, epochs=150, validation_data=(x_val, y_val), \n",
    "    batch_size=16, callbacks=callbacks, shuffle=True)\n",
    "\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Keras Tuner results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lstm_layers</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>activation_out</th>\n",
       "      <th>actvation_all</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>Clip Value</th>\n",
       "      <th>lstm_units_l-0</th>\n",
       "      <th>dropout_lstm_l-0</th>\n",
       "      <th>dense_units_l-0</th>\n",
       "      <th>dropout_dense_l-0</th>\n",
       "      <th>dense_units_l-1</th>\n",
       "      <th>dropout_dense_l-1</th>\n",
       "      <th>lstm_units_l-1</th>\n",
       "      <th>dropout_lstm_l-1</th>\n",
       "      <th>lstm_units_l-2</th>\n",
       "      <th>dropout_lstm_l-2</th>\n",
       "      <th>dense_units_l-2</th>\n",
       "      <th>dropout_dense_l-2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.015366</td>\n",
       "      <td>0.85608</td>\n",
       "      <td>101</td>\n",
       "      <td>0.629574</td>\n",
       "      <td>143</td>\n",
       "      <td>0.089334</td>\n",
       "      <td>94</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>168</td>\n",
       "      <td>0.474183</td>\n",
       "      <td>99</td>\n",
       "      <td>0.809028</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.924784</td>\n",
       "      <td>159</td>\n",
       "      <td>0.485672</td>\n",
       "      <td>27</td>\n",
       "      <td>0.797441</td>\n",
       "      <td>280</td>\n",
       "      <td>0.613588</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.175540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>linear</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>0.616201</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.175632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.506015</td>\n",
       "      <td>77</td>\n",
       "      <td>0.553023</td>\n",
       "      <td>248</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>153</td>\n",
       "      <td>0.632039</td>\n",
       "      <td>106</td>\n",
       "      <td>0.316873</td>\n",
       "      <td>72</td>\n",
       "      <td>0.599389</td>\n",
       "      <td>107</td>\n",
       "      <td>0.360547</td>\n",
       "      <td>0.177570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.035422</td>\n",
       "      <td>0.930294</td>\n",
       "      <td>85</td>\n",
       "      <td>0.150921</td>\n",
       "      <td>269</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>248</td>\n",
       "      <td>0.099953</td>\n",
       "      <td>91</td>\n",
       "      <td>0.357913</td>\n",
       "      <td>153</td>\n",
       "      <td>0.04135</td>\n",
       "      <td>167</td>\n",
       "      <td>0.419179</td>\n",
       "      <td>0.177570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.307246</td>\n",
       "      <td>172</td>\n",
       "      <td>0.592044</td>\n",
       "      <td>53</td>\n",
       "      <td>0.137369</td>\n",
       "      <td>210</td>\n",
       "      <td>0.089077</td>\n",
       "      <td>182</td>\n",
       "      <td>0.529741</td>\n",
       "      <td>170</td>\n",
       "      <td>0.621117</td>\n",
       "      <td>233</td>\n",
       "      <td>0.520172</td>\n",
       "      <td>0.177599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>56</td>\n",
       "      <td>0.597157</td>\n",
       "      <td>257</td>\n",
       "      <td>0.481553</td>\n",
       "      <td>41</td>\n",
       "      <td>0.076912</td>\n",
       "      <td>35</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>107</td>\n",
       "      <td>0.869561</td>\n",
       "      <td>293</td>\n",
       "      <td>0.673312</td>\n",
       "      <td>0.177622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.025983</td>\n",
       "      <td>0.721979</td>\n",
       "      <td>171</td>\n",
       "      <td>0.193108</td>\n",
       "      <td>50</td>\n",
       "      <td>0.805949</td>\n",
       "      <td>225</td>\n",
       "      <td>0.363898</td>\n",
       "      <td>197</td>\n",
       "      <td>0.081307</td>\n",
       "      <td>10</td>\n",
       "      <td>0.157251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lstm_layers dense_layers activation_out actvation_all learning_rate  \\\n",
       "0           1            3           tanh           elu      0.015366   \n",
       "0           3            1         linear        linear      0.002417   \n",
       "0           1            2         linear        linear      0.023143   \n",
       "0           3            1        sigmoid       sigmoid      0.010709   \n",
       "0           3            1        sigmoid       sigmoid      0.035422   \n",
       "0           2            2           tanh       sigmoid      0.009179   \n",
       "0           1            2           tanh          relu      0.013841   \n",
       "0           2            2        sigmoid        linear      0.025983   \n",
       "\n",
       "  Clip Value lstm_units_l-0 dropout_lstm_l-0 dense_units_l-0  \\\n",
       "0    0.85608            101         0.629574             143   \n",
       "0   0.924784            159         0.485672              27   \n",
       "0   0.616201              2              0.0              25   \n",
       "0   0.506015             77         0.553023             248   \n",
       "0   0.930294             85         0.150921             269   \n",
       "0   0.307246            172         0.592044              53   \n",
       "0   0.375405             56         0.597157             257   \n",
       "0   0.721979            171         0.193108              50   \n",
       "\n",
       "  dropout_dense_l-0 dense_units_l-1 dropout_dense_l-1 lstm_units_l-1  \\\n",
       "0          0.089334              94          0.015374            168   \n",
       "0          0.797441             280          0.613588              2   \n",
       "0               0.0              25               0.0            NaN   \n",
       "0          0.592647             153          0.632039            106   \n",
       "0          0.603896             248          0.099953             91   \n",
       "0          0.137369             210          0.089077            182   \n",
       "0          0.481553              41          0.076912             35   \n",
       "0          0.805949             225          0.363898            197   \n",
       "\n",
       "  dropout_lstm_l-1 lstm_units_l-2 dropout_lstm_l-2 dense_units_l-2  \\\n",
       "0         0.474183             99         0.809028              25   \n",
       "0              0.0              2              0.0             NaN   \n",
       "0              NaN            NaN              NaN             NaN   \n",
       "0         0.316873             72         0.599389             107   \n",
       "0         0.357913            153          0.04135             167   \n",
       "0         0.529741            170         0.621117             233   \n",
       "0         0.058192            107         0.869561             293   \n",
       "0         0.081307             10         0.157251             NaN   \n",
       "\n",
       "  dropout_dense_l-2     score  \n",
       "0               0.0  0.174708  \n",
       "0               NaN  0.175540  \n",
       "0               NaN  0.175632  \n",
       "0          0.360547  0.177570  \n",
       "0          0.419179  0.177570  \n",
       "0          0.520172  0.177599  \n",
       "0          0.673312  0.177622  \n",
       "0               NaN       NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "base_path = 'keras-tuner/holiday_run'\n",
    "folders = os.listdir(base_path)\n",
    "folders = [folder for folder in folders if folder.startswith('trial_')]\n",
    "hyperparams = []\n",
    "scores = []\n",
    "for folder in folders:\n",
    "    # Load trial data\n",
    "    with open(base_path + '/' + folder + '/trial.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    try:\n",
    "        # extract score\n",
    "        scores.append( data['metrics']['metrics']['val_loss']['observations'][0]['value'][0] )\n",
    "        # extract hyperparameters\n",
    "        hyperparams.append( data['hyperparameters']['values'] )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dfs = []\n",
    "for i, (score, hyper) in enumerate(zip(scores, hyperparams)):\n",
    "    df = pd.DataFrame.from_dict(hyper, orient='index').T\n",
    "    df['score'] = score\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs).sort_values('score')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'C:\\Users\\lukas\\Dokumente\\projects\\esinet\\keras-tuner\\holidayrun.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & train LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "patience = 20\n",
    "activation_funcion = 'relu'\n",
    "loss = 'huber'\n",
    "dropout = 0.2\n",
    "# Train\n",
    "# model_params = dict(activation_function=activation_funcion, n_dense_layers=2, \n",
    "#     n_dense_units=200)\n",
    "# train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "#     dropout=dropout, loss=loss, optimizer='adam', return_history=True)\n",
    "# net_dense = Net(fwd, **model_params)\n",
    "# _, history_dense = net_dense.fit(sim_dense, **train_params)\n",
    "\n",
    "# LSTM v2\n",
    "model_params = dict(activation_function=activation_funcion, n_lstm_layers=2, \n",
    "    n_lstm_units=75, model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=dropout, loss=loss, optimizer=None, return_history=True, \n",
    "    batch_size=8)\n",
    "\n",
    "net_lstm_v2 = Net(fwd, **model_params)\n",
    "_, history_lstm = net_lstm_v2.fit(sim_lstm, **train_params)\n",
    "\n",
    "# LSTM v3\n",
    "# model_params = dict(activation_function=activation_funcion, n_lstm_layers=2, \n",
    "#     n_lstm_units=75, model_type='v3')\n",
    "# train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "#     dropout=dropout, loss=loss, optimizer=None, return_history=True, \n",
    "#     batch_size=8)\n",
    "\n",
    "# net_lstm_v3 = Net(fwd, **model_params)\n",
    "# _, history_lstm = net_lstm_v3.fit(sim_lstm, **train_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d3482bf8de4a88ac22eb47ac5b107a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcf7d1d3ce64b28a73ccf0b16ad2f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4cc4d4f72f4345886650f9e5cf0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prediction_dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12716/2758817871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mevoked_esi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_topomap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0merror_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_dense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msim_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0merror_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msim_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_dense' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered issue in callback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1348, in _on_button_release\n",
      "    self.picked_renderer = self.plotter.iren.FindPokedRenderer(x, y)\n",
      "AttributeError: 'RenderWindowInteractor' object has no attribute 'FindPokedRenderer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [3.86058518e-09 3.86058518e-09 5.35169863e-09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered issue in callback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1348, in _on_button_release\n",
      "    self.picked_renderer = self.plotter.iren.FindPokedRenderer(x, y)\n",
      "AttributeError: 'RenderWindowInteractor' object has no attribute 'FindPokedRenderer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [1.14141424e-09 2.45816320e-09 6.30531853e-09]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "settings_eval = dict(duration_of_trial=0.2, target_snr=(0.5, 10))\n",
    "\n",
    "# Simulate new data\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(1)\n",
    "idx = 0\n",
    "# Predict sources\n",
    "# prediction_dense = net_dense.predict(sim_test)\n",
    "prediction_lstm = net_lstm_v2.predict(sim_test)\n",
    "\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth', 'title')\n",
    "\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "\n",
    "# Plot predicted source Dense\n",
    "# brain = prediction_dense.plot(**plot_params)\n",
    "# brain.add_text(0.1, 0.9, 'Dense', 'title')\n",
    "# # Plot predicted EEG\n",
    "# evoked_esi = util.get_eeg_from_source(prediction_dense, fwd, info, tmin=0.)\n",
    "# evoked_esi.plot()\n",
    "# evoked_esi.plot_topomap(title='Dense')\n",
    "\n",
    "# Plot predicted source LSTM\n",
    "brain = prediction_lstm.plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'LSTM', 'title')\n",
    "# Plot predicted EEG\n",
    "evoked_esi = util.get_eeg_from_source(prediction_lstm, fwd, info, tmin=0.)\n",
    "evoked_esi.plot()\n",
    "evoked_esi.plot_topomap(title='LSTM')\n",
    "\n",
    "error_dense = ((prediction_dense.data - sim_test.source_data[idx].data)**2).flatten()\n",
    "error_lstm = ((prediction_lstm.data - sim_test.source_data[idx].data)**2).flatten()\n",
    "\n",
    "diff = error_dense - error_lstm\n",
    "relative_better_predictions = np.sum(diff>0)/ len(diff)\n",
    "title = f'{relative_better_predictions*100:.1f} % of samples were better with lstm'\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = dict(duration_of_trial=0.2, target_snr=(0.5, 10))\n",
    "sim_lstm_test = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=20)\n",
    "sim_dense_test = util.convert_simulation_temporal_to_single(sim_lstm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "models = [net_dense, net_lstm_v2, net_lstm_v3]\n",
    "model_names = [model.model.name for model in models]\n",
    "predictions = [model.predict(sim_lstm_test) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Mean Localization Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12716/2368899421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mchoice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_sources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtrue_sources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_sources\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mpredicted_sources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollapse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from scipy.spatial.distance import cdist\n",
    "size = 30\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "distance_matrix = cdist(pos, pos)\n",
    "\n",
    "mean_localization_errors = []\n",
    "aucs = []\n",
    "nmses = []\n",
    "mses = []\n",
    "true_sources = np.stack([src.data for src in sim_lstm_test.source_data], axis=0)\n",
    "true_sources = util.collapse(true_sources)\n",
    "choice = np.random.choice(np.arange(true_sources.shape[0]), size=size, replace=False)\n",
    "true_sources = true_sources[choice]\n",
    "for prediction in predictions:\n",
    "    predicted_sources = util.collapse(np.stack([src.data for src in prediction], axis=0))\n",
    "    \n",
    "    predicted_sources = predicted_sources[choice]\n",
    "    \n",
    "    print('mle calculation....')\n",
    "    mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, distance_matrix=distance_matrix) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "    auc = [eval_auc(true_source, predicted_source, pos) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "    nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "    mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "    \n",
    "    mean_localization_errors.append(mean_localization_error)\n",
    "    aucs.append(auc)\n",
    "    nmses.append(nmse)\n",
    "    mses.append(mse)\n",
    "\n",
    "aucs_far = [auc[1] for auc in aucs]\n",
    "aucs_close = [auc[0] for auc in aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=nmses)\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=mses)\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=aucs_far)\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=aucs_close)\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=mean_localization_errors)\n",
    "plt.title('Mean Localization Errors')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.scatter(nmses[0], nmses[2], s=0.5)\n",
    "ax.plot([0, 1], [0, 1], linewidth=2, color='black')\n",
    "ax.set_xlim([-np.percentile(nmses[0], 5), np.percentile(nmses[0], 99)])\n",
    "ax.set_ylim([-np.percentile(nmses[0], 5), np.percentile(nmses[0], 99)])\n",
    "ax.set_xlabel('Dense')\n",
    "ax.set_ylabel('LSTM v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "print('AUC_far:', ttest_rel(aucs_far[0], aucs_far[1]))\n",
    "print('AUC_close:', ttest_rel(aucs_close[0], aucs_close[1]))\n",
    "print('MLE:', wilcoxon(mean_localization_errors[0], mean_localization_errors[1]))\n",
    "print('nMSE:', ttest_rel(nmses[0], nmses[1]))\n",
    "print('MSE:', ttest_rel(mses[0], mses[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbb001003ceff4ec0bc12bddb6bde082b9f9e9bd295b9232bbf70f72f8799a35"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
