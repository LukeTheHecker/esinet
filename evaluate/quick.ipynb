{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, r'C:\\Users\\lukas\\Dokumente\\projects\\esinet')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    2.2s remaining:    3.7s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.7s remaining:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = create_forward_model(info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate Source\n",
      "doing the noise-based source simulation thing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fa7833806946b2ad8415dc37ce2da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Source Data to mne.SourceEstimate object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624aedd25d33489fb919a3db3c71f53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project sources to EEG...\n",
      "\n",
      "Create EEG trials with noise...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0f6eafbc3a4b07868e981bb02aac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert EEG matrices to a single instance of mne.Epochs...\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "duration_of_trial = 1.0\n",
    "settings = dict(duration_of_trial=duration_of_trial, method='noise')\n",
    "\n",
    "sim_lstm = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=n_samples)\n",
    "# sim_lstm.save(f'simulations/sim_{n_samples}_{int(duration_of_trial*100)}points.pkl')\n",
    "# sim_lstm_prime = Simulation(fwd, info, verbose=True, settings=settings_prime).simulate(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RNN_0 (Bidirectional)        (None, None, 150)         82200     \n",
      "_________________________________________________________________\n",
      "RNN_1 (Bidirectional)        (None, None, 150)         135600    \n",
      "_________________________________________________________________\n",
      "FC_Out (TimeDistributed)     (None, None, 1284)        193884    \n",
      "=================================================================\n",
      "Total params: 411,684\n",
      "Trainable params: 411,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "handle data\n",
      "preprocess data\n",
      "reshape data\n",
      "fit model\n",
      "Epoch 1/150\n",
      "113/113 [==============================] - 20s 124ms/step - loss: 0.2310 - mae: 0.2659 - val_loss: 0.0922 - val_mae: 0.2434\n",
      "Epoch 2/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0887 - mae: 0.2377 - val_loss: 0.0851 - val_mae: 0.2324\n",
      "Epoch 3/150\n",
      "113/113 [==============================] - 12s 108ms/step - loss: 0.0825 - mae: 0.2286 - val_loss: 0.0794 - val_mae: 0.2239\n",
      "Epoch 4/150\n",
      "113/113 [==============================] - 13s 111ms/step - loss: 0.0779 - mae: 0.2217 - val_loss: 0.0774 - val_mae: 0.2213\n",
      "Epoch 5/150\n",
      "113/113 [==============================] - 12s 105ms/step - loss: 0.0725 - mae: 0.2133 - val_loss: 0.0727 - val_mae: 0.2133\n",
      "Epoch 6/150\n",
      "113/113 [==============================] - 12s 104ms/step - loss: 0.0687 - mae: 0.2074 - val_loss: 0.0701 - val_mae: 0.2091\n",
      "Epoch 7/150\n",
      "113/113 [==============================] - 12s 106ms/step - loss: 0.0669 - mae: 0.2045 - val_loss: 0.0677 - val_mae: 0.2056\n",
      "Epoch 8/150\n",
      "113/113 [==============================] - 12s 104ms/step - loss: 0.0654 - mae: 0.2021 - val_loss: 0.0674 - val_mae: 0.2051\n",
      "Epoch 9/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0638 - mae: 0.1995 - val_loss: 0.0676 - val_mae: 0.2052\n",
      "Epoch 10/150\n",
      "113/113 [==============================] - 11s 102ms/step - loss: 0.0620 - mae: 0.1965 - val_loss: 0.0634 - val_mae: 0.1985\n",
      "Epoch 11/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0619 - mae: 0.1965 - val_loss: 0.0649 - val_mae: 0.2007\n",
      "Epoch 12/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0600 - mae: 0.1932 - val_loss: 0.0627 - val_mae: 0.1972\n",
      "Epoch 13/150\n",
      "113/113 [==============================] - 12s 110ms/step - loss: 0.0599 - mae: 0.1931 - val_loss: 0.0618 - val_mae: 0.1958\n",
      "Epoch 14/150\n",
      "113/113 [==============================] - 12s 105ms/step - loss: 0.0590 - mae: 0.1915 - val_loss: 0.0597 - val_mae: 0.1927\n",
      "Epoch 15/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0579 - mae: 0.1897 - val_loss: 0.0598 - val_mae: 0.1928\n",
      "Epoch 16/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0566 - mae: 0.1874 - val_loss: 0.0608 - val_mae: 0.1939\n",
      "Epoch 17/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0562 - mae: 0.1868 - val_loss: 0.0574 - val_mae: 0.1886\n",
      "Epoch 18/150\n",
      "113/113 [==============================] - 12s 104ms/step - loss: 0.0556 - mae: 0.1858 - val_loss: 0.0581 - val_mae: 0.1897\n",
      "Epoch 19/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0547 - mae: 0.1841 - val_loss: 0.0585 - val_mae: 0.1902\n",
      "Epoch 20/150\n",
      "113/113 [==============================] - 12s 107ms/step - loss: 0.0544 - mae: 0.1837 - val_loss: 0.0577 - val_mae: 0.1888\n",
      "Epoch 21/150\n",
      "113/113 [==============================] - 13s 114ms/step - loss: 0.0546 - mae: 0.1839 - val_loss: 0.0556 - val_mae: 0.1854\n",
      "Epoch 22/150\n",
      "113/113 [==============================] - 12s 110ms/step - loss: 0.0541 - mae: 0.1832 - val_loss: 0.0568 - val_mae: 0.1874\n",
      "Epoch 23/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0528 - mae: 0.1809 - val_loss: 0.0541 - val_mae: 0.1830\n",
      "Epoch 24/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0528 - mae: 0.1810 - val_loss: 0.0551 - val_mae: 0.1847\n",
      "Epoch 25/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0524 - mae: 0.1803 - val_loss: 0.0555 - val_mae: 0.1853\n",
      "Epoch 26/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0523 - mae: 0.1800 - val_loss: 0.0543 - val_mae: 0.1830\n",
      "Epoch 27/150\n",
      "113/113 [==============================] - 11s 101ms/step - loss: 0.0516 - mae: 0.1788 - val_loss: 0.0549 - val_mae: 0.1843\n",
      "Epoch 28/150\n",
      "113/113 [==============================] - 13s 112ms/step - loss: 0.0513 - mae: 0.1781 - val_loss: 0.0569 - val_mae: 0.1873\n",
      "Epoch 29/150\n",
      "113/113 [==============================] - 12s 103ms/step - loss: 0.0511 - mae: 0.1778 - val_loss: 0.0537 - val_mae: 0.1823\n",
      "Epoch 30/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0508 - mae: 0.1774 - val_loss: 0.0538 - val_mae: 0.1822\n",
      "Epoch 31/150\n",
      "113/113 [==============================] - 13s 112ms/step - loss: 0.0511 - mae: 0.1779 - val_loss: 0.0551 - val_mae: 0.1848\n",
      "Epoch 32/150\n",
      "113/113 [==============================] - 11s 93ms/step - loss: 0.0501 - mae: 0.1762 - val_loss: 0.0530 - val_mae: 0.1810\n",
      "Epoch 33/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0505 - mae: 0.1770 - val_loss: 0.0535 - val_mae: 0.1819\n",
      "Epoch 34/150\n",
      "113/113 [==============================] - 10s 93ms/step - loss: 0.0496 - mae: 0.1751 - val_loss: 0.0546 - val_mae: 0.1837\n",
      "Epoch 35/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0497 - mae: 0.1753 - val_loss: 0.0542 - val_mae: 0.1832\n",
      "Epoch 36/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0495 - mae: 0.1751 - val_loss: 0.0546 - val_mae: 0.1834\n",
      "Epoch 37/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0504 - mae: 0.1767 - val_loss: 0.0540 - val_mae: 0.1825\n",
      "Epoch 38/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0494 - mae: 0.1749 - val_loss: 0.0571 - val_mae: 0.1879\n",
      "Epoch 39/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0494 - mae: 0.1749 - val_loss: 0.0525 - val_mae: 0.1799\n",
      "Epoch 40/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0496 - mae: 0.1752 - val_loss: 0.0553 - val_mae: 0.1846\n",
      "Epoch 41/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0483 - mae: 0.1728 - val_loss: 0.0522 - val_mae: 0.1795\n",
      "Epoch 42/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0489 - mae: 0.1739 - val_loss: 0.0535 - val_mae: 0.1819\n",
      "Epoch 43/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0481 - mae: 0.1725 - val_loss: 0.0533 - val_mae: 0.1815\n",
      "Epoch 44/150\n",
      "113/113 [==============================] - 11s 98ms/step - loss: 0.0479 - mae: 0.1721 - val_loss: 0.0535 - val_mae: 0.1815\n",
      "Epoch 45/150\n",
      "113/113 [==============================] - 11s 97ms/step - loss: 0.0476 - mae: 0.1717 - val_loss: 0.0546 - val_mae: 0.1833\n",
      "Epoch 46/150\n",
      "113/113 [==============================] - 11s 94ms/step - loss: 0.0475 - mae: 0.1714 - val_loss: 0.0535 - val_mae: 0.1816\n",
      "Epoch 47/150\n",
      "113/113 [==============================] - 11s 96ms/step - loss: 0.0478 - mae: 0.1719 - val_loss: 0.0541 - val_mae: 0.1827\n",
      "Epoch 48/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0470 - mae: 0.1705 - val_loss: 0.0537 - val_mae: 0.1817\n",
      "Epoch 49/150\n",
      "113/113 [==============================] - 11s 99ms/step - loss: 0.0470 - mae: 0.1704 - val_loss: 0.0539 - val_mae: 0.1822\n",
      "Epoch 50/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0469 - mae: 0.1702 - val_loss: 0.0538 - val_mae: 0.1821\n",
      "Epoch 51/150\n",
      "113/113 [==============================] - 11s 95ms/step - loss: 0.0465 - mae: 0.1695 - val_loss: 0.0539 - val_mae: 0.1822\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "epochs = 150\n",
    "patience = 10\n",
    "activation_function = 'relu'\n",
    "loss = 'mean_squared_error'\n",
    "dropout = 0.2\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=0.5) #  'adam'  # tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# Dense net\n",
    "model_params = dict(activation_function=activation_function, n_dense_layers=2, \n",
    "    n_dense_units=400, n_lstm_layers=0, model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=dropout, loss=loss, optimizer=optimizer, return_history=True,\n",
    "    batch_size=8)\n",
    "# Train\n",
    "# net_dense = Net(fwd, **model_params)\n",
    "# _, history_dense = net_dense.fit(sim_lstm, **train_params)\n",
    "\n",
    "# LSTM v2\n",
    "model_params = dict(activation_function=activation_function, n_lstm_layers=2, \n",
    "    n_lstm_units=75, n_dense_layers=0, \n",
    "    model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=0.1, loss=loss, optimizer=optimizer, return_history=True,\n",
    "    batch_size=8)\n",
    "\n",
    "# Train\n",
    "net_lstm = Net(fwd, **model_params)\n",
    "_, history_lstm = net_lstm.fit(sim_lstm, **train_params)\n",
    "\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']\n",
    "# net_dense.save(r'models', name='dense-net-100points-noise')\n",
    "# net_lstm.save(r'models', name='lstm-net-100points-noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
