{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet import forward\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = forward.get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = forward.create_forward_model(info=info)\n",
    "fwd_free = forward.create_forward_model(info=info, fixed_ori=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_standard = util.load_net('models/LSTM Medium_1-1000points_standard-cosine-mse_0')\n",
    "lstm_noise = util.load_net('models/LSTM Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "dense_standard = util.load_net('models/Dense Medium_1-1000points_standard-cosine-mse_0')\n",
    "dense_noise = util.load_net('models/Dense Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "convdip_standard = util.load_net('models/ConvDip Medium_1-1000points_standard-cosine-mse_0')\n",
    "convdip_noise = util.load_net('models/ConvDip Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "models = [lstm_standard, lstm_noise, dense_standard, dense_noise, convdip_standard, convdip_noise]\n",
    "model_names = ['LSTM Standard', 'LSTM Noise', 'Dense Standard', 'Dense Noise', 'ConvDip Standard', 'ConvDip Noise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ability to process long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "settings_eval = dict(duration_of_trial=100, method='standard', number_of_sources=10)\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(2)\n",
    "idx = 0\n",
    "durs = [100, 50, 10, 1, 0.1, 0.02]\n",
    "# durs = [np.random.uniform(0.01, 100) for _ in range(5)]\n",
    "for net, name in zip(models, model_names):\n",
    "    for dur in durs:\n",
    "        sim_test.crop(tmax=dur)\n",
    "        prediction = net.predict(sim_test)\n",
    "        error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "        r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "        title = f'{dur}: error: {error:.4}, r: {r:.2f}\\n'\n",
    "        print(f\"{name}:\")\n",
    "        print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(duration_of_trial=1, method='standard')\n",
    "# settings_eval = dict(duration_of_trial=1, method='noise', exponent=3)\n",
    "\n",
    "# Simulate new data\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(2)\n",
    "snr = sim_test.simulation_info['target_snr'].values[0]\n",
    "# print(sim_test.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "\n",
    "# # Predict sources with classical methods\n",
    "# # eLORETA\n",
    "# prediction_elor_data = util.wrap_mne_inverse(fwd, sim_test, snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_elor = deepcopy(predictions[0][0])\n",
    "# prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "# # MNE\n",
    "# prediction_mne_data = util.wrap_mne_inverse(fwd, sim_test, method='MNE', snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_mne = deepcopy(predictions[0][0])\n",
    "# prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "# # Beamformer\n",
    "# prediction_lcmv_data = util.wrap_mne_inverse(fwd_free, sim_test, method='beamformer', snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_lcmv = deepcopy(predictions[0][0])\n",
    "# prediction_lcmv.data = prediction_lcmv_data / np.abs(np.max(prediction_lcmv_data))\n",
    "\n",
    "# Get predictions and names in order\n",
    "# predictions.append([prediction_elor])\n",
    "# predictions.append([prediction_mne])\n",
    "# predictions.append([prediction_lcmv])\n",
    "\n",
    "# model_names.append('eLORETA')\n",
    "# model_names.append('MNE')\n",
    "# model_names.append('Beamformer')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim_test.simulation_info.number_of_sources.values[0]} sources, snr={snr:.1f}', 'title')\n",
    "# Plot True EEG\n",
    "# evoked = sim_test.eeg_data[idx].average()\n",
    "# evoked.plot()\n",
    "# evoked.plot_topomap(title='Ground Truth')\n",
    "# evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "# evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "model_selection = model_names#['net_lstm_standard', 'net_lstm_noise', 'eLORETA']\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    \n",
    "    if not any([model_name.lower() in model_select.lower() for model_select in model_selection]):\n",
    "        continue\n",
    "    error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    \n",
    "    brain = prediction[idx].plot(**plot_params)\n",
    "\n",
    "    title = f'{model_name}, error: {error:.4}, r: {r}'\n",
    "    print(title)\n",
    "    brain.add_text(0.1, 0.9, title, 'title')\n",
    "    # Plot predicted EEG\n",
    "    # evoked_esi = util.get_eeg_from_source(prediction[idx], fwd, info, tmin=0.)\n",
    "    # evoked_esi.plot_topomap(title=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 90.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 444.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 714) (1284, 160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.16it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "duration_of_trial = (0.01, 10)\n",
    "method = 'standard'\n",
    "settings = dict(duration_of_trial=duration_of_trial, method=method)\n",
    "sim_test = Simulation(fwd, info, verbose=False, settings=settings).simulate(n_samples=n_samples)\n",
    "# if type(duration_of_trial) == tuple:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial[0]*100)}-{int(duration_of_trial[1]*100)}points_{method}.pkl')\n",
    "# else:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_{method}.pkl')\n",
    "\n",
    "# or Load\n",
    "# with open(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_{method}.pkl', 'rb') as f:\n",
    "#     sim_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "predict esinets...\n",
      "interpolating for convdip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:37,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolating for convdip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:37,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict elor\n",
      "predict MNE\n",
      "predict LCMV\n",
      "(50807, 1284)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12690b32534dadbb622e6b4b60a1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LSTM Standard :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6780da9116014c22a43f65944cd8db16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8130a95eba67448b8cab06c326084115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923dd04b8b34a48abb2c573e6f6347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LSTM Noise :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a8000abace492fb122e2a6136137e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a71d837f664ebd9bf14ee7c7d9563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed637e98e204450f9043463a9c49cb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dense Standard :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec843a4b9d3b428bb8b8fdbe867624d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60041fc139154811a33ed0e524f167db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958fd5a69e4c49c29574e5975a03bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dense Noise :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4b44ce371b45c8b45d4adf08377b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f290490ffc40e4974a7190cf27d778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac71782470b14ccdab529172686857f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ConvDip Standard :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cc456a769f4f7a81ee5d74d37b7141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49077d3fa874451833341c1974e9c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c472ca38cb22499caf9dbff1fcb5e805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ConvDip Noise :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a4db6d1e14656807d25da6f79753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e606af98f2c247c08b9e5da487a48ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b66428586d4df5a2d998558caf6cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " eLORETA :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927e0fc5289d472a99148d5c296d14ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5118f195db8c430a9e7b3ff78d73ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd11f7097a54880a7083f1063fcaf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MNE :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2d3303bfe54accbc0b408d7c3229da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81ab200e45844289c262863d3dba013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d1616060f940c2918709c12adf1f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Beamformer :\n",
      "\n",
      "(50807, 1284)\n",
      "mle calculation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1aafe1c59c411c87140479e4a6855f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee7a94d9d145b8b24d4df2b2a03337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af99291cbd774bd1b6ff133a4c5bbd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from esinet.util import wrap_mne_inverse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "model_names_tmp = deepcopy(model_names)\n",
    "# Predict\n",
    "print('predict esinets...')\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "\n",
    "print('predict elor')\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_test, method='eLORETA')\n",
    "model_names_tmp.append('eLORETA')\n",
    "predictions.append(pred_elor)\n",
    "\n",
    "print('predict MNE')\n",
    "pred_mne = wrap_mne_inverse(fwd, sim_test, method='MNE')\n",
    "model_names_tmp.append('MNE')\n",
    "predictions.append(pred_mne)\n",
    "\n",
    "print('predict LCMV')\n",
    "pred_lcmv = wrap_mne_inverse(fwd_free, sim_test, method='beamformer')\n",
    "model_names_tmp.append('LCMV')\n",
    "predictions.append(pred_lcmv)\n",
    "\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "mean_localization_errors = []\n",
    "aucs = []\n",
    "nmses = []\n",
    "mses = []\n",
    "true_sources = np.concatenate([src.data for src in sim_test.source_data], axis=1).T\n",
    "print(true_sources.shape)\n",
    "# size = 500\n",
    "# n_total = true_sources.shape[0]\n",
    "# choice = np.random.choice(np.arange(n_total), size=size, replace=False)\n",
    "# true_sources = true_sources[choice]\n",
    "for prediction, model_name in tqdm(zip(predictions, model_names_tmp)):\n",
    "    print('\\n', model_name, ':\\n')\n",
    "    predicted_sources = np.concatenate([src.data for src in prediction], axis=1).T\n",
    "\n",
    "    # predicted_sources = predicted_sources[choice]\n",
    "    print(predicted_sources.shape)\n",
    "    print('mle calculation....')\n",
    "    mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    # auc = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=25) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    auc = Parallel(n_jobs=-1, backend='loky') \\\n",
    "        (delayed(eval_auc)(true_source, predicted_source, pos, epsilon=0.25, n_redraw=25)\n",
    "        for true_source, predicted_source in zip(true_sources, predicted_sources))\n",
    "    nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    \n",
    "    mean_localization_errors.append(mean_localization_error)\n",
    "    aucs.append(auc)\n",
    "    nmses.append(nmse)\n",
    "    mses.append(mse)\n",
    "\n",
    "aucs_far = [auc[:, 1] for auc in np.array(aucs)]\n",
    "aucs_close = [auc[:, 0] for auc in np.array(aucs)]\n",
    "aucs_combined = [(auc[:, 0]+auc[:, 1])/2 for auc in np.array(aucs)]\n",
    "metrics = dict(\n",
    "    mean_localization_errors=mean_localization_errors,\n",
    "    aucs_far=aucs_far,\n",
    "    aucs_close=aucs_close,\n",
    "    aucs_combined=aucs_combined,\n",
    "    nmses=nmses,\n",
    "    mses=mses\n",
    ")\n",
    "\n",
    "with open(f'results\\\\metrics_{len(true_sources)}_{sim_test.source_data[0].data.shape[1]}points_{method}.pkl', 'wb') as f:\n",
    "    pkl.dump([metrics, None], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tmax = sim_test.source_data[0].times.max()\n",
    "tmin = sim_test.source_data[0].times.min()\n",
    "eval_tmin = tmax - 1\n",
    "new_tmins = [0.0, 4, 16, 18, 18.25, 18.5, 18.75, eval_tmin]\n",
    "sim_cropped = deepcopy(sim_test)\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "mean_localization_errors = {model_name: [] for model_name in model_names}\n",
    "aucs = {model_name: [] for model_name in model_names}\n",
    "nmses = {model_name: [] for model_name in model_names}\n",
    "mses = {model_name: [] for model_name in model_names}\n",
    "true_sources = np.stack([src.crop(tmin=eval_tmin, tmax=tmax, include_tmax=True).data for src in deepcopy(sim_cropped.source_data)], axis=0)\n",
    "true_sources = util.collapse(true_sources)\n",
    "for i, new_tmin in enumerate(new_tmins):\n",
    "    sim_cropped = sim_cropped.crop(tmin=new_tmin, tmax=tmax, include_tmax=True)\n",
    "    print(f'new tmin: {sim_cropped.source_data[0].times.min()}, tmax: {sim_cropped.source_data[0].times.max()}')\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        print('\\t', model_name)\n",
    "        print(f'\\tpredicting {len(sim_cropped.source_data[0].times)} time points')\n",
    "        prediction = model.predict(sim_cropped)\n",
    "        # eval_tmin = tmax-1\n",
    "        predicted_sources = [pred.crop(tmin=eval_tmin, tmax=tmax, include_tmax=True).data for pred in prediction]\n",
    "        print(f'\\ttesting {predicted_sources[0].shape[1]} time points')\n",
    "        predicted_sources = util.collapse(np.stack(predicted_sources, axis=0))\n",
    "        \n",
    "        mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "        # auc = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=25) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "        nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "        mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in zip(true_sources, predicted_sources)]\n",
    "        \n",
    "        mean_localization_errors[model_name].append(np.nanmedian(mean_localization_error))\n",
    "        # aucs[model_name].append(np.nanmedian(auc))\n",
    "        nmses[model_name].append(np.nanmedian(nmse))\n",
    "        mses[model_name].append(np.nanmedian(mse))\n",
    "    \n",
    "%matplotlib qt\n",
    "metrics = [mean_localization_errors, nmses, mses]\n",
    "metric_names = ['mean_localization_error', 'nmses', 'mses']\n",
    "x = (tmax-new_tmins)\n",
    "for metric, metric_name in zip(metrics, metric_names):\n",
    "    plt.figure()\n",
    "    for key, val in metric.items():\n",
    "        plt.plot(x, val, label=key)\n",
    "    plt.legend()\n",
    "    plt.title(metric_name)\n",
    "    plt.xlabel(\"Duration of Trial for prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_{len(true_sources)}_{sim_test.source_data[0].data.shape[1]}points_standard.pkl', 'rb') as f:\n",
    "    [metrics, choice] = pkl.load(f)\n",
    "\n",
    "# with open(f'simulations\\\\sim_test_5000_10points.pkl', 'rb') as f:\n",
    "#     sim_lstm_test = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_localization_errors Mean Localization Errors 331\n",
      "aucs_far Far area under the curve 332\n",
      "aucs_close Close area under the curve 333\n",
      "aucs_combined Combined Area under the curve 334\n",
      "nmses Normalized Mean Squared Errors 335\n",
      "mses Mean Squared Errors 336\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "classic_names = ['eLORETA', 'MNE', 'Beamformer']\n",
    "for n in classic_names:\n",
    "    if not n in model_names:\n",
    "        model_names.append(n)\n",
    "model_names\n",
    "xticks = dict(ticks=np.arange(len(model_names)), labels=model_names)\n",
    "plot = sns.boxplot  # violinplot\n",
    "variable_keys = metrics.keys()\n",
    "names = [ 'Mean Localization Errors',  'Far area under the curve', 'Close area under the curve', 'Combined Area under the curve', 'Normalized Mean Squared Errors', 'Mean Squared Errors']\n",
    "plt.figure(figsize=(14, 8))\n",
    "start = int((np.ceil(np.sqrt(len(names)))*100) + (np.ceil(np.sqrt(5))*10) + 1)\n",
    "end = int(start + len(names))\n",
    "subplot_nums = np.arange(start, end+1)\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    print(variable_key, name, num)\n",
    "    plt.subplot(num)\n",
    "    plot(data=metrics[variable_key])\n",
    "    plt.title(name)\n",
    "    plt.xticks(**xticks)\n",
    "plt.tight_layout(pad=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "start = int((np.ceil(np.sqrt(len(names)))*100) + (np.ceil(np.sqrt(5))*10) + 1)\n",
    "end = int(start + len(names))\n",
    "subplot_nums = np.arange(start, end+1)\n",
    "inverse_idc = [0,1]\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    var = np.array(metrics[variable_key])[inverse_idc]\n",
    "    var = var[:, ~np.isnan(var).any(axis=0)]\n",
    "    if np.any(np.isnan(var)):\n",
    "        print(\"still nans\")\n",
    "\n",
    "    plt.subplot(num)\n",
    "    plt.scatter(var[0], var[1], edgecolors='k', s=2)\n",
    "    # origin\n",
    "    lo_lim = np.nanmin(var)\n",
    "    hi_lim = np.nanmax(var)\n",
    "    plt.plot([lo_lim, hi_lim], [lo_lim, hi_lim], '--r')\n",
    "\n",
    "    prop_higher = np.sum(var[1] > var[0]) / len(var[0])\n",
    "    cohens_d = (np.nanmean(var[0]) - np.nanmean(var[1])) / np.mean([np.nanstd(var[0]), np.nanstd(var[1])])\n",
    "    median_diff = np.abs(np.nanmedian(var[0]-var[1]))\n",
    "    title = f'{name} (higher in {100*prop_higher:.1f} %)\\nmedian_difference: {median_diff}\\ncohens d: {abs(cohens_d):.2f}'\n",
    "    plt.title(title)\n",
    "    plt.xlabel(model_names[inverse_idc[0]])\n",
    "    plt.ylabel(model_names[inverse_idc[1]])\n",
    "    \n",
    "    # pad\n",
    "    lo_lim -= abs(hi_lim-lo_lim)*0.05\n",
    "    hi_lim += abs(hi_lim-lo_lim)*0.05\n",
    "    \n",
    "    # plot properties\n",
    "    plt.xlim(lo_lim, hi_lim)\n",
    "    plt.ylim(lo_lim, hi_lim)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes',\n",
       "       'target_snr', 'betas', 'duration_of_trials'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_test.simulation_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes',\n",
      "       'target_snr', 'betas', 'duration_of_trials', 'duration_bins',\n",
      "       'LSTM_Standard_mean_localization_errors', 'LSTM_Standard_aucs_far',\n",
      "       'LSTM_Standard_aucs_close', 'LSTM_Standard_aucs_combined',\n",
      "       'LSTM_Standard_nmses', 'LSTM_Standard_mses',\n",
      "       'LSTM_Noise_mean_localization_errors', 'LSTM_Noise_aucs_far',\n",
      "       'LSTM_Noise_aucs_close', 'LSTM_Noise_aucs_combined', 'LSTM_Noise_nmses',\n",
      "       'LSTM_Noise_mses', 'Dense_Standard_mean_localization_errors',\n",
      "       'Dense_Standard_aucs_far', 'Dense_Standard_aucs_close',\n",
      "       'Dense_Standard_aucs_combined', 'Dense_Standard_nmses',\n",
      "       'Dense_Standard_mses', 'Dense_Noise_mean_localization_errors',\n",
      "       'Dense_Noise_aucs_far', 'Dense_Noise_aucs_close',\n",
      "       'Dense_Noise_aucs_combined', 'Dense_Noise_nmses', 'Dense_Noise_mses',\n",
      "       'ConvDip_Standard_mean_localization_errors',\n",
      "       'ConvDip_Standard_aucs_far', 'ConvDip_Standard_aucs_close',\n",
      "       'ConvDip_Standard_aucs_combined', 'ConvDip_Standard_nmses',\n",
      "       'ConvDip_Standard_mses', 'ConvDip_Noise_mean_localization_errors',\n",
      "       'ConvDip_Noise_aucs_far', 'ConvDip_Noise_aucs_close',\n",
      "       'ConvDip_Noise_aucs_combined', 'ConvDip_Noise_nmses',\n",
      "       'ConvDip_Noise_mses', 'eLORETA_mean_localization_errors',\n",
      "       'eLORETA_aucs_far', 'eLORETA_aucs_close', 'eLORETA_aucs_combined',\n",
      "       'eLORETA_nmses', 'eLORETA_mses', 'MNE_mean_localization_errors',\n",
      "       'MNE_aucs_far', 'MNE_aucs_close', 'MNE_aucs_combined', 'MNE_nmses',\n",
      "       'MNE_mses', 'Beamformer_mean_localization_errors',\n",
      "       'Beamformer_aucs_far', 'Beamformer_aucs_close',\n",
      "       'Beamformer_aucs_combined', 'Beamformer_nmses', 'Beamformer_mses'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = list(np.concatenate([[i]*src.data.shape[1] for i, src in enumerate(sim_test.source_data)]))\n",
    "target_column = 'target_snr'\n",
    "binning = True\n",
    "n_bins = 4\n",
    "\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "if binning:\n",
    "    bins = np.linspace(df[target_column].min(), df[target_column].max()*1.01, num=n_bins)\n",
    "    bin_labels = [str(int(round(bins[i]))) + ' - ' + str(int(round(bins[i+1]))) for i in range(len(bins)-1)]\n",
    "    df['duration_bins'] = np.digitize(df[target_column].values, bins=bins)\n",
    "    target_column = 'duration_bins'\n",
    "else:\n",
    "    bins = list(set(df[target_column].values))\n",
    "    bins[-1] *= 1.01\n",
    "    bin_labels = [str(bins[i]) for i in range(len(bins))]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name.replace(' ', '_') + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = target_column\n",
    "dep_var_label = target_column.replace('_', ' ').title()\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]',  'AUC Combined [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes',\n",
      "       'target_snr', 'betas', 'target_snr_bins',\n",
      "       'LSTM Standard_mean_localization_errors', 'LSTM Standard_aucs_far',\n",
      "       'LSTM Standard_aucs_close', 'LSTM Standard_aucs_combined',\n",
      "       'LSTM Standard_nmses', 'LSTM Standard_mses',\n",
      "       'LSTM Noise_mean_localization_errors', 'LSTM Noise_aucs_far',\n",
      "       'LSTM Noise_aucs_close', 'LSTM Noise_aucs_combined', 'LSTM Noise_nmses',\n",
      "       'LSTM Noise_mses', 'Dense Standard_mean_localization_errors',\n",
      "       'Dense Standard_aucs_far', 'Dense Standard_aucs_close',\n",
      "       'Dense Standard_aucs_combined', 'Dense Standard_nmses',\n",
      "       'Dense Standard_mses', 'Dense Noise_mean_localization_errors',\n",
      "       'Dense Noise_aucs_far', 'Dense Noise_aucs_close',\n",
      "       'Dense Noise_aucs_combined', 'Dense Noise_nmses', 'Dense Noise_mses',\n",
      "       'ConvDip Standard_mean_localization_errors',\n",
      "       'ConvDip Standard_aucs_far', 'ConvDip Standard_aucs_close',\n",
      "       'ConvDip Standard_aucs_combined', 'ConvDip Standard_nmses',\n",
      "       'ConvDip Standard_mses', 'ConvDip Noise_mean_localization_errors',\n",
      "       'ConvDip Noise_aucs_far', 'ConvDip Noise_aucs_close',\n",
      "       'ConvDip Noise_aucs_combined', 'ConvDip Noise_nmses',\n",
      "       'ConvDip Noise_mses', 'eLORETA_mean_localization_errors',\n",
      "       'eLORETA_aucs_far', 'eLORETA_aucs_close', 'eLORETA_aucs_combined',\n",
      "       'eLORETA_nmses', 'eLORETA_mses', 'MNE_mean_localization_errors',\n",
      "       'MNE_aucs_far', 'MNE_aucs_close', 'MNE_aucs_combined', 'MNE_nmses',\n",
      "       'MNE_mses', 'Beamformer_mean_localization_errors',\n",
      "       'Beamformer_aucs_far', 'Beamformer_aucs_close',\n",
      "       'Beamformer_aucs_combined', 'Beamformer_nmses', 'Beamformer_mses'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_test.n_samples), np.mean([src.data.shape[1] for src in sim_test.source_data]))\n",
    "snr_bins = (2, 4, 6, 8, 10, 20)\n",
    "# snr_bins = (0., 10, 20)\n",
    "\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "# df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "# df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "# df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex='snr_'), df.filter(regex=metric_name)), axis=1).melt('target_snr_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='target_snr_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=snr_bin_labels, ylabel=metric_name_nice, xlabel='Signal to Noise Ratio (SNR)')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_test.n_samples), np.mean([src.data.shape[1] for src in sim_test.source_data]))\n",
    "\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "ecc_bins = np.linspace(int(df['avg_eccentricity'].values.min()), np.ceil(df['avg_eccentricity'].values.max()).astype(int), num=5)\n",
    "ecc_bin_labels = [str(ecc_bins[i]) + ' - ' + str(ecc_bins[i+1]) for i in range(len(ecc_bins)-1)]\n",
    "df['avg_eccentricity_bins'] = np.digitize(df['avg_eccentricity'].values, bins=ecc_bins)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'avg_eccentricity_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Average eccentricity'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=ecc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on No. of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes',\n",
      "       'target_snr', 'betas', 'duration_of_trials', 'number_of_sources_bins',\n",
      "       'LSTM Standard_mean_localization_errors', 'LSTM Standard_aucs_far',\n",
      "       'LSTM Standard_aucs_close', 'LSTM Standard_aucs_combined',\n",
      "       'LSTM Standard_nmses', 'LSTM Standard_mses',\n",
      "       'LSTM Noise_mean_localization_errors', 'LSTM Noise_aucs_far',\n",
      "       'LSTM Noise_aucs_close', 'LSTM Noise_aucs_combined', 'LSTM Noise_nmses',\n",
      "       'LSTM Noise_mses', 'Dense Standard_mean_localization_errors',\n",
      "       'Dense Standard_aucs_far', 'Dense Standard_aucs_close',\n",
      "       'Dense Standard_aucs_combined', 'Dense Standard_nmses',\n",
      "       'Dense Standard_mses', 'Dense Noise_mean_localization_errors',\n",
      "       'Dense Noise_aucs_far', 'Dense Noise_aucs_close',\n",
      "       'Dense Noise_aucs_combined', 'Dense Noise_nmses', 'Dense Noise_mses',\n",
      "       'ConvDip Standard_mean_localization_errors',\n",
      "       'ConvDip Standard_aucs_far', 'ConvDip Standard_aucs_close',\n",
      "       'ConvDip Standard_aucs_combined', 'ConvDip Standard_nmses',\n",
      "       'ConvDip Standard_mses', 'ConvDip Noise_mean_localization_errors',\n",
      "       'ConvDip Noise_aucs_far', 'ConvDip Noise_aucs_close',\n",
      "       'ConvDip Noise_aucs_combined', 'ConvDip Noise_nmses',\n",
      "       'ConvDip Noise_mses', 'eLORETA_mean_localization_errors',\n",
      "       'eLORETA_aucs_far', 'eLORETA_aucs_close', 'eLORETA_aucs_combined',\n",
      "       'eLORETA_nmses', 'eLORETA_mses', 'MNE_mean_localization_errors',\n",
      "       'MNE_aucs_far', 'MNE_aucs_close', 'MNE_aucs_combined', 'MNE_nmses',\n",
      "       'MNE_mses', 'Beamformer_mean_localization_errors',\n",
      "       'Beamformer_aucs_far', 'Beamformer_aucs_close',\n",
      "       'Beamformer_aucs_combined', 'Beamformer_nmses', 'Beamformer_mses'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (1), usually from a call to set_ticks, does not match the number of ticklabels (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5900/2370250573.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdep_var_regex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdep_var_regex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cols'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vals'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdep_var_regex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vals'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cols'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'point'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsrc_bin_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_name_nice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdep_var_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox_to_anchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborderaxespad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Handle removed axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmove_color_to_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfindobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m   1062\u001b[0m                         raise AttributeError(f\"{type(self).__name__!r} object \"\n\u001b[0;32m   1063\u001b[0m                                              f\"has no property {k!r}\")\n\u001b[1;32m-> 1064\u001b[1;33m                     \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_set_ticklabels\u001b[1;34m(self, labels, fontdict, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfontdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[1;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1709\u001b[0m             \u001b[1;31m# remove all tick labels, so only error for > 0 ticklabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1711\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1712\u001b[0m                     \u001b[1;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m                     \u001b[1;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of FixedLocator locations (1), usually from a call to set_ticks, does not match the number of ticklabels (4)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = list(np.concatenate([[i]*src.data.shape[1] for i, src in enumerate(sim_test.source_data)]))\n",
    "\n",
    "# nsrc_bins = (1, 2, 4, 8, 16, 20)\n",
    "nsrc_bins = (1, 2, 4, 8, 10)\n",
    "\n",
    "nsrc_bin_labels = [str(nsrc_bins[i]) + ' - ' + str(nsrc_bins[i+1]) for i in range(len(nsrc_bins)-1)]\n",
    "\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['number_of_sources_bins'] = np.digitize(df['number_of_sources'].values, bins=nsrc_bins)\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'number_of_sources_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Number of True Sources'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=nsrc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "\n",
    "beta_bins = (0.5, 0.75, 1, 1.25, 1.5)\n",
    "beta_bin_labels = [str(beta_bins[i]) + ' - ' + str(beta_bins[i+1]) for i in range(len(beta_bins)-1)]\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta_bins']  = np.digitize(df.avg_beta, bins=beta_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'beta_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_beta_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_beta_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=beta_bin_labels, ylabel=metric_name_nice, xlabel='Beta Exponent')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "\n",
    "extent_bins = np.linspace(20, 40, 5)\n",
    "extent_bin_labels = [str(int(round(extent_bins[i]))) + ' - ' + str(int(round(extent_bins[i+1]))) for i in range(len(extent_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_extent_bins']  = np.digitize(df.avg_extent, bins=extent_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'extent_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_extent_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_extent_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=extent_bin_labels, ylabel=metric_name_nice, xlabel='Source extent [mm]')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def multipage(filename, figs=None, dpi=300, png=False):\n",
    "    ''' Saves all open (or list of) figures to filename.pdf with dpi''' \n",
    "    pp = PdfPages(filename)\n",
    "    path = os.path.dirname(filename)\n",
    "    fn = os.path.basename(filename)[:-4]\n",
    "\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for i, fig in enumerate(figs):\n",
    "        print(f'saving fig {fig}\\n')\n",
    "        fig.savefig(pp, format='pdf', dpi=dpi)\n",
    "        if png:\n",
    "            fig.savefig(f'{path}\\\\{i}_{fn}.png', dpi=600)\n",
    "    pp.close()\n",
    "\n",
    "multipage(f'figures\\\\figs_5000_10samples\\\\figs_5000_10samples.pdf', png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "model_names[0] = 'Dense'\n",
    "\n",
    "data = {model_name: nmse for model_name, nmse in zip(model_names, nmses)}\n",
    "plt.figure()\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mse for model_name, mse in zip(model_names, mses)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_far)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_close)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mle for model_name, mle in zip(model_names, mean_localization_errors)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "plt.title('Mean Localization Errors')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance per Difficulty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [nmses, mean_localization_errors, aucs_far, aucs_close]\n",
    "metric_names = ['nmses', 'mean_localization_errors', 'aucs_far', 'aucs_close']\n",
    "\n",
    "covariates = ['target_snr', 'number_of_sources']\n",
    "\n",
    "n_samples = int(np.array(nmses).shape[1] / 20)\n",
    "for covariate in covariates:\n",
    "    sim_params = sim_lstm_test.simulation_info[covariate].values[:n_samples]\n",
    "    sim_params = np.repeat(sim_params, 20)\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        plt.figure()\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            plt.scatter(sim_params, metric[i], label=model_name, s=.7)\n",
    "        plt.title(f'{covariate}, {metric_name}')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def new_sim_params(sr=100, packages_per_second=20):\n",
    "    package_size = int( round( sr / packages_per_second  ) )\n",
    "    package_interval = package_size/sr\n",
    "\n",
    "    n_chan = len(sim_lstm_test.eeg_data.ch_names)\n",
    "    data_package = np.random.randn(n_chan, package_size)\n",
    "\n",
    "    sim_data_package = Simulation(fwd, info, settings=dict(duration_of_trial=0.01*package_size)).simulate(1)\n",
    "    print(f'performing predictions {packages_per_second} times per second')\n",
    "\n",
    "    return sim_data_package, package_interval\n",
    "\n",
    "packages_per_second = 50\n",
    "sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    # stc = net_dense.predict(sim_data_package)\n",
    "    stc = net_lstm.predict(sim_data_package)\n",
    "\n",
    "    stop = time.time()\n",
    "    diff = stop-start\n",
    "    if stop-start > package_interval:\n",
    "        print(f\"took longer than expected: {diff} (instead of {package_interval})\")\n",
    "        print(f'decreasing package interval by one')\n",
    "        packages_per_second -= 1\n",
    "        sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "        print(f'packages_per_second={packages_per_second}\\n')\n",
    "        continue\n",
    "    time.sleep(package_interval-diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
