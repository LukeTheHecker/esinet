{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, r'C:\\Users\\lukas\\Dokumente\\projects\\esinet')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = create_forward_model(info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate Source\n",
      "doing the noise-based source simulation thing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d372d2724bd43ce93a1b630974a36e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Source Data to mne.SourceEstimate object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce5ccfdefd4098bf49deb33e7caf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project sources to EEG...\n",
      "\n",
      "Create EEG trials with noise...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e461a543e1e4d7c94f59422320942c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert EEG matrices to a single instance of mne.Epochs...\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "duration_of_trial = 1.0\n",
    "settings = dict(duration_of_trial=duration_of_trial, method='noise')\n",
    "\n",
    "sim_lstm = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=n_samples)\n",
    "sim_lstm.save(f'simulations/sim_{n_samples}_{int(duration_of_trial*100)}points.pkl')\n",
    "# sim_lstm_prime = Simulation(fwd, info, verbose=True, settings=settings_prime).simulate(n_samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'simulations/sim_10000_100points.pkl', 'rb') as f:\n",
    "    sim_lstm = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & train LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RNN_0 (Bidirectional)        (None, None, 150)         82200     \n",
      "_________________________________________________________________\n",
      "RNN_1 (Bidirectional)        (None, None, 150)         135600    \n",
      "_________________________________________________________________\n",
      "FC_Out (TimeDistributed)     (None, None, 1284)        193884    \n",
      "=================================================================\n",
      "Total params: 411,684\n",
      "Trainable params: 411,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "handle data\n",
      "preprocess data\n",
      "reshape data\n",
      "fit model\n",
      "Epoch 1/150\n",
      " 897/4500 [====>.........................] - ETA: 3:19 - loss: 1273135393960349971647561728.0000 - mae: 397880655872.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14860/2337805250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mnet_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_lstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnet_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet_lstm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dokumente\\projects\\esinet\\esinet\\net.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, optimizer, learning_rate, validation_split, epochs, metrics, device, false_positive_penalty, delta, batch_size, loss, sample_weight, return_history, dropout, patience, tensorboard, *args)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fit model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             history = self.model.fit(x_scaled, y_scaled, \n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# epochs = 150\n",
    "# patience = 10\n",
    "# activation_function = 'relu'\n",
    "# loss = 'mean_squared_error'\n",
    "# dropout = 0.2\n",
    "# optimizer = tf.keras.optimizers.Adam(clipvalue=0.5) #  'adam'  # tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# # Dense net\n",
    "# model_params = dict(activation_function=activation_function, n_dense_layers=2, \n",
    "#     n_dense_units=400, n_lstm_layers=0, model_type='v2')\n",
    "# train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "#     dropout=dropout, loss=loss, optimizer=optimizer, return_history=True,\n",
    "#     batch_size=8)\n",
    "# # Train\n",
    "# net_dense = Net(fwd, **model_params)\n",
    "# _, history_dense = net_dense.fit(sim_lstm, **train_params)\n",
    "\n",
    "# LSTM v2\n",
    "model_params = dict(activation_function=activation_function, n_lstm_layers=2, \n",
    "    n_lstm_units=75, n_dense_layers=0, \n",
    "    model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, tensorboard=True, \n",
    "    dropout=0.1, loss=loss, optimizer=optimizer, return_history=True,\n",
    "    batch_size=2)\n",
    "\n",
    "# Train\n",
    "net_lstm = Net(fwd, **model_params)\n",
    "_, history_lstm = net_lstm.fit(sim_lstm, **train_params)\n",
    "\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']\n",
    "net_dense.save(r'models', name='dense-net-100points-noise')\n",
    "net_lstm.save(r'models', name='lstm-net-100points-noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dense = util.load_net('models/dense-net-100points-noise_0')\n",
    "net_lstm = util.load_net('models/lstm-net-100points-noise_0')\n",
    "\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.util import wrap_mne_inverse\n",
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(duration_of_trial=1, method='noise')\n",
    "# Simulate new data\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(1)\n",
    "snr = sim_test.simulation_info['target_snr'].values[0]\n",
    "# print(sim_test.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "# Predict sources with classical methods\n",
    "prediction_elor_data = wrap_mne_inverse(fwd, sim_test, snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_elor = deepcopy(predictions[0])\n",
    "prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "prediction_mne_data = wrap_mne_inverse(fwd, sim_test, method='MNE', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_mne = deepcopy(predictions[0])\n",
    "prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "prediction_lcmv_data = wrap_mne_inverse(fwd, sim_test, method='beamformer', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_lcmv = deepcopy(predictions[0])\n",
    "prediction_lcmv.data = prediction_lcmv_data / np.abs(np.max(prediction_lcmv_data))\n",
    "# Get predictions and names in order\n",
    "predictions.append(prediction_elor)\n",
    "predictions.append(prediction_mne)\n",
    "predictions.append(prediction_lcmv)\n",
    "model_names.append('eLORETA')\n",
    "model_names.append('MNE')\n",
    "model_names.append('Beamformer')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim_test.simulation_info.number_of_sources.values[0]} sources', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "evoked.plot()\n",
    "# evoked.plot_topomap(title='Ground Truth')\n",
    "evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    \n",
    "    if not 'eloreta' in model_name.lower() and not 'lstm' in model_name.lower() :\n",
    "        continue\n",
    "    try:\n",
    "        brain = prediction.plot(**plot_params)\n",
    "        brain.add_text(0.1, 0.9, model_name, 'title')\n",
    "        # Plot predicted EEG\n",
    "        evoked_esi = util.get_eeg_from_source(prediction, fwd, info, tmin=0.)\n",
    "        evoked_esi.plot()\n",
    "        evoked_esi.plot_topomap(title=model_name)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Evaluation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate\n",
    "n_samples = 5000\n",
    "duration_of_trial = 1.0\n",
    "settings = dict(duration_of_trial=duration_of_trial, method='noise')\n",
    "sim_lstm_test = Simulation(fwd, info, verbose=False, settings=settings).simulate(n_samples=n_samples)\n",
    "sim_lstm_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_noise.pkl')\n",
    "\n",
    "# or Load\n",
    "# with open(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_noise.pkl', 'rb') as f:\n",
    "#     sim_lstm_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from esinet.util import wrap_mne_inverse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model_names = ['Dense', 'LSTM']\n",
    "# Predict\n",
    "print('predict esinets...')\n",
    "predictions = [model.predict(sim_lstm_test) for model in models]\n",
    "\n",
    "print('predict elor')\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_lstm_test, method='eLORETA')\n",
    "model_names.append('eLORETA')\n",
    "predictions.append(pred_elor)\n",
    "\n",
    "print('predict MNE')\n",
    "pred_mne = wrap_mne_inverse(fwd, sim_lstm_test, method='MNE')\n",
    "model_names.append('MNE')\n",
    "predictions.append(pred_mne)\n",
    "\n",
    "print('predict LCMV')\n",
    "pred_lcmv = wrap_mne_inverse(fwd, sim_lstm_test, method='beamformer')\n",
    "model_names.append('LCMV')\n",
    "predictions.append(pred_lcmv)\n",
    "\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "mean_localization_errors = []\n",
    "aucs = []\n",
    "nmses = []\n",
    "mses = []\n",
    "true_sources = np.stack([src.data for src in sim_lstm_test.source_data], axis=0)\n",
    "true_sources = util.collapse(true_sources)\n",
    "\n",
    "size = 5000\n",
    "n_total = true_sources.shape[0]\n",
    "choice = np.random.choice(np.arange(n_total), size=size, replace=False)\n",
    "true_sources = true_sources[choice]\n",
    "for prediction in tqdm(predictions):\n",
    "\n",
    "    predicted_sources = util.collapse(np.stack([src.data for src in prediction], axis=0))\n",
    "    print(predicted_sources.shape)\n",
    "    predicted_sources = predicted_sources[choice]\n",
    "    print(predicted_sources.shape)\n",
    "    print('mle calculation....')\n",
    "    mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    auc = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=25) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    \n",
    "    mean_localization_errors.append(mean_localization_error)\n",
    "    aucs.append(auc)\n",
    "    nmses.append(nmse)\n",
    "    mses.append(mse)\n",
    "\n",
    "aucs_far = [auc[:, 1] for auc in np.array(aucs)]\n",
    "aucs_close = [auc[:, 0] for auc in np.array(aucs)]\n",
    "\n",
    "metrics = dict(\n",
    "    mean_localization_errors=mean_localization_errors,\n",
    "    aucs_far=aucs_far,\n",
    "    aucs_close=aucs_close,\n",
    "    nmses=nmses,\n",
    "    mses=mses\n",
    ")\n",
    "\n",
    "with open(f'results\\\\metrics_{size}_{sim_lstm_test.source_data[0].data.shape[1]}points_noise.pkl', 'wb') as f:\n",
    "    pkl.dump([metrics, choice], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_5000_10points.pkl', 'rb') as f:\n",
    "    [metrics, choice] = pkl.load(f)\n",
    "with open(f'simulations\\\\sim_test_5000_10points.pkl', 'rb') as f:\n",
    "    sim_lstm_test = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.median(df.Dense_nmses.values) / np.median(df.LSTM_nmses.values)\n",
    "# np.median(df.Dense_mean_localization_errors.values) / np.median(df.LSTM_mean_localization_errors.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Dense', 'LSTM', 'eLORETA', 'MNE', 'Beamformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "xticks = dict(ticks=np.arange(len(model_names)), labels=model_names)\n",
    "plot = sns.boxplot  # violinplot\n",
    "variable_keys = metrics.keys()\n",
    "names = [ 'Mean Localization Errors',  'Far area under the curve', 'Close area under the curve', 'Normalized Mean Squared Errors']\n",
    "plt.figure(figsize=(14, 8))\n",
    "    \n",
    "subplot_nums = np.arange(221, 226)\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    plt.subplot(num)\n",
    "    plot(data=metrics[variable_key])\n",
    "    plt.title(name)\n",
    "    plt.xticks(**xticks)\n",
    "plt.tight_layout(pad=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "subplot_nums = np.arange(221, 226)\n",
    "inverse_idc = [0,1]\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    var = np.array(metrics[variable_key])[inverse_idc]\n",
    "    var = var[:, ~np.isnan(var).any(axis=0)]\n",
    "    if np.any(np.isnan(var)):\n",
    "        print(\"still nans\")\n",
    "\n",
    "    plt.subplot(num)\n",
    "    plt.scatter(var[0], var[1], edgecolors='k', s=2)\n",
    "    # origin\n",
    "    lo_lim = np.nanmin(var)\n",
    "    hi_lim = np.nanmax(var)\n",
    "    plt.plot([lo_lim, hi_lim], [lo_lim, hi_lim], '--r')\n",
    "\n",
    "    prop_higher = np.sum(var[1] > var[0]) / len(var[0])\n",
    "    cohens_d = (np.nanmean(var[0]) - np.nanmean(var[1])) / np.mean([np.nanstd(var[0]), np.nanstd(var[1])])\n",
    "    median_diff = np.abs(np.nanmedian(var[0]-var[1]))\n",
    "    title = f'{name} (higher in {100*prop_higher:.1f} %)\\nmedian_difference: {median_diff}\\ncohens d: {abs(cohens_d):.2f}'\n",
    "    plt.title(title)\n",
    "    plt.xlabel(model_names[inverse_idc[0]])\n",
    "    plt.ylabel(model_names[inverse_idc[1]])\n",
    "    \n",
    "    # pad\n",
    "    lo_lim -= abs(hi_lim-lo_lim)*0.05\n",
    "    hi_lim += abs(hi_lim-lo_lim)*0.05\n",
    "    \n",
    "    # plot properties\n",
    "    plt.xlim(lo_lim, hi_lim)\n",
    "    plt.ylim(lo_lim, hi_lim)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "# snr_bins = (0.5, 5, 10)\n",
    "\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "# df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "# df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "# df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex='snr_'), df.filter(regex=metric_name)), axis=1).melt('target_snr_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='target_snr_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=snr_bin_labels, ylabel=metric_name_nice, xlabel='Signal to Noise Ratio (SNR)')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), 100)\n",
    "\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "ecc_bins = np.linspace(int(df['avg_eccentricity'].values.min()), np.ceil(df['avg_eccentricity'].values.max()).astype(int), num=5)\n",
    "ecc_bin_labels = [str(ecc_bins[i]) + ' - ' + str(ecc_bins[i+1]) for i in range(len(ecc_bins)-1)]\n",
    "df['avg_eccentricity_bins'] = np.digitize(df['avg_eccentricity'].values, bins=ecc_bins)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'avg_eccentricity_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Average eccentricity'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=ecc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on No. of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), 100)\n",
    "\n",
    "nsrc_bins = (1, 2, 4, 8, 16, 20)\n",
    "nsrc_bin_labels = [str(nsrc_bins[i]) + ' - ' + str(nsrc_bins[i+1]) for i in range(len(nsrc_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['number_of_sources_bins'] = np.digitize(df['number_of_sources'].values, bins=nsrc_bins)\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'number_of_sources_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Number of True Sources'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=nsrc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "\n",
    "beta_bins = (0.5, 0.75, 1, 1.25, 1.5)\n",
    "beta_bin_labels = [str(beta_bins[i]) + ' - ' + str(beta_bins[i+1]) for i in range(len(beta_bins)-1)]\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta_bins']  = np.digitize(df.avg_beta, bins=beta_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'beta_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_beta_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_beta_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=beta_bin_labels, ylabel=metric_name_nice, xlabel='Beta Exponent')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "\n",
    "extent_bins = np.linspace(0, 50, 5)\n",
    "extent_bin_labels = [str(int(round(extent_bins[i]))) + ' - ' + str(int(round(extent_bins[i+1]))) for i in range(len(extent_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_extent_bins']  = np.digitize(df.avg_extent, bins=extent_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'extent_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_extent_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_extent_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=extent_bin_labels, ylabel=metric_name_nice, xlabel='Source extent [mm]')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def multipage(filename, figs=None, dpi=300, png=False):\n",
    "    ''' Saves all open (or list of) figures to filename.pdf with dpi''' \n",
    "    pp = PdfPages(filename)\n",
    "    path = os.path.dirname(filename)\n",
    "    fn = os.path.basename(filename)[:-4]\n",
    "\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for i, fig in enumerate(figs):\n",
    "        print(f'saving fig {fig}\\n')\n",
    "        fig.savefig(pp, format='pdf', dpi=dpi)\n",
    "        if png:\n",
    "            fig.savefig(f'{path}\\\\{i}_{fn}.png', dpi=600)\n",
    "    pp.close()\n",
    "\n",
    "multipage(f'figures\\\\figs_5000_10samples\\\\figs_5000_10samples.pdf', png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "model_names[0] = 'Dense'\n",
    "\n",
    "data = {model_name: nmse for model_name, nmse in zip(model_names, nmses)}\n",
    "plt.figure()\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mse for model_name, mse in zip(model_names, mses)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_far)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_close)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mle for model_name, mle in zip(model_names, mean_localization_errors)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "plt.title('Mean Localization Errors')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance per Difficulty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [nmses, mean_localization_errors, aucs_far, aucs_close]\n",
    "metric_names = ['nmses', 'mean_localization_errors', 'aucs_far', 'aucs_close']\n",
    "\n",
    "covariates = ['target_snr', 'number_of_sources']\n",
    "\n",
    "n_samples = int(np.array(nmses).shape[1] / 20)\n",
    "for covariate in covariates:\n",
    "    sim_params = sim_lstm_test.simulation_info[covariate].values[:n_samples]\n",
    "    sim_params = np.repeat(sim_params, 20)\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        plt.figure()\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            plt.scatter(sim_params, metric[i], label=model_name, s=.7)\n",
    "        plt.title(f'{covariate}, {metric_name}')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def new_sim_params(sr=100, packages_per_second=20):\n",
    "    package_size = int( round( sr / packages_per_second  ) )\n",
    "    package_interval = package_size/sr\n",
    "\n",
    "    n_chan = len(sim_lstm_test.eeg_data.ch_names)\n",
    "    data_package = np.random.randn(n_chan, package_size)\n",
    "\n",
    "    sim_data_package = Simulation(fwd, info, settings=dict(duration_of_trial=0.01*package_size)).simulate(1)\n",
    "    print(f'performing predictions {packages_per_second} times per second')\n",
    "\n",
    "    return sim_data_package, package_interval\n",
    "\n",
    "packages_per_second = 50\n",
    "sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    # stc = net_dense.predict(sim_data_package)\n",
    "    stc = net_lstm.predict(sim_data_package)\n",
    "\n",
    "    stop = time.time()\n",
    "    diff = stop-start\n",
    "    if stop-start > package_interval:\n",
    "        print(f\"took longer than expected: {diff} (instead of {package_interval})\")\n",
    "        print(f'decreasing package interval by one')\n",
    "        packages_per_second -= 1\n",
    "        sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "        print(f'packages_per_second={packages_per_second}\\n')\n",
    "        continue\n",
    "    time.sleep(package_interval-diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
