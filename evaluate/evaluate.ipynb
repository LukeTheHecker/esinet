{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet import forward\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = forward.get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = forward.create_forward_model(info=info)\n",
    "fwd_free = forward.create_forward_model(info=info, fixed_ori=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_standard = util.load_net('models/LSTM Medium_1-1000points_standard-cosine-mse_0')\n",
    "# lstm_noise = util.load_net('models/LSTM Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "dense_standard = util.load_net('models/Dense Medium_1-1000points_standard-cosine-mse_0')\n",
    "# dense_noise = util.load_net('models/Dense Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "convdip_standard = util.load_net('models/ConvDip Medium_1-1000points_standard-cosine-mse_0')\n",
    "# convdip_noise = util.load_net('models/ConvDip Medium_1-1000points_noise-cosine-mse_0')\n",
    "\n",
    "# models = [lstm_standard, lstm_noise, dense_standard, dense_noise, convdip_standard, convdip_noise]\n",
    "# model_names = ['LSTM Standard', 'LSTM Noise', 'Dense Standard', 'Dense Noise', 'ConvDip Standard', 'ConvDip Noise']\n",
    "models = [lstm_standard, dense_standard, convdip_standard]\n",
    "model_names = ['LSTM Standard', 'Dense Standard', 'ConvDip Standard']\n",
    "\n",
    "# models = [lstm_noise, dense_noise, convdip_noise]\n",
    "# model_names = ['LSTM Noise', 'Dense Noise', 'ConvDip Noise']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ability to process long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "settings_eval = dict(duration_of_trial=100, method='standard', number_of_sources=10)\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(2)\n",
    "idx = 0\n",
    "durs = [100, 50, 10, 1, 0.1, 0.02]\n",
    "# durs = [np.random.uniform(0.01, 100) for _ in range(5)]\n",
    "for net, name in zip(models, model_names):\n",
    "    for dur in durs:\n",
    "        sim_test.crop(tmax=dur)\n",
    "        prediction = net.predict(sim_test)\n",
    "        error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "        r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "        title = f'{dur}: error: {error:.4}, r: {r:.2f}\\n'\n",
    "        print(f\"{name}:\")\n",
    "        print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(duration_of_trial=1, method='standard')\n",
    "# settings_eval = dict(duration_of_trial=1, method='noise', exponent=3)\n",
    "\n",
    "# Simulate new data\n",
    "# sim_test = Simulation(fwd, info, settings=settings_eval).simulate(2)\n",
    "snr = sim_test.simulation_info['target_snr'].values[0]\n",
    "# print(sim_test.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "\n",
    "# # Predict sources with classical methods\n",
    "# eLORETA\n",
    "# prediction_elor_data = util.wrap_mne_inverse(fwd, sim_test, snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_elor = deepcopy(predictions[0][0])\n",
    "# prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "# # MNE\n",
    "# prediction_mne_data = util.wrap_mne_inverse(fwd, sim_test, method='MNE', snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_mne = deepcopy(predictions[0][0])\n",
    "# prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "# # Beamformer\n",
    "# prediction_lcmv_data = util.wrap_mne_inverse(fwd_free, sim_test, method='beamformer', snr=snr)[idx].data.astype(np.float32)\n",
    "# prediction_lcmv = deepcopy(predictions[0][0])\n",
    "# prediction_lcmv.data = prediction_lcmv_data / np.abs(np.max(prediction_lcmv_data))\n",
    "\n",
    "# Get predictions and names in order\n",
    "# predictions.append([prediction_elor])\n",
    "# predictions.append([prediction_mne])\n",
    "# predictions.append([prediction_lcmv])\n",
    "\n",
    "# model_names.append('eLORETA')\n",
    "# model_names.append('MNE')\n",
    "# model_names.append('Beamformer')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim_test.simulation_info.number_of_sources.values[0]} sources, snr={snr:.1f}', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "# evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "# evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "# evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "model_selection = model_names#['LSTM Noise', 'LSTM Noise', 'eLORETA']\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    \n",
    "    if not any([model_name.lower() in model_select.lower() for model_select in model_selection]):\n",
    "        continue\n",
    "    error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    \n",
    "    brain = prediction[idx].plot(**plot_params)\n",
    "\n",
    "    title = f'{model_name}, error: {error:.4}, r: {r}'\n",
    "    print(title)\n",
    "    brain.add_text(0.1, 0.9, title, 'title')\n",
    "    # Plot predicted EEG\n",
    "    evoked_esi = util.get_eeg_from_source(prediction[idx], fwd, info, tmin=0.)\n",
    "    evoked_esi.plot_topomap(title=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 1000\n",
    "# duration_of_trial = (0.01, 2)\n",
    "# method = 'standard'\n",
    "# settings = dict(duration_of_trial=duration_of_trial, method=method)\n",
    "# sim_test = Simulation(fwd, info, verbose=False, settings=settings).simulate(n_samples=n_samples)\n",
    "# if type(duration_of_trial) == tuple:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial[0]*100)}-{int(duration_of_trial[1]*100)}points_{method}.pkl')\n",
    "# else:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_{method}.pkl')\n",
    "\n",
    "# or Load\n",
    "with open(f'simulations\\\\sim_test_1000_1-200points_standard.pkl', 'rb') as f:\n",
    "    sim_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "# from esinet.util import wrap_mne_inverse\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from tqdm.notebook import tqdm\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# model_names_tmp = deepcopy(model_names)\n",
    "# # Predict\n",
    "# print('predict esinets...')\n",
    "# predictions = [model.predict(sim_test) for model in models]\n",
    "\n",
    "# print('predict elor')\n",
    "# pred_elor = wrap_mne_inverse(fwd, sim_test, method='eLORETA')\n",
    "# model_names_tmp.append('eLORETA')\n",
    "# predictions.append(pred_elor)\n",
    "\n",
    "# print('predict MNE')\n",
    "# pred_mne = wrap_mne_inverse(fwd, sim_test, method='MNE')\n",
    "# model_names_tmp.append('MNE')\n",
    "# predictions.append(pred_mne)\n",
    "\n",
    "# print('predict LCMV')\n",
    "# pred_lcmv = wrap_mne_inverse(fwd_free, sim_test, method='beamformer', parallel=False)\n",
    "# model_names_tmp.append('LCMV')\n",
    "# predictions.append(pred_lcmv)\n",
    "\n",
    "# pos = util.unpack_fwd(fwd)[2]\n",
    "# argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "# metrics = dict()\n",
    "# true_sources = np.concatenate([src.data for src in sim_test.source_data], axis=1).T\n",
    "\n",
    "for prediction, model_name in tqdm(zip(predictions[3:], model_names_tmp[3:])):\n",
    "    print('\\n', model_name, ':\\n')\n",
    "     \n",
    "    predicted_sources = np.concatenate([src.data for src in prediction], axis=1).T\n",
    "\n",
    "    print('mle calculation....')\n",
    "    mean_localization_errors = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    \n",
    "    print('auc calculation....')\n",
    "    # aucs_combined = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=5) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    aucs_combined = Parallel(n_jobs=-1, backend='loky') \\\n",
    "        (delayed(eval_auc)(true_source, predicted_source, pos, epsilon=0.25, n_redraw=5)\n",
    "        for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources)))\n",
    "    print('nmse calculation....')\n",
    "    nmses = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    print('mse calculation....')\n",
    "    mses = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "\n",
    "    aucs_far = [auc[1] for auc in np.array(aucs_combined)]\n",
    "    aucs_close = [auc[0] for auc in np.array(aucs_combined)]\n",
    "    aucs_combined = [np.nanmean([auc[0], auc[1]]) for auc in np.array(aucs_combined)]\n",
    "\n",
    "    metric = pd.DataFrame(dict(\n",
    "        mean_localization_errors=mean_localization_errors,\n",
    "        aucs_combined=aucs_combined,\n",
    "        aucs_far=aucs_far,\n",
    "        aucs_close=aucs_close,\n",
    "        nmses=nmses,\n",
    "        mses=mses\n",
    "        )\n",
    "    )\n",
    "    metric.name = model_name\n",
    "    metrics[model_name] = metric\n",
    "    \n",
    "\n",
    "\n",
    "with open(f'results\\\\metrics_{len(true_sources)}_1-200points_standard.pkl', 'wb') as f:\n",
    "    pkl.dump([metrics, sim_test.simulation_info], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_101947_1-200points_standard.pkl', 'rb') as f:\n",
    "    [metrics, simulation_info] = pkl.load(f)\n",
    "\n",
    "\n",
    "# with open(f'simulations\\\\sim_test_5000_10points.pkl', 'rb') as f:\n",
    "#     sim_lstm_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df[1] for df in list(metrics.items())]\n",
    "\n",
    "for i, (key, df) in enumerate(metrics.items()):\n",
    "    dfs[i]['method'] = [key]*df.shape[0]\n",
    "    dfs[i]['sample_id'] = np.arange(df.shape[0])\n",
    "df_aio = pd.concat(dfs)\n",
    "df_aio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "\n",
    "cols = df_aio.iloc[:, 0:6].columns\n",
    "for y in cols:\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df_aio, x='method', y=y)\n",
    "    plt.title(y.replace('_', ' ').title())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "methods_of_interest = ['Dense Standard', 'LSTM Standard']\n",
    "# df_select = df_aio[df_aio['method'].str.contains('|'.join(methods_of_interest))]\n",
    "\n",
    "cols = df_aio.iloc[:, 0:6].columns\n",
    "for method_name in cols:\n",
    "\n",
    "    vals_A = df_aio[df_aio['method'].str.contains(methods_of_interest[0])][method_name].values\n",
    "    vals_B = df_aio[df_aio['method'].str.contains(methods_of_interest[1])][method_name].values\n",
    "    d = {methods_of_interest[0]: vals_A, methods_of_interest[1]: vals_B,} \n",
    "    df_tmp = pd.DataFrame(d)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = sns.scatterplot(data=df_tmp, x=methods_of_interest[0], y=methods_of_interest[1])\n",
    "\n",
    "\n",
    "    xlim, ylim = (plt.xlim(), plt.ylim())\n",
    "    plt.plot(xlim, ylim, '--k')\n",
    "    xlim = (xlim[0]*0.95, xlim[1]*1.05)\n",
    "    ylim = (ylim[0]*0.95, ylim[1]*1.05)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "    \n",
    "    # Title\n",
    "    prop_higher = np.sum(vals_B > vals_A) / len(vals_A)\n",
    "    cohens_d = (np.nanmean(vals_A) - np.nanmean(vals_B)) / np.mean([np.nanstd(vals_A), np.nanstd(vals_B)])\n",
    "    median_diff = np.abs(np.nanmedian(vals_A-vals_B))\n",
    "    method_name_title = method_name.replace('_', ' ').title()\n",
    "    title = f'{method_name_title} ({methods_of_interest[1]} higher in {100*prop_higher:.1f} %)\\nmedian_difference: {median_diff}\\ncohens d: {abs(cohens_d):.2f}'\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    # break\n",
    "del d, df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "variable = df.duration_of_trials.values\n",
    "values = df.Dense_Standard_aucs_combined\n",
    "plt.figure()\n",
    "plt.scatter(variable, values, s=0.1)\n",
    "plt.xlabel(\"Duration of Trials\")\n",
    "plt.ylabel(\"LSTM AUC Combined\")\n",
    "r, p = pearsonr(variable, values)\n",
    "title = f\"r={np.real(r):.2f}, p={p:.4f}\"\n",
    "plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "target_column = 'duration_of_trials'\n",
    "binning = True\n",
    "n_bins = 8\n",
    "stretched_indices = list(np.concatenate([[i]*src.data.shape[1] for i, src in enumerate(sim_test.source_data)]))\n",
    "\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "if binning:\n",
    "    bins = np.linspace(df[target_column].min(), df[target_column].max()*1.01, num=n_bins)\n",
    "    bin_labels = [str(round(bins[i], 1)) + ' - ' + str(round(bins[i+1], 1)) for i in range(len(bins)-1)]\n",
    "    df['bins'] = np.digitize(df[target_column].values, bins=bins)\n",
    "    target_column = 'bins'\n",
    "else:\n",
    "    bins = list(set(df[target_column].values))\n",
    "    bins[-1] *= 1.01\n",
    "    bin_labels = [str(bins[i]) for i in range(len(bins))]\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(list(set(df_aio.method.values))):\n",
    "    if \"lcmv\" in model_name.lower():\n",
    "        continue\n",
    "    cols = df_aio[df_aio.method==model_name].iloc[:, 0:6].columns\n",
    "    values = df_aio[df_aio.method==model_name].iloc[:, 0:6].values\n",
    "\n",
    "    for metric_name, metric in zip(cols, values.T):\n",
    "        col_name = model_name.replace(' ', '_') + '_' + metric_name.replace(' ', '_')\n",
    "        df[col_name] = metric\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = target_column\n",
    "dep_var_label = target_column.replace('_', ' ').title()\n",
    "metric_names_nice = [col.replace('_', ' ').title() for col in df_aio.iloc[:, :6].columns]\n",
    "for metric_name, metric_name_nice in zip(df_aio.iloc[:, :6].columns,  metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = list(np.concatenate([[i]*src.data.shape[1] for i, src in enumerate(sim_test.source_data)]))\n",
    "target_column = 'target_snr'\n",
    "binning = True\n",
    "n_bins = 4\n",
    "\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "if binning:\n",
    "    bins = np.linspace(df[target_column].min(), df[target_column].max()*1.01, num=n_bins)\n",
    "    bin_labels = [str(int(round(bins[i]))) + ' - ' + str(int(round(bins[i+1]))) for i in range(len(bins)-1)]\n",
    "    df['duration_bins'] = np.digitize(df[target_column].values, bins=bins)\n",
    "    target_column = 'duration_bins'\n",
    "else:\n",
    "    bins = list(set(df[target_column].values))\n",
    "    bins[-1] *= 1.01\n",
    "    bin_labels = [str(bins[i]) for i in range(len(bins))]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name.replace(' ', '_') + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = target_column\n",
    "dep_var_label = target_column.replace('_', ' ').title()\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]',  'AUC Combined [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_test.n_samples), np.mean([src.data.shape[1] for src in sim_test.source_data]))\n",
    "snr_bins = (2, 4, 6, 8, 10, 20)\n",
    "# snr_bins = (0., 10, 20)\n",
    "\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "# df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "# df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "# df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex='snr_'), df.filter(regex=metric_name)), axis=1).melt('target_snr_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='target_snr_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=snr_bin_labels, ylabel=metric_name_nice, xlabel='Signal to Noise Ratio (SNR)')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_test.n_samples), np.mean([src.data.shape[1] for src in sim_test.source_data]))\n",
    "\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "ecc_bins = np.linspace(int(df['avg_eccentricity'].values.min()), np.ceil(df['avg_eccentricity'].values.max()).astype(int), num=5)\n",
    "ecc_bin_labels = [str(ecc_bins[i]) + ' - ' + str(ecc_bins[i+1]) for i in range(len(ecc_bins)-1)]\n",
    "df['avg_eccentricity_bins'] = np.digitize(df['avg_eccentricity'].values, bins=ecc_bins)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'avg_eccentricity_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Average eccentricity'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=ecc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on No. of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = list(np.concatenate([[i]*src.data.shape[1] for i, src in enumerate(sim_test.source_data)]))\n",
    "\n",
    "# nsrc_bins = (1, 2, 4, 8, 16, 20)\n",
    "nsrc_bins = (1, 2, 4, 8, 10)\n",
    "\n",
    "nsrc_bin_labels = [str(nsrc_bins[i]) + ' - ' + str(nsrc_bins[i+1]) for i in range(len(nsrc_bins)-1)]\n",
    "\n",
    "params = list(sim_test.simulation_info.columns)\n",
    "series = [sim_test.simulation_info.iloc[stretched_indices][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['number_of_sources_bins'] = np.digitize(df['number_of_sources'].values, bins=nsrc_bins)\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'number_of_sources_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Number of True Sources'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=nsrc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "\n",
    "beta_bins = (0.5, 0.75, 1, 1.25, 1.5)\n",
    "beta_bin_labels = [str(beta_bins[i]) + ' - ' + str(beta_bins[i+1]) for i in range(len(beta_bins)-1)]\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta_bins']  = np.digitize(df.avg_beta, bins=beta_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'beta_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_beta_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_beta_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=beta_bin_labels, ylabel=metric_name_nice, xlabel='Beta Exponent')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "\n",
    "extent_bins = np.linspace(20, 40, 5)\n",
    "extent_bin_labels = [str(int(round(extent_bins[i]))) + ' - ' + str(int(round(extent_bins[i+1]))) for i in range(len(extent_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_extent_bins']  = np.digitize(df.avg_extent, bins=extent_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'extent_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_extent_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_extent_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=extent_bin_labels, ylabel=metric_name_nice, xlabel='Source extent [mm]')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def multipage(filename, figs=None, dpi=300, png=False):\n",
    "    ''' Saves all open (or list of) figures to filename.pdf with dpi''' \n",
    "    pp = PdfPages(filename)\n",
    "    path = os.path.dirname(filename)\n",
    "    fn = os.path.basename(filename)[:-4]\n",
    "\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for i, fig in enumerate(figs):\n",
    "        print(f'saving fig {fig}\\n')\n",
    "        fig.savefig(pp, format='pdf', dpi=dpi)\n",
    "        if png:\n",
    "            fig.savefig(f'{path}\\\\{i}_{fn}.png', dpi=600)\n",
    "    pp.close()\n",
    "\n",
    "multipage(f'figures\\\\figs_5000_10samples\\\\figs_5000_10samples.pdf', png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "model_names[0] = 'Dense'\n",
    "\n",
    "data = {model_name: nmse for model_name, nmse in zip(model_names, nmses)}\n",
    "plt.figure()\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mse for model_name, mse in zip(model_names, mses)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_far)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_close)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mle for model_name, mle in zip(model_names, mean_localization_errors)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "plt.title('Mean Localization Errors')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance per Difficulty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [nmses, mean_localization_errors, aucs_far, aucs_close]\n",
    "metric_names = ['nmses', 'mean_localization_errors', 'aucs_far', 'aucs_close']\n",
    "\n",
    "covariates = ['target_snr', 'number_of_sources']\n",
    "\n",
    "n_samples = int(np.array(nmses).shape[1] / 20)\n",
    "for covariate in covariates:\n",
    "    sim_params = sim_lstm_test.simulation_info[covariate].values[:n_samples]\n",
    "    sim_params = np.repeat(sim_params, 20)\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        plt.figure()\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            plt.scatter(sim_params, metric[i], label=model_name, s=.7)\n",
    "        plt.title(f'{covariate}, {metric_name}')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def new_sim_params(sr=100, packages_per_second=20):\n",
    "    package_size = int( round( sr / packages_per_second  ) )\n",
    "    package_interval = package_size/sr\n",
    "\n",
    "    n_chan = len(sim_lstm_test.eeg_data.ch_names)\n",
    "    data_package = np.random.randn(n_chan, package_size)\n",
    "\n",
    "    sim_data_package = Simulation(fwd, info, settings=dict(duration_of_trial=0.01*package_size)).simulate(1)\n",
    "    print(f'performing predictions {packages_per_second} times per second')\n",
    "\n",
    "    return sim_data_package, package_interval\n",
    "\n",
    "packages_per_second = 50\n",
    "sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    # stc = net_dense.predict(sim_data_package)\n",
    "    stc = net_lstm.predict(sim_data_package)\n",
    "\n",
    "    stop = time.time()\n",
    "    diff = stop-start\n",
    "    if stop-start > package_interval:\n",
    "        print(f\"took longer than expected: {diff} (instead of {package_interval})\")\n",
    "        print(f'decreasing package interval by one')\n",
    "        packages_per_second -= 1\n",
    "        sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "        print(f'packages_per_second={packages_per_second}\\n')\n",
    "        continue\n",
    "    time.sleep(package_interval-diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
