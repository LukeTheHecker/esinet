{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet import forward\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.9s remaining:    2.1s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.9s remaining:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "info = forward.get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = forward.create_forward_model(info=info)\n",
    "fwd_free = forward.create_forward_model(info=info, fixed_ori=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_standard = util.load_net('models/LSTM Medium_1-1000points_standard-cosine_0')\n",
    "dense_standard = util.load_net('models/Dense Medium_1-1000points_standard-cosine_0')\n",
    "convdip_standard = util.load_net('models/ConvDip Medium_1-1000points_standard-cosine_0')\n",
    "\n",
    "models = [lstm_standard, dense_standard, convdip_standard]\n",
    "model_names = ['LSTM', 'Fully-Connected', 'ConvDip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "model_names_tmp = deepcopy(model_names)\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(method='standard')\n",
    "# settings_eval = dict( method='standard')\n",
    "\n",
    "# Simulate new data\n",
    "sim = Simulation(fwd, info, settings=settings_eval).simulate(2)\n",
    "# snr = sim.simulation_info['target_snr'].values[0]\n",
    "snr = None\n",
    "# print(sim.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim) for model in models]\n",
    "\n",
    "# Predict sources with classical methods\n",
    "# eLORETA\n",
    "prediction_elor_data = util.wrap_mne_inverse(fwd, sim, method='eLORETA', \n",
    "    add_baseline=True, n_baseline=400)[idx].data.astype(np.float32)\n",
    "prediction_elor = deepcopy(predictions[0][0])\n",
    "prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "# MNE\n",
    "prediction_mne_data = util.wrap_mne_inverse(fwd, sim, method='MNE', \n",
    "    add_baseline=True, n_baseline=400)[idx].data.astype(np.float32)\n",
    "prediction_mne = deepcopy(predictions[0][0])\n",
    "prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "# Beamformer\n",
    "prediction_lcmv_data = util.wrap_mne_inverse(fwd, sim, method='lcmv', \n",
    "    add_baseline=True, n_baseline=400, parallel=False)[idx].data.astype(np.float32)\n",
    "prediction_lcmv = deepcopy(predictions[0][0])\n",
    "prediction_lcmv.data = prediction_lcmv_data / np.max(np.abs(prediction_lcmv_data))\n",
    "\n",
    "# Get predictions and names in order\n",
    "predictions.append([prediction_elor])\n",
    "predictions.append([prediction_mne])\n",
    "predictions.append([prediction_lcmv])\n",
    "\n",
    "model_names_tmp.append('eLORETA')\n",
    "model_names_tmp.append('MNE')\n",
    "model_names_tmp.append('LCMV')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim.simulation_info.number_of_sources.values[0]} sources', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim.eeg_data[idx].average()\n",
    "# evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "# evoked = util.get_eeg_from_source(sim.source_data[idx], fwd, info, tmin=0.)\n",
    "# evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "model_selection = model_names_tmp#['LCMV',]\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names_tmp, predictions):\n",
    "    \n",
    "    if not any([model_name.lower() in model_select.lower() for model_select in model_selection]):\n",
    "        continue\n",
    "    error = util.batch_nmse(sim.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim.source_data[idx].data, prediction[idx].data)\n",
    "    \n",
    "    brain = prediction[idx].plot(**plot_params)\n",
    "\n",
    "    title = f'{model_name}, error: {error:.4}, r: {r}'\n",
    "    print(title)\n",
    "    brain.add_text(0.1, 0.9, title, 'title')\n",
    "    # Plot predicted EEG\n",
    "    # evoked_esi = util.get_eeg_from_source(prediction[idx], fwd, info, tmin=0.)\n",
    "    # evoked_esi.plot_topomap(title=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 1000\n",
    "# duration_of_trial = (0.01, 2)\n",
    "# method = 'standard'\n",
    "# settings = dict(duration_of_trial=duration_of_trial, method=method)\n",
    "# sim_test = Simulation(fwd, info, verbose=False, settings=settings).simulate(n_samples=n_samples)\n",
    "# if type(duration_of_trial) == tuple:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial[0]*100)}-{int(duration_of_trial[1]*100)}points_{method}.pkl')\n",
    "# else:\n",
    "#     sim_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_{method}.pkl')\n",
    "\n",
    "# or Load\n",
    "with open(f'simulations\\\\sim_test_1000_1-200points_standard.pkl', 'rb') as f:\n",
    "    sim_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from esinet.util import wrap_mne_inverse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "model_names_tmp = deepcopy(model_names)\n",
    "# Predict\n",
    "print('predict esinets...')\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "\n",
    "print('predict elor')\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_test, method='eLORETA', add_baseline=True, \n",
    "    n_baseline=400)\n",
    "model_names_tmp.append('eLORETA')\n",
    "predictions.append(pred_elor)\n",
    "\n",
    "print('predict MNE')\n",
    "pred_mne = wrap_mne_inverse(fwd, sim_test, method='MNE', add_baseline=True, \n",
    "    n_baseline=400)\n",
    "model_names_tmp.append('MNE')\n",
    "predictions.append(pred_mne)\n",
    "\n",
    "print('predict LCMV')\n",
    "pred_lcmv = wrap_mne_inverse(fwd, sim_test, method='lcmv', \n",
    "    parallel=False, add_baseline=True, n_baseline=400)\n",
    "model_names_tmp.append('LCMV')\n",
    "predictions.append(pred_lcmv)\n",
    "\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "metrics = dict()\n",
    "true_sources = np.concatenate([src.data for src in sim_test.source_data], axis=1).T\n",
    "\n",
    "for prediction, model_name in tqdm(zip(predictions, model_names_tmp)):\n",
    "    print('\\n', model_name, ':\\n')\n",
    "     \n",
    "    predicted_sources = np.concatenate([src.data for src in prediction], axis=1).T\n",
    "\n",
    "    print('mle calculation....')\n",
    "    mean_localization_errors = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    print(len(mean_localization_errors), len(true_sources), len(predicted_sources))\n",
    "    print('auc calculation....')\n",
    "    # aucs_combined = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=5) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    aucs_combined = Parallel(n_jobs=-1, backend='loky') \\\n",
    "        (delayed(eval_auc)(true_source, predicted_source, pos, epsilon=0.25, n_redraw=5)\n",
    "        for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources)))\n",
    "    print('nmse calculation....')\n",
    "    nmses = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    print('mse calculation....')\n",
    "    mses = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "\n",
    "    aucs_far = [auc[1] for auc in np.array(aucs_combined)]\n",
    "    aucs_close = [auc[0] for auc in np.array(aucs_combined)]\n",
    "    aucs_combined = [np.nanmean([auc[0], auc[1]]) for auc in np.array(aucs_combined)]\n",
    "\n",
    "    metric = pd.DataFrame(dict(\n",
    "        mean_localization_errors=mean_localization_errors,\n",
    "        aucs_combined=aucs_combined,\n",
    "        aucs_far=aucs_far,\n",
    "        aucs_close=aucs_close,\n",
    "        nmses=nmses,\n",
    "        mses=mses\n",
    "        )\n",
    "    )\n",
    "    metric.name = model_name\n",
    "    metrics[model_name] = metric\n",
    "    \n",
    "\n",
    "with open(f'results\\\\metrics_{len(true_sources)}_1-200points_standard.pkl', 'wb') as f:\n",
    "    pkl.dump([metrics, sim_test.simulation_info], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_101947_1-200points_standard.pkl', 'rb') as f:\n",
    "    [metrics, simulation_info] = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs_in_samples = [round(dur*100) for dur in sim_test.simulation_info.duration_of_trials.values]\n",
    "\n",
    "idx = 0\n",
    "indices = []\n",
    "for dur in durs_in_samples:\n",
    "    idc = [idx, dur+idx]\n",
    "    indices.append(idc)\n",
    "    idx += dur\n",
    "metrics_short = dict()\n",
    "\n",
    "\n",
    "for method in metrics.keys():\n",
    "    metrics_short[method] = pd.DataFrame(columns=metrics[method].columns)\n",
    "    for id, idc in enumerate(indices):\n",
    "        sample_summary = metrics[method].iloc[idc[0]:idc[1]].apply(np.nanmean, axis=0)\n",
    "        sample_summary.sample_id = id\n",
    "        metrics_short[method] = metrics_short[method].append(sample_summary, ignore_index=True)\n",
    "    metrics_short[method].method = method\n",
    "    # metrics_short[method].iloc[:, 0:6].values = np.real(metrics_short[method].iloc[:, 0:6])\n",
    "\n",
    "dfs = [df[1] for df in list(metrics_short.items())]\n",
    "\n",
    "for i, (key, df) in enumerate(metrics_short.items()):\n",
    "    dfs[i]['method'] = [key]*df.shape[0]\n",
    "    dfs[i]['sample_id'] = np.arange(df.shape[0])\n",
    "df_aio = pd.concat(dfs)\n",
    "df_aio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Overview \n",
    "### All Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.6, font='helvetica')\n",
    "%matplotlib qt\n",
    "\n",
    "cols = ['mean_localization_errors', 'aucs_combined', 'nmses']  # df_aio.iloc[:, 0:6].columns\n",
    "metric_names = [\"Mean Localization Error [mm]\", \"Area Under the Curve\", \"Normalized Mean Squared Error\"]\n",
    "for y, metric_name in zip(cols, metric_names):\n",
    "    plt.figure(figsize=(13, 7))\n",
    "    sns.boxplot(data=df_aio, x='method', y=y)\n",
    "    plt.title(metric_name)\n",
    "    plt.xlabel(\"Inverse Solution\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.ylim(0, None)\n",
    "# util.multipage(r'C:\\Users\\lukas\\Sync\\lstm_inverse_problem\\figures\\results\\boxplot_overview_allsims.pdf', png=True)\n",
    "print(df_aio.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.6, font='helvetica')\n",
    "%matplotlib qt\n",
    "\n",
    "idc_single_source = np.argwhere(simulation_info.number_of_sources.values==1)[:, 0]\n",
    "df_single = df_aio[df_aio.sample_id.isin(idc_single_source)]\n",
    "\n",
    "cols = ['mean_localization_errors', 'aucs_combined', 'nmses']  # df_single.iloc[:, 0:6].columns\n",
    "metric_names = [\"Mean Localization Error [mm]\", \"Area Under the Curve\", \"Normalized Mean Squared Error\"]\n",
    "for y, metric_name in zip(cols, metric_names):\n",
    "    plt.figure(figsize=(13, 7))\n",
    "    sns.boxplot(data=df_single, x='method', y=y)\n",
    "    # sns.swarmplot(data=df_single, x='method', y=y)\n",
    "    plt.title(metric_name)\n",
    "    plt.xlabel(\"Inverse Solution\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.ylim(0, None)\n",
    "# util.multipage(r'C:\\Users\\lukas\\Sync\\lstm_inverse_problem\\figures\\results\\boxplot_overview_singlesource.pdf', png=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_interest = df_aio\n",
    "\n",
    "methods = set(df_of_interest.method.values)\n",
    "columns = ['method', 'mean_localization_errors', 'aucs_combined', 'nmses']  # df_aio.iloc[:, 0:6].columns\n",
    "column_names = [\"Inverse Solution\", \"MLE [mm] (SD)\", \"AUC [%] (SD)\", \"nMSE (SD)\"]\n",
    "decimals = [None, 2, 2, 4]\n",
    "\n",
    "table = pd.DataFrame(columns=column_names)\n",
    "for i, method in enumerate(methods):\n",
    "    row_dict = {column_names[0]:method}\n",
    "    for j, (column, column_name, decimal) in enumerate(zip(columns[1:], column_names[1:], decimals[1:])):\n",
    "        values = df_of_interest[df_of_interest.method==method][column].values\n",
    "        row_dict[column_name] = str(round(np.nanmedian(values), decimal)) + ' (' + str(round(np.nanstd(values), decimal)) + ')'\n",
    "    table = table.append(row_dict, ignore_index=True)\n",
    "table = table.sort_values(\"AUC [%] (SD)\", ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = df_aio[df_aio.sample_id.isin(idc_single_source)]\n",
    "\n",
    "df_of_interest = df_single\n",
    "\n",
    "methods = set(df_of_interest.method.values)\n",
    "columns = ['method', 'mean_localization_errors', 'aucs_combined', 'nmses']  # df_aio.iloc[:, 0:6].columns\n",
    "column_names = [\"Inverse Solution\", \"MLE [mm] (SD)\", \"AUC [%] (SD)\", \"nMSE (SD)\"]\n",
    "decimals = [None, 2, 2, 4]\n",
    "\n",
    "table = pd.DataFrame(columns=column_names)\n",
    "for i, method in enumerate(methods):\n",
    "    row_dict = {column_names[0]:method}\n",
    "    for j, (column, column_name, decimal) in enumerate(zip(columns[1:], column_names[1:], decimals[1:])):\n",
    "        values = df_of_interest[df_of_interest.method==method][column].values\n",
    "        row_dict[column_name] = str(round(np.nanmedian(values), decimal)) + ' (' + str(round(np.nanstd(values), decimal)) + ')'\n",
    "    table = table.append(row_dict, ignore_index=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "methods_of_interest = ['LSTM Standard', 'Dense Standard']\n",
    "# df_select = df_aio[df_aio['method'].str.contains('|'.join(methods_of_interest))]\n",
    "\n",
    "cols = df_aio.iloc[:, 0:6].columns\n",
    "for method_name in cols:\n",
    "\n",
    "    vals_A = df_aio[df_aio['method'].str.contains(methods_of_interest[0])][method_name].values\n",
    "    vals_B = df_aio[df_aio['method'].str.contains(methods_of_interest[1])][method_name].values\n",
    "    d = {methods_of_interest[0]: vals_A, methods_of_interest[1]: vals_B,} \n",
    "    df_tmp = pd.DataFrame(d)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = sns.scatterplot(data=df_tmp, x=methods_of_interest[0], y=methods_of_interest[1])\n",
    "\n",
    "\n",
    "    xlim, ylim = (plt.xlim(), plt.ylim())\n",
    "    plt.plot(xlim, ylim, '--k')\n",
    "    xlim = (xlim[0]*0.95, xlim[1]*1.05)\n",
    "    ylim = (ylim[0]*0.95, ylim[1]*1.05)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "    \n",
    "    # Title\n",
    "    prop_higher = np.sum(vals_B > vals_A) / len(vals_A)\n",
    "    cohens_d = (np.nanmean(vals_A) - np.nanmean(vals_B)) / np.mean([np.nanstd(vals_A), np.nanstd(vals_B)])\n",
    "    median_diff = np.abs(np.nanmedian(vals_A-vals_B))\n",
    "    method_name_title = method_name.replace('_', ' ').title()\n",
    "    title = f'{method_name_title} ({methods_of_interest[1]} higher in {100*prop_higher:.1f} %)\\nmedian_difference: {median_diff}\\ncohens d: {abs(cohens_d):.2f}'\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    del d, df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependence on anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib qt\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "target_column = 'target_snr'\n",
    "binning = True\n",
    "n_bins = 6\n",
    "\n",
    "\n",
    "df = sim_test.simulation_info\n",
    "if binning:\n",
    "    minimum = np.min([np.min(arr) for arr in df[target_column].values])\n",
    "    maximum = np.max([np.max(arr) for arr in df[target_column].values])\n",
    "    \n",
    "    bins = np.linspace(minimum, maximum*1.01, num=n_bins)\n",
    "    bin_labels = [str(round(bins[i], 1)) + ' - ' + str(round(bins[i+1], 1)) for i in range(len(bins)-1)]\n",
    "    new_target_column = 'bins ' + target_column\n",
    "    df[new_target_column] = np.digitize(df[target_column].values, bins=bins)\n",
    "    target_column = new_target_column\n",
    "    \n",
    "else:\n",
    "    bins = list(set(df[target_column].values))\n",
    "    bins[-1] *= 1.01\n",
    "    bin_labels = [str(bins[i]) for i in range(len(bins))]\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(list(set(df_aio.method.values))):\n",
    "    cols = df_aio[df_aio.method==model_name].iloc[:, 0:6].columns\n",
    "    values = df_aio[df_aio.method==model_name].iloc[:, 0:6].values\n",
    "\n",
    "    for metric_name, metric in zip(cols, values.T):\n",
    "        col_name = model_name.replace(' ', '_') + '_' + metric_name.replace(' ', '_')\n",
    "        df[col_name] = metric\n",
    "\n",
    "dep_var_regex = target_column\n",
    "dep_var_label = target_column.replace('_', ' ').title()\n",
    "metric_names_nice = [col.replace('_', ' ').title() for col in df_aio.iloc[:, :6].columns]\n",
    "for metric_name, metric_name_nice in zip(df_aio.iloc[:, :6].columns,  metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex='_'+metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependence on Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f1862413b14b249afd5ffde6189e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from esinet import evaluate\n",
    "\n",
    "# n_simulations = 1000\n",
    "# settings = dict(duration_of_trial=2.0,)\n",
    "# sim = Simulation(fwd, info, settings=settings).simulate(n_simulations)\n",
    "\n",
    "# times = sim.source_data[0].times\n",
    "\n",
    "# pos = util.unpack_fwd(fwd)[2]\n",
    "# argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "mles = dict()\n",
    "nmses = dict()\n",
    "mses = dict()\n",
    "aucs_combined = dict()\n",
    "aucs_close = dict()\n",
    "aucs_far = dict()\n",
    "\n",
    "points = dict()\n",
    "for i, time in tqdm(enumerate(times[:-1][::5])):\n",
    "    for model, model_name in zip(models[:1], model_names[:1]):\n",
    "        if not model_name in mles.keys():\n",
    "            mles[model_name] = []    \n",
    "            nmses[model_name] = []\n",
    "            mses[model_name] = []    \n",
    "            aucs_combined[model_name] = []\n",
    "            aucs_close[model_name] = []\n",
    "            aucs_far[model_name] = []\n",
    "            points[model_name] = []\n",
    "\n",
    "        mle = []\n",
    "        nmse = []\n",
    "        mse = []\n",
    "        auc = []\n",
    "        new_sim = deepcopy(sim).crop(tmin=time)\n",
    "        y_pred = model.predict(new_sim)\n",
    "        \n",
    "        for idx in range(n_simulations):\n",
    "            y_true = sim.source_data[idx].data[:, -1][:, np.newaxis]\n",
    "            y_est = y_pred[idx].data[:, -1][:, np.newaxis]\n",
    "            mle.append( evaluate.eval_mean_localization_error(y_true, y_est, pos, \n",
    "                argsorted_distance_matrix=argsorted_distance_matrix) )\n",
    "            nmse.append( evaluate.eval_nmse(y_true, y_est) )\n",
    "            mse.append( evaluate.eval_mse(y_true, y_est) )\n",
    "            auc.append( evaluate.eval_auc(y_true, y_est, pos, n_redraw=5, epsilon=0.25) )\n",
    "\n",
    "        auc_combined = [(a[0]+a[1])/2 for a in auc]\n",
    "        auc_close = [a[0] for a in auc]\n",
    "        auc_far = [a[1] for a in auc]\n",
    "            \n",
    "\n",
    "        \n",
    "        mles[model_name].append(np.nanmean(mle))\n",
    "        nmses[model_name].append(np.nanmean(nmse))\n",
    "        mses[model_name].append(np.nanmean(mse))\n",
    "        aucs_combined[model_name].append(np.nanmean(auc_combined))\n",
    "        aucs_close[model_name].append(np.nanmean(auc_close))\n",
    "        aucs_far[model_name].append(np.nanmean(auc_far))\n",
    "        points[model_name].append(new_sim.source_data[idx].shape[1])\n",
    "\n",
    "for model, model_name in zip(models[1:], model_names[1:]):\n",
    "    print(model_name)\n",
    "    if not model_name in mles.keys():\n",
    "        mles[model_name] = []    \n",
    "        nmses[model_name] = []\n",
    "        mses[model_name] = []    \n",
    "        aucs_combined[model_name] = []\n",
    "        aucs_close[model_name] = []\n",
    "        aucs_far[model_name] = []\n",
    "        points[model_name] = []\n",
    "\n",
    "    mle = []\n",
    "    nmse = []\n",
    "    mse = []\n",
    "    auc = []\n",
    "    new_sim = deepcopy(sim).crop(tmin=times[-2])\n",
    "    y_pred = model.predict(new_sim)\n",
    "    for idx in tqdm(range(len(y_pred))):\n",
    "        y_true = sim.source_data[idx].data[:, -1][:, np.newaxis]\n",
    "        y_est = y_pred[idx].data[:, -1][:, np.newaxis]\n",
    "        mle.append( evaluate.eval_mean_localization_error(y_true, y_est, pos, \n",
    "            argsorted_distance_matrix=argsorted_distance_matrix) )\n",
    "        nmse.append( evaluate.eval_nmse(y_true, y_est) )\n",
    "        mse.append( evaluate.eval_mse(y_true, y_est) )\n",
    "        auc.append( evaluate.eval_auc(y_true, y_est, pos, n_redraw=5, epsilon=0.25) )\n",
    "    \n",
    "    auc_combined = [(a[0]+a[1])/2 for a in auc]\n",
    "    auc_close = [a[0] for a in auc]\n",
    "    auc_far = [a[1] for a in auc]\n",
    "        \n",
    "\n",
    "    \n",
    "    mles[model_name].append(np.nanmean(mle))\n",
    "    nmses[model_name].append(np.nanmean(nmse))\n",
    "    mses[model_name].append(np.nanmean(mse))\n",
    "    aucs_combined[model_name].append(np.nanmean(auc_combined))\n",
    "    aucs_close[model_name].append(np.nanmean(auc_close))\n",
    "    aucs_far[model_name].append(np.nanmean(auc_far))\n",
    "    points[model_name].append(new_sim.source_data[idx].shape[1])\n",
    "\n",
    "params = [mles, nmses, mses, aucs_combined, aucs_close, aucs_far]\n",
    "param_names = ['mles', 'nmses', 'mses', 'aucs_combined', 'aucs_close', 'aucs_far']\n",
    "\n",
    "# with open(f'results\\\\metrics_duration.pkl', 'wb') as f:\n",
    "#     pkl.dump([params, param_names], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependence on Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_duration.pkl', 'rb') as f:\n",
    "    [params, param_names] = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Dependence On Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "sns.set(style='white', font_scale=1.6, font='helvetica')\n",
    "params = [mles, nmses, aucs_combined]\n",
    "param_names = ['mles', 'nmses', 'aucs_combined']\n",
    "param_names_nice = [\"Mean Localization Error [mm]\", \"Normalized Mean Squared Error\", \"AUC [%]\"]\n",
    "ylims = [[None, None], [None, None], [None, None]]\n",
    "x = points[\"LSTM\"]\n",
    "for param, name, name_nice, ylim in zip(params, param_names, param_names_nice, ylims):\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    for model_name in points.keys():\n",
    "        if len(param[model_name])==1:\n",
    "            plt.plot(x, param[model_name]*len(x), label=model_name)\n",
    "        else:\n",
    "            plt.plot(x, param[model_name], label=model_name)\n",
    "\n",
    "\n",
    "    plt.xlabel('Number of available data points')\n",
    "    plt.ylabel(name_nice)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(name_nice)\n",
    "    plt.legend(loc=2, bbox_to_anchor=(1.05,1), borderaxespad=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "util.multipage(r'C:\\Users\\lukas\\Sync\\lstm_inverse_problem\\figures\\results\\dependence_duration.pdf', png=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get interpolated Topomap for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.viz.topomap import (_setup_interp, _make_head_outlines, _check_sphere, \n",
    "    _check_extrapolate)\n",
    "from mne.channels.layout import _find_topomap_coords\n",
    "\n",
    "model = models[0]\n",
    "eeg, src = model._handle_data_input((sim,))\n",
    "eeg_prep = np.swapaxes(eeg[0].get_data(), 1,2)\n",
    "elec_pos = _find_topomap_coords(model.info, model.info.ch_names)\n",
    "interpolator = model.make_interpolator(elec_pos, res=model.interp_channel_shape[0])\n",
    "eeg_prep_interp = deepcopy(eeg_prep)\n",
    "for i, sample in tqdm(enumerate(eeg_prep)):\n",
    "    list_of_time_slices = []\n",
    "    for time_slice in sample:\n",
    "        time_slice_interp = interpolator.set_values(time_slice)()[::-1]\n",
    "        list_of_time_slices.append(time_slice_interp)\n",
    "\n",
    "sns.set(style='white')\n",
    "tick_font_size = 50\n",
    "plt.figure()\n",
    "plt.imshow(np.mean(list_of_time_slices, axis=0)*8e6, cmap='RdBu_r')\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.tick_params(labelsize=tick_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def new_sim_params(sr=100, packages_per_second=20):\n",
    "    package_size = int( round( sr / packages_per_second  ) )\n",
    "    package_interval = package_size/sr\n",
    "\n",
    "    n_chan = len(sim_test.eeg_data.ch_names)\n",
    "    data_package = np.random.randn(n_chan, package_size)\n",
    "\n",
    "    sim_data_package = Simulation(fwd, info, settings=dict(duration_of_trial=0.01*package_size)).simulate(1)\n",
    "    print(f'performing predictions {packages_per_second} times per second')\n",
    "\n",
    "    return sim_data_package, package_interval\n",
    "\n",
    "packages_per_second = 50\n",
    "sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    # stc = net_dense.predict(sim_data_package)\n",
    "    stc = models[0].predict(sim_data_package)\n",
    "\n",
    "    stop = time.time()\n",
    "    diff = stop-start\n",
    "    if stop-start > package_interval:\n",
    "        print(f\"took longer than expected: {diff} (instead of {package_interval})\")\n",
    "        print(f'decreasing package interval by one')\n",
    "        packages_per_second -= 1\n",
    "        sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "        print(f'packages_per_second={packages_per_second}\\n')\n",
    "        continue\n",
    "    time.sleep(package_interval-diff)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
