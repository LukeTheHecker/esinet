{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet import forward\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.2s remaining:    2.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "info = forward.get_info()\n",
    "info['sfreq'] = 100\n",
    "fwd = forward.create_forward_model(info=info)\n",
    "fwd_free = forward.create_forward_model(info=info, fixed_ori=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "duration_of_trial = (0.5, 2.0)\n",
    "settings = dict(duration_of_trial=duration_of_trial, method='standard')\n",
    "\n",
    "sim_lstm = Simulation(fwd, info, verbose=True, settings=settings).simulate(n_samples=n_samples)\n",
    "if type(duration_of_trial) == tuple:\n",
    "    sim_lstm.save(f'simulations/sim_{n_samples}_{int(duration_of_trial[0]*100)}-{int(duration_of_trial[1]*100)}points.pkl')\n",
    "else:\n",
    "    sim_lstm.save(f'simulations/sim_{n_samples}_{int(duration_of_trial*100)}points.pkl')\n",
    "# sim_lstm_prime = Simulation(fwd, info, verbose=True, settings=settings_prime).simulate(n_samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'simulations/sim_10000_50-200points.pkl', 'rb') as f:\n",
    "    sim_lstm = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & train LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from esinet.losses import combi as loss\n",
    "\n",
    "epochs = 150\n",
    "patience = 10\n",
    "dropout = 0.2\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "# Dense net\n",
    "model_params = dict(n_dense_layers=2, n_dense_units=200, \n",
    "    n_lstm_layers=0)\n",
    "train_params = dict(epochs=epochs, patience=patience, loss=loss, \n",
    "    optimizer=optimizer, return_history=True, \n",
    "    metrics=[tf.keras.losses.mean_squared_error], batch_size=8)\n",
    "# Train\n",
    "net_dense = Net(fwd, **model_params)\n",
    "_, history_dense = net_dense.fit(sim_lstm, **train_params)\n",
    "net_dense.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "LSTM v2\n",
    "model_params = dict(n_lstm_layers=2, \n",
    "    n_lstm_units=100, n_dense_layers=0, \n",
    "    model_type='v2')\n",
    "train_params = dict(epochs=epochs, patience=patience, loss=loss, \n",
    "    optimizer=optimizer, return_history=True,\n",
    "    metrics=[tf.keras.losses.mean_squared_error], batch_size=8, device='/CPU:0')\n",
    "\n",
    "# Train\n",
    "net_lstm = Net(fwd, **model_params)\n",
    "_, history_lstm = net_lstm.fit(sim_lstm, **train_params)\n",
    "net_lstm.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']\n",
    "net_dense.save(r'models', name='dense-net-50-200points-noise-cosine')\n",
    "net_lstm.save(r'models', name='lstm-net-50-200points-noise-cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dense = util.load_net('models/dense-net-50-200points-noise-cosine_0')\n",
    "net_lstm = util.load_net('models/lstm-net-50-200points-noise-cosine_0')\n",
    "\n",
    "models = [net_dense, net_lstm]\n",
    "model_names = ['Dense', 'LSTM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ability to process long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2eab0daaac4415ebcd95171e069fdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054c8e840e394c0094179b59c60f0d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c1067b7a3345478b718973dd2b1420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\simulation.py:719: RuntimeWarning: tmax is not in epochs time interval. tmax is set to epochs.tmax\n",
      "  cropped_eeg = self.eeg_data[i].crop(tmin=tmin, tmax=tmax, include_tmax=include_tmax, verbose=verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: error: 0.01233, r: 0.5603878760533244\n",
      "99.98 50\n",
      "50: error: 0.01225, r: 0.5560365303641887\n",
      "49.99 10\n",
      "10: error: 0.01226, r: 0.5430790848872763\n",
      "9.99 1\n",
      "1: error: 0.01183, r: 0.5841004663866514\n",
      "0.99 0.1\n",
      "0.1: error: 0.01253, r: 0.581792029518154\n",
      "0.09 0.01\n",
      "0.01: error: 0.01404, r: 0.28993192433172693\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "settings_eval = dict(duration_of_trial=100, method='standard', number_of_sources=10)\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(1)\n",
    "idx = 0\n",
    "durs = [100, 50, 10, 1, 0.1, 0.01]\n",
    "# durs = [np.random.uniform(0.01, 100) for _ in range(5)]\n",
    "for dur in durs:\n",
    "    sim_test.crop(tmax=dur)\n",
    "    prediction = net_lstm.predict(sim_test)\n",
    "\n",
    "    error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    title = f'{dur}: error: {error:.4}, r: {r}'\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bfcb6a800d4d459804cdbd58ebc05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac53da40ab9549ccb997d266863a352a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eac6bfacff47e79771fda2d85e780b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of eeg_prep before prediciton:  (10000, 61)\n",
      "shape of predicted sources:  (1284, 10000)\n",
      "shape of eeg_prep before prediciton:  (10000, 61)\n",
      "shape of predicted sources:  (1284, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\util\\util.py:382: RuntimeWarning: An average reference projection was already added. The data has been left untouched.\n",
      "  epochs.set_eeg_reference(projection=True, verbose=verbose).apply_baseline(baseline=(None, None), verbose=verbose)\n",
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\util\\util.py:382: RuntimeWarning: An average reference projection was already added. The data has been left untouched.\n",
      "  epochs.set_eeg_reference(projection=True, verbose=verbose).apply_baseline(baseline=(None, None), verbose=verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense, error: 0.01129, r: 0.414403778336662\n",
      "LSTM, error: 0.008243, r: 0.4631179135662733\n",
      "eLORETA, error: 0.02743, r: 0.19217337526556894\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(duration_of_trial=100, method='standard', number_of_sources=3)\n",
    "# settings_eval = dict(duration_of_trial=1, method='noise')\n",
    "\n",
    "# Simulate new data\n",
    "sim_test = Simulation(fwd, info, settings=settings_eval).simulate(1)\n",
    "snr = sim_test.simulation_info['target_snr'].values[0]\n",
    "# print(sim_test.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "# Predict sources with classical methods\n",
    "prediction_elor_data = util.wrap_mne_inverse(fwd, sim_test, snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_elor = deepcopy(predictions[0][0])\n",
    "prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "prediction_mne_data = util.wrap_mne_inverse(fwd, sim_test, method='MNE', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_mne = deepcopy(predictions[0][0])\n",
    "prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "prediction_lcmv_data = util.wrap_mne_inverse(fwd_free, sim_test, method='beamformer', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_lcmv = deepcopy(predictions[0][0])\n",
    "prediction_lcmv.data = prediction_lcmv_data / np.abs(np.max(prediction_lcmv_data))\n",
    "# Get predictions and names in order\n",
    "predictions.append([prediction_elor])\n",
    "predictions.append([prediction_mne])\n",
    "predictions.append([prediction_lcmv])\n",
    "model_names.append('eLORETA')\n",
    "model_names.append('MNE')\n",
    "model_names.append('Beamformer')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim_test.simulation_info.number_of_sources.values[0]} sources, snr={snr:.1f}', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "# evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "model_selection = ['lstm', 'dense', 'eloreta']\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    \n",
    "    if not any([model_name.lower() in model_select.lower() for model_select in model_selection]):\n",
    "        continue\n",
    "    # try:\n",
    "    error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    \n",
    "    brain = prediction[idx].plot(**plot_params)\n",
    "\n",
    "    title = f'{model_name}, error: {error:.4}, r: {r}'\n",
    "    print(title)\n",
    "    brain.add_text(0.1, 0.9, title, 'title')\n",
    "    # Plot predicted EEG\n",
    "    evoked_esi = util.get_eeg_from_source(prediction[idx], fwd, info, tmin=0.)\n",
    "    # evoked_esi.plot()\n",
    "    evoked_esi.plot_topomap(title=model_name)\n",
    "    # except:\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\util\\util.py:382: RuntimeWarning: An average reference projection was already added. The data has been left untouched.\n",
      "  epochs.set_eeg_reference(projection=True, verbose=verbose).apply_baseline(baseline=(None, None), verbose=verbose)\n",
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\util\\util.py:382: RuntimeWarning: An average reference projection was already added. The data has been left untouched.\n",
      "  epochs.set_eeg_reference(projection=True, verbose=verbose).apply_baseline(baseline=(None, None), verbose=verbose)\n",
      "c:\\Users\\lukas\\Dokumente\\projects\\esinet\\evaluate\\..\\esinet\\util\\util.py:382: RuntimeWarning: An average reference projection was already added. The data has been left untouched.\n",
      "  epochs.set_eeg_reference(projection=True, verbose=verbose).apply_baseline(baseline=(None, None), verbose=verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense, error: 0.0117, r: 0.34472718417932785\n",
      "LSTM, error: 0.007316, r: 0.3944348183079149\n",
      "eLORETA, error: 0.02557, r: 0.16853603244869317\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0, \n",
    "    clim=dict(kind='percent', pos_lims=[20, 30, 100]))\n",
    "\n",
    "settings_eval = dict(duration_of_trial=100, method='standard', number_of_sources=3)\n",
    "# settings_eval = dict(duration_of_trial=1, method='noise')\n",
    "\n",
    "# Simulate new data\n",
    "sim_test.crop(tmin=0, tmax=5)\n",
    "snr = sim_test.simulation_info['target_snr'].values[0]\n",
    "# print(sim_test.simulation_info)\n",
    "idx = 0\n",
    "# Predict sources using the esinet models\n",
    "predictions = [model.predict(sim_test) for model in models]\n",
    "# Predict sources with classical methods\n",
    "prediction_elor_data = util.wrap_mne_inverse(fwd, sim_test, snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_elor = deepcopy(predictions[0][0])\n",
    "prediction_elor.data = prediction_elor_data / np.abs(np.max(prediction_elor_data))\n",
    "prediction_mne_data = util.wrap_mne_inverse(fwd, sim_test, method='MNE', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_mne = deepcopy(predictions[0][0])\n",
    "prediction_mne.data = prediction_mne_data / np.abs(np.max(prediction_mne_data))\n",
    "prediction_lcmv_data = util.wrap_mne_inverse(fwd_free, sim_test, method='beamformer', snr=snr)[idx].data.astype(np.float32)\n",
    "prediction_lcmv = deepcopy(predictions[0][0])\n",
    "prediction_lcmv.data = prediction_lcmv_data / np.abs(np.max(prediction_lcmv_data))\n",
    "# Get predictions and names in order\n",
    "predictions.append([prediction_elor])\n",
    "predictions.append([prediction_mne])\n",
    "predictions.append([prediction_lcmv])\n",
    "model_names.append('eLORETA')\n",
    "model_names.append('MNE')\n",
    "model_names.append('Beamformer')\n",
    "\n",
    "# Plot True Source\n",
    "brain = sim_test.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, f'Ground Truth {sim_test.simulation_info.number_of_sources.values[0]} sources, snr={snr:.1f}', 'title')\n",
    "# Plot True EEG\n",
    "evoked = sim_test.eeg_data[idx].average()\n",
    "# evoked.plot()\n",
    "evoked.plot_topomap(title='Ground Truth')\n",
    "evoked = util.get_eeg_from_source(sim_test.source_data[idx], fwd, info, tmin=0.)\n",
    "evoked.plot_topomap(title='Ground Truth Noiseless')\n",
    "\n",
    "model_selection = ['lstm', 'dense', 'eloreta']\n",
    "# Plot predicted sources\n",
    "for model_name, prediction in zip(model_names, predictions):\n",
    "    \n",
    "    if not any([model_name.lower() in model_select.lower() for model_select in model_selection]):\n",
    "        continue\n",
    "    # try:\n",
    "    error = util.batch_nmse(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    r = util.batch_corr(sim_test.source_data[idx].data, prediction[idx].data)\n",
    "    \n",
    "    brain = prediction[idx].plot(**plot_params)\n",
    "\n",
    "    title = f'{model_name}, error: {error:.4}, r: {r}'\n",
    "    print(title)\n",
    "    brain.add_text(0.1, 0.9, title, 'title')\n",
    "    # Plot predicted EEG\n",
    "    evoked_esi = util.get_eeg_from_source(prediction[idx], fwd, info, tmin=0.)\n",
    "    # evoked_esi.plot()\n",
    "    evoked_esi.plot_topomap(title=model_name)\n",
    "    # except:\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Evaluation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or Load Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "duration_of_trial = 10\n",
    "# settings = dict(duration_of_trial=duration_of_trial, method='standard')\n",
    "# sim_lstm_test = Simulation(fwd, info, verbose=False, settings=settings).simulate(n_samples=n_samples)\n",
    "# sim_lstm_test.save(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_standard.pkl')\n",
    "\n",
    "# or Load\n",
    "with open(f'simulations\\\\sim_test_{n_samples}_{int(duration_of_trial*100)}points_standard.pkl', 'rb') as f:\n",
    "    sim_lstm_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_mean_localization_error, eval_nmse, eval_auc, eval_mse\n",
    "from esinet.util import wrap_mne_inverse\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "model_names = ['Dense', 'LSTM', 'LSTM large']\n",
    "# Predict\n",
    "print('predict esinets...')\n",
    "predictions = [model.predict(sim_lstm_test) for model in models]\n",
    "\n",
    "print('predict elor')\n",
    "pred_elor = wrap_mne_inverse(fwd, sim_lstm_test, method='eLORETA')\n",
    "model_names.append('eLORETA')\n",
    "predictions.append(pred_elor)\n",
    "\n",
    "print('predict MNE')\n",
    "pred_mne = wrap_mne_inverse(fwd, sim_lstm_test, method='MNE')\n",
    "model_names.append('MNE')\n",
    "predictions.append(pred_mne)\n",
    "\n",
    "print('predict LCMV')\n",
    "pred_lcmv = wrap_mne_inverse(fwd_free, sim_lstm_test, method='beamformer')\n",
    "model_names.append('LCMV')\n",
    "predictions.append(pred_lcmv)\n",
    "\n",
    "pos = util.unpack_fwd(fwd)[2]\n",
    "argsorted_distance_matrix = np.argsort(cdist(pos, pos), axis=-1)\n",
    "\n",
    "mean_localization_errors = []\n",
    "aucs = []\n",
    "nmses = []\n",
    "mses = []\n",
    "true_sources = np.stack([src.data for src in sim_lstm_test.source_data], axis=0)\n",
    "true_sources = util.collapse(true_sources)\n",
    "\n",
    "size = 500\n",
    "n_total = true_sources.shape[0]\n",
    "choice = np.random.choice(np.arange(n_total), size=size, replace=False)\n",
    "# true_sources = true_sources[choice]\n",
    "for prediction in tqdm(predictions):\n",
    "\n",
    "    predicted_sources = util.collapse(np.stack([src.data for src in prediction], axis=0))\n",
    "    print(predicted_sources.shape)\n",
    "    # predicted_sources = predicted_sources[choice]\n",
    "    print(predicted_sources.shape)\n",
    "    print('mle calculation....')\n",
    "    mean_localization_error = [eval_mean_localization_error(true_source, predicted_source, pos, argsorted_distance_matrix=argsorted_distance_matrix) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    auc = [eval_auc(true_source, predicted_source, pos, epsilon=0.25, n_redraw=25) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    nmse = [eval_nmse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    mse = [eval_mse(true_source, predicted_source) for true_source, predicted_source in tqdm(zip(true_sources, predicted_sources))]\n",
    "    \n",
    "    mean_localization_errors.append(mean_localization_error)\n",
    "    aucs.append(auc)\n",
    "    nmses.append(nmse)\n",
    "    mses.append(mse)\n",
    "\n",
    "aucs_far = [auc[:, 1] for auc in np.array(aucs)]\n",
    "aucs_close = [auc[:, 0] for auc in np.array(aucs)]\n",
    "\n",
    "metrics = dict(\n",
    "    mean_localization_errors=mean_localization_errors,\n",
    "    aucs_far=aucs_far,\n",
    "    aucs_close=aucs_close,\n",
    "    nmses=nmses,\n",
    "    mses=mses\n",
    ")\n",
    "\n",
    "with open(f'results\\\\metrics_{size}_{sim_lstm_test.source_data[0].data.shape[1]}points_standard.pkl', 'wb') as f:\n",
    "    pkl.dump([metrics, choice], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results\\\\metrics_5000_100points_standard.pkl', 'rb') as f:\n",
    "    [metrics, choice] = pkl.load(f)\n",
    "\n",
    "# with open(f'simulations\\\\sim_test_5000_10points.pkl', 'rb') as f:\n",
    "#     sim_lstm_test = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Dense', 'LSTM', 'LSTM large', 'eLORETA', 'MNE', 'Beamformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib qt\n",
    "xticks = dict(ticks=np.arange(len(model_names)), labels=model_names)\n",
    "plot = sns.boxplot  # violinplot\n",
    "variable_keys = metrics.keys()\n",
    "names = [ 'Mean Localization Errors',  'Far area under the curve', 'Close area under the curve', 'Normalized Mean Squared Errors']\n",
    "plt.figure(figsize=(14, 8))\n",
    "    \n",
    "subplot_nums = np.arange(221, 226)\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    plt.subplot(num)\n",
    "    plot(data=metrics[variable_key])\n",
    "    plt.title(name)\n",
    "    plt.xticks(**xticks)\n",
    "plt.tight_layout(pad=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='whitegrid', font_scale=1.2, font='helvetica')\n",
    "%matplotlib inline\n",
    "l = [10, 100, 1000]\n",
    "for n in l:\n",
    "    with open(f'results\\\\metrics_500_{n}points_standard.pkl', 'rb') as f:\n",
    "        [metrics, choice] = pkl.load(f)\n",
    "\n",
    "    df = pd.DataFrame(metrics, index=model_names)\n",
    "    df['model_names'] = df.index\n",
    "    df.index = np.arange(df.shape[0])\n",
    "\n",
    "    df = df.melt(id_vars=[\"model_names\"])\n",
    "    df[\"value\"] = [np.nanmedian(vals) for vals in df[\"value\"].values]\n",
    "    \n",
    "    \n",
    "    xticks = dict(ticks=np.arange(len(model_names)), labels=model_names)\n",
    "    plot = sns.boxplot  # violinplot\n",
    "    variable_keys = metrics.keys()\n",
    "    names = [ 'Mean Localization Errors',  'Far area under the curve', 'Close area under the curve', 'Normalized Mean Squared Errors']\n",
    "    a = plt.figure(figsize=(14, 8))\n",
    "    subplot_nums = np.arange(221, 226)\n",
    "    for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "        plt.subplot(num)\n",
    "        plot(data=metrics[variable_key])\n",
    "        plt.title(name)\n",
    "        plt.xticks(**xticks)\n",
    "    plt.suptitle(str(n) + \" timepoints\")\n",
    "    plt.tight_layout(pad=2)\n",
    "    \n",
    "    plt.show()\n",
    "    print(df[df[\"model_names\"]==\"LSTM large\"])\n",
    "    print(df[df[\"model_names\"]==\"Dense\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "subplot_nums = np.arange(221, 226)\n",
    "inverse_idc = [4,2]\n",
    "for variable_key, name, num in zip(variable_keys, names, subplot_nums):\n",
    "    var = np.array(metrics[variable_key])[inverse_idc]\n",
    "    var = var[:, ~np.isnan(var).any(axis=0)]\n",
    "    if np.any(np.isnan(var)):\n",
    "        print(\"still nans\")\n",
    "\n",
    "    plt.subplot(num)\n",
    "    plt.scatter(var[0], var[1], edgecolors='k', s=2)\n",
    "    # origin\n",
    "    lo_lim = np.nanmin(var)\n",
    "    hi_lim = np.nanmax(var)\n",
    "    plt.plot([lo_lim, hi_lim], [lo_lim, hi_lim], '--r')\n",
    "\n",
    "    prop_higher = np.sum(var[1] > var[0]) / len(var[0])\n",
    "    cohens_d = (np.nanmean(var[0]) - np.nanmean(var[1])) / np.mean([np.nanstd(var[0]), np.nanstd(var[1])])\n",
    "    median_diff = np.abs(np.nanmedian(var[0]-var[1]))\n",
    "    title = f'{name} (higher in {100*prop_higher:.1f} %)\\nmedian_difference: {median_diff}\\ncohens d: {abs(cohens_d):.2f}'\n",
    "    plt.title(title)\n",
    "    plt.xlabel(model_names[inverse_idc[0]])\n",
    "    plt.ylabel(model_names[inverse_idc[1]])\n",
    "    \n",
    "    # pad\n",
    "    lo_lim -= abs(hi_lim-lo_lim)*0.05\n",
    "    hi_lim += abs(hi_lim-lo_lim)*0.05\n",
    "    \n",
    "    # plot properties\n",
    "    plt.xlim(lo_lim, hi_lim)\n",
    "    plt.ylim(lo_lim, hi_lim)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "# snr_bins = (0.5, 5, 10)\n",
    "\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "# df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "# df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "# df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex='snr_'), df.filter(regex=metric_name)), axis=1).melt('target_snr_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='target_snr_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=snr_bin_labels, ylabel=metric_name_nice, xlabel='Signal to Noise Ratio (SNR)')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), int(duration_of_trial*100))\n",
    "\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['avg_eccentricity'] = [np.median((df.positions.values[i]**2).sum(axis=1)**(1/2)) for i in range(df.shape[0])]\n",
    "ecc_bins = np.linspace(int(df['avg_eccentricity'].values.min()), np.ceil(df['avg_eccentricity'].values.max()).astype(int), num=5)\n",
    "ecc_bin_labels = [str(ecc_bins[i]) + ' - ' + str(ecc_bins[i+1]) for i in range(len(ecc_bins)-1)]\n",
    "df['avg_eccentricity_bins'] = np.digitize(df['avg_eccentricity'].values, bins=ecc_bins)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'avg_eccentricity_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Average eccentricity'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=ecc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on No. of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), int(duration_of_trial*100))\n",
    "\n",
    "# nsrc_bins = (1, 2, 4, 8, 16, 20)\n",
    "nsrc_bins = (1, 2, 4, 8, 10)\n",
    "\n",
    "nsrc_bin_labels = [str(nsrc_bins[i]) + ' - ' + str(nsrc_bins[i+1]) for i in range(len(nsrc_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['number_of_sources_bins'] = np.digitize(df['number_of_sources'].values, bins=nsrc_bins)\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "dep_var_regex = 'number_of_sources_bins'  # 'target_snr_bins'\n",
    "dep_var_label = 'Number of True Sources'  # 'Signal to Noise Ratio (SNR)'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=dep_var_regex), df.filter(regex=metric_name)), axis=1).melt(dep_var_regex, var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x=dep_var_regex, y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=nsrc_bin_labels, ylabel=metric_name_nice, xlabel=dep_var_label)\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "snr_bins = (0.5, 2, 4, 6, 8, 10)\n",
    "snr_bin_labels = [str(snr_bins[i]) + ' - ' + str(snr_bins[i+1]) for i in range(len(snr_bins)-1)]\n",
    "\n",
    "beta_bins = (0.5, 0.75, 1, 1.25, 1.5)\n",
    "beta_bin_labels = [str(beta_bins[i]) + ' - ' + str(beta_bins[i+1]) for i in range(len(beta_bins)-1)]\n",
    "\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "df['target_snr_bins'] = np.digitize(df.target_snr, bins=snr_bins)\n",
    "\n",
    "df['avg_beta'] = [np.mean(df.betas.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_beta_bins']  = np.digitize(df.avg_beta, bins=beta_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'beta_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_beta_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_beta_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=beta_bin_labels, ylabel=metric_name_nice, xlabel='Beta Exponent')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sns.set(font_scale=1.2, font='helvetica')\n",
    "# pd.DataFrame( metrics , index=model_names)\n",
    "stretched_indices = np.repeat(np.arange(sim_lstm_test.n_samples), sim_lstm_test.source_data[0].data.shape[1])\n",
    "\n",
    "extent_bins = np.linspace(20, 40, 5)\n",
    "extent_bin_labels = [str(int(round(extent_bins[i]))) + ' - ' + str(int(round(extent_bins[i+1]))) for i in range(len(extent_bins)-1)]\n",
    "\n",
    "params = list(sim_lstm_test.simulation_info.columns)\n",
    "series = [sim_lstm_test.simulation_info.iloc[stretched_indices[choice]][param] for param in params]\n",
    "df = pd.concat(series, axis=1)\n",
    "\n",
    "df['avg_extent'] = [np.mean(df.extents.values[i]) for i in range(df.shape[0])]\n",
    "df['avg_extent_bins']  = np.digitize(df.avg_extent, bins=extent_bins)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        col_name = model_name + '_' + metric_name\n",
    "\n",
    "        df[col_name] = metric[i]\n",
    "\n",
    "print(df.columns)\n",
    "regex = 'extent_'\n",
    "metric_names_nice = ['Mean Localization Error [mm]', 'AUC Far [%]', 'AUC Close [%]', 'Normalized Mean Squared Error', 'Mean Squared Error']\n",
    "for metric_name, metric_name_nice in zip(metrics.keys(), metric_names_nice):\n",
    "    df_temp = pd.concat((df.filter(regex=regex), df.filter(regex=metric_name)), axis=1).melt('avg_extent_bins', var_name='cols', value_name='vals')\n",
    "    g = sns.catplot(x='avg_extent_bins', y='vals', hue='cols', capsize=.2, kind='point', data=df_temp)\n",
    "    g.set(xticklabels=extent_bin_labels, ylabel=metric_name_nice, xlabel='Source extent [mm]')\n",
    "    g._legend.remove()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def multipage(filename, figs=None, dpi=300, png=False):\n",
    "    ''' Saves all open (or list of) figures to filename.pdf with dpi''' \n",
    "    pp = PdfPages(filename)\n",
    "    path = os.path.dirname(filename)\n",
    "    fn = os.path.basename(filename)[:-4]\n",
    "\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for i, fig in enumerate(figs):\n",
    "        print(f'saving fig {fig}\\n')\n",
    "        fig.savefig(pp, format='pdf', dpi=dpi)\n",
    "        if png:\n",
    "            fig.savefig(f'{path}\\\\{i}_{fn}.png', dpi=600)\n",
    "    pp.close()\n",
    "\n",
    "multipage(f'figures\\\\figs_5000_10samples\\\\figs_5000_10samples.pdf', png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib qt\n",
    "model_names[0] = 'Dense'\n",
    "\n",
    "data = {model_name: nmse for model_name, nmse in zip(model_names, nmses)}\n",
    "plt.figure()\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Normalized Mean Squared Errors')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mse for model_name, mse in zip(model_names, mses)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Mean Squared Errors')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_far)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Far area under the curve')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: auc for model_name, auc in zip(model_names, aucs_close)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "\n",
    "plt.figure()\n",
    "data = {model_name: mle for model_name, mle in zip(model_names, mean_localization_errors)}\n",
    "sns.kdeplot(data=data, multiple='stack')\n",
    "plt.title('Close area under the curve')\n",
    "plt.title('Mean Localization Errors')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance per Difficulty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [nmses, mean_localization_errors, aucs_far, aucs_close]\n",
    "metric_names = ['nmses', 'mean_localization_errors', 'aucs_far', 'aucs_close']\n",
    "\n",
    "covariates = ['target_snr', 'number_of_sources']\n",
    "\n",
    "n_samples = int(np.array(nmses).shape[1] / 20)\n",
    "for covariate in covariates:\n",
    "    sim_params = sim_lstm_test.simulation_info[covariate].values[:n_samples]\n",
    "    sim_params = np.repeat(sim_params, 20)\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        plt.figure()\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            plt.scatter(sim_params, metric[i], label=model_name, s=.7)\n",
    "        plt.title(f'{covariate}, {metric_name}')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def new_sim_params(sr=100, packages_per_second=20):\n",
    "    package_size = int( round( sr / packages_per_second  ) )\n",
    "    package_interval = package_size/sr\n",
    "\n",
    "    n_chan = len(sim_lstm_test.eeg_data.ch_names)\n",
    "    data_package = np.random.randn(n_chan, package_size)\n",
    "\n",
    "    sim_data_package = Simulation(fwd, info, settings=dict(duration_of_trial=0.01*package_size)).simulate(1)\n",
    "    print(f'performing predictions {packages_per_second} times per second')\n",
    "\n",
    "    return sim_data_package, package_interval\n",
    "\n",
    "packages_per_second = 50\n",
    "sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    # stc = net_dense.predict(sim_data_package)\n",
    "    stc = net_lstm.predict(sim_data_package)\n",
    "\n",
    "    stop = time.time()\n",
    "    diff = stop-start\n",
    "    if stop-start > package_interval:\n",
    "        print(f\"took longer than expected: {diff} (instead of {package_interval})\")\n",
    "        print(f'decreasing package interval by one')\n",
    "        packages_per_second -= 1\n",
    "        sim_data_package, package_interval = new_sim_params(packages_per_second=packages_per_second)\n",
    "        print(f'packages_per_second={packages_per_second}\\n')\n",
    "        continue\n",
    "    time.sleep(package_interval-diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
