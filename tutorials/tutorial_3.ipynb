{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 3: How simulations define your predictions\r\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\r\n",
    "\r\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "\r\n",
    "import mne\r\n",
    "import numpy as np\r\n",
    "from copy import deepcopy\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import sys; sys.path.insert(0, '../')\r\n",
    "from esinet import util\r\n",
    "from esinet import Simulation\r\n",
    "from esinet import Net\r\n",
    "from esinet.forward import create_forward_model, get_info\r\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Forward model\r\n",
    "First we create a template forward model which comes with the esinet package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "info = get_info()\r\n",
    "fwd = create_forward_model(info=info)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.2s remaining:    2.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulate\r\n",
    "Next, we simulate two types of data: \r\n",
    "1. Data containing small sources with 15-25 mm in diameter.\r\n",
    "2. Data containing large sources with 35-45 mm in diameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "n_samples = 10000\r\n",
    "settings_small = dict(n_sources=(1, 10), extents=(15, 25))\r\n",
    "settings_large = dict(n_sources=(1, 10), extents=(35, 45))\r\n",
    "\r\n",
    "sim_small = Simulation(fwd, info, settings=settings_small).simulate(n_samples=n_samples)\r\n",
    "sim_large = Simulation(fwd, info, settings=settings_large).simulate(n_samples=n_samples)\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b49048b0c1144793897fba826a8be8ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "677aed2ef7374c23b021aee97b9d3d53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be3619b89f44892a08d6b7e59bc4e12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "525a31db6e414b7293f388621f077c74"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lets visualize the two types of simulations\r\n",
    "The two brain plots should now look quite different, as one contains large and extended sources while the other contains tiny point-like sources."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "brain = sim_small.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Small sources', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "brain = sim_large.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Large sources', 'title',\r\n",
    "               font_size=14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train individual neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "net_small = Net(fwd, verbose=True).fit(sim_small)\r\n",
    "net_large = Net(fwd, verbose=True).fit(sim_large)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               7936      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1284)              165636    \n",
      "=================================================================\n",
      "Total params: 173,572\n",
      "Trainable params: 173,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "71/71 [==============================] - 1s 6ms/step - loss: 0.0018 - mean_squared_error: 0.0038 - val_loss: 0.0016 - val_mean_squared_error: 0.0037\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0016 - mean_squared_error: 0.0037 - val_loss: 0.0016 - val_mean_squared_error: 0.0037\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0016 - mean_squared_error: 0.0036 - val_loss: 0.0016 - val_mean_squared_error: 0.0036\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0016 - mean_squared_error: 0.0036 - val_loss: 0.0016 - val_mean_squared_error: 0.0036\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0036 - val_loss: 0.0016 - val_mean_squared_error: 0.0036\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0016 - val_mean_squared_error: 0.0036\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0036\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0034 - val_loss: 0.0015 - val_mean_squared_error: 0.0035\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0033 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0034\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0016 - val_mean_squared_error: 0.0034\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0016 - val_mean_squared_error: 0.0034\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0016 - val_mean_squared_error: 0.0034\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0016 - val_mean_squared_error: 0.0034\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0013 - mean_squared_error: 0.0030 - val_loss: 0.0016 - val_mean_squared_error: 0.0034\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00048: early stopping\n",
      "Model: \"net_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 128)               7936      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1284)              165636    \n",
      "=================================================================\n",
      "Total params: 173,572\n",
      "Trainable params: 173,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "71/71 [==============================] - 1s 6ms/step - loss: 0.0078 - mean_squared_error: 0.0174 - val_loss: 0.0072 - val_mean_squared_error: 0.0158\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0070 - mean_squared_error: 0.0153 - val_loss: 0.0067 - val_mean_squared_error: 0.0145\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0067 - mean_squared_error: 0.0142 - val_loss: 0.0065 - val_mean_squared_error: 0.0137\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0064 - mean_squared_error: 0.0135 - val_loss: 0.0063 - val_mean_squared_error: 0.0131\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0062 - mean_squared_error: 0.0130 - val_loss: 0.0061 - val_mean_squared_error: 0.0128\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0061 - mean_squared_error: 0.0127 - val_loss: 0.0060 - val_mean_squared_error: 0.0125\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0060 - mean_squared_error: 0.0124 - val_loss: 0.0059 - val_mean_squared_error: 0.0123\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0059 - mean_squared_error: 0.0122 - val_loss: 0.0059 - val_mean_squared_error: 0.0122\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0058 - mean_squared_error: 0.0120 - val_loss: 0.0058 - val_mean_squared_error: 0.0121\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0057 - mean_squared_error: 0.0119 - val_loss: 0.0058 - val_mean_squared_error: 0.0120\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0057 - mean_squared_error: 0.0117 - val_loss: 0.0058 - val_mean_squared_error: 0.0119\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0056 - mean_squared_error: 0.0116 - val_loss: 0.0057 - val_mean_squared_error: 0.0118\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0056 - mean_squared_error: 0.0115 - val_loss: 0.0057 - val_mean_squared_error: 0.0118\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0055 - mean_squared_error: 0.0114 - val_loss: 0.0057 - val_mean_squared_error: 0.0117\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0055 - mean_squared_error: 0.0114 - val_loss: 0.0057 - val_mean_squared_error: 0.0117\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0055 - mean_squared_error: 0.0113 - val_loss: 0.0057 - val_mean_squared_error: 0.0116\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0054 - mean_squared_error: 0.0112 - val_loss: 0.0057 - val_mean_squared_error: 0.0116\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0054 - mean_squared_error: 0.0112 - val_loss: 0.0056 - val_mean_squared_error: 0.0116\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0054 - mean_squared_error: 0.0111 - val_loss: 0.0056 - val_mean_squared_error: 0.0116\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0053 - mean_squared_error: 0.0111 - val_loss: 0.0056 - val_mean_squared_error: 0.0115\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0053 - mean_squared_error: 0.0110 - val_loss: 0.0056 - val_mean_squared_error: 0.0115\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 0.0053 - mean_squared_error: 0.0110 - val_loss: 0.0056 - val_mean_squared_error: 0.0115\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0053 - mean_squared_error: 0.0109 - val_loss: 0.0056 - val_mean_squared_error: 0.0115\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0053 - mean_squared_error: 0.0109 - val_loss: 0.0056 - val_mean_squared_error: 0.0115\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0052 - mean_squared_error: 0.0108 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0052 - mean_squared_error: 0.0108 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0052 - mean_squared_error: 0.0108 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0052 - mean_squared_error: 0.0107 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0052 - mean_squared_error: 0.0107 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0106 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0106 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0106 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0106 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0105 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0105 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0105 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0104 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0104 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0104 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0103 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0103 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0103 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0103 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0050 - mean_squared_error: 0.0103 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0102 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0102 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0102 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0102 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 49/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0102 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 50/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0101 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 51/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0101 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0101 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0101 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 54/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0049 - mean_squared_error: 0.0101 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 56/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 57/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0113\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 59/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 60/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0100 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 61/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0099 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.0048 - mean_squared_error: 0.0099 - val_loss: 0.0056 - val_mean_squared_error: 0.0114\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00062: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have simulated two different types of source & eeg data and build two neural networks that each was trained on one of these simulations. Lets see how they perform within their own simulation type."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Simulate some new, unseen test data    \r\n",
    "n_test_samples = 1\r\n",
    "sim_test_small = Simulation(fwd, info, settings=settings_small).simulate(n_samples=n_test_samples)\r\n",
    "sim_test_large = Simulation(fwd, info, settings=settings_large).simulate(n_samples=n_test_samples)\r\n",
    "\r\n",
    "\r\n",
    "brain = sim_test_small.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of small data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "brain = net_small.predict(sim_test_small).plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Small-Net on small data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "brain = sim_test_large.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of large data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "brain = net_large.predict(sim_test_large).plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Large-Net on large data', 'title',\r\n",
    "               font_size=14)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7589f2d929c24aa98ff1e6c664c18b2e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "200945d823a345e0b6af5223c39066ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ef5460ab55e457381238216764c551d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b4d9e37b954478b3ea485f3e7a5cc3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 61, 1)\n",
      "(1, 61, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will use the large-net to predict the small simulation and vice versa."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "brain = sim_test_small.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of small data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "brain = net_large.predict(sim_test_small).plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Large-Net on small data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "brain = sim_test_large.source_data.plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of large data', 'title',\r\n",
    "               font_size=14)\r\n",
    "\r\n",
    "\r\n",
    "brain = net_small.predict(sim_test_large).plot(**plot_params)\r\n",
    "brain.add_text(0.1, 0.9, 'Small-Net on large data', 'title',\r\n",
    "               font_size=14)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 61, 1)\n",
      "(1, 61, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now find that the Net which was trained on large simulations always tends to find large sources - even when confronted with data in which small sources were active. \r\n",
    "\r\n",
    "Conversely, the Net which was trained on simulations that contain small sources finds sparse sources when confronted with data containing large-source activity.\r\n",
    "\r\n",
    "This demonstrates that our simulation settings function like priors. Further, it emphasizes the importance to state your priors and to motivate your choice.\r\n",
    "\r\n",
    "In many cases we can't make a choice and we want to make as few assumptions into our models as possible. In this case we propose that you use large ranges in your settings to maximize the diversity of your training data.\r\n",
    "\r\n",
    "A sample of a diverse setting is given in the next cell:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "settings = {\r\n",
    "    'number_of_sources': (1, 20),  # The range of simultaneously active sources.\r\n",
    "    'extents': (2, 50),  # The range of source diameters in mm \r\n",
    "    'amplitudes': (1, 100),  # Defines the range of amplitudes (in arbitrary units)\r\n",
    "    'shapes': 'both',  # Gives you both gaussian-shaped and flat sources\r\n",
    "    'beta': (0, 1.5),  # Defines the distribution of the noise in terms of 1/f**beta\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('esienv': virtualenv)"
  },
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}