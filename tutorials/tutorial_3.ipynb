{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate\n",
    "Next, we simulate two types of data: \n",
    "1. Data containing small sources with 15-25 mm in diameter.\n",
    "2. Data containing large sources with 35-45 mm in diameter.\n",
    "\n",
    "Note, that for publication-ready inverse solutions you should increase the number of training samples to 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:01<00:00, 162.58it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6025.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 100) (1284, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:34<00:00, 64.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:35<00:00, 104.87it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1837.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 100) (1284, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:53<00:00, 42.88it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "settings_small = dict(number_of_sources=(1, 10), extents=(15, 25))\n",
    "settings_large = dict(number_of_sources=(1, 10), extents=(35, 45))\n",
    "\n",
    "sim_small = Simulation(fwd, info, settings=settings_small).simulate(n_samples=n_samples)\n",
    "sim_large = Simulation(fwd, info, settings=settings_large).simulate(n_samples=n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets visualize the two types of simulations\n",
    "The two brain plots should now look quite different, as one contains large and extended sources while the other contains tiny point-like sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "brain = sim_small.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Small sources', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "brain = sim_large.source_data[idx].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Large sources', 'title',\n",
    "               font_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train individual neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess data\n",
      "werks3\n",
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, None, 200)    12400       ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 200)    0           ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, None, 64)     44928       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, None, 1284)   258084      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 1284)   83460       ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 1284)   0           ['FC2[0][0]',                    \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 398,872\n",
      "Trainable params: 398,872\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "fit model\n",
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 66s 19ms/step - loss: -0.1676 - mae: 0.0492 - val_loss: -0.2174 - val_mae: 0.0478\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "model_type = 'LSTM'  # can be 'LSTM' or 'ConvDip', too\n",
    "net_small = Net(fwd, verbose=True, model_type=model_type).fit(sim_small, epochs=10)\n",
    "net_large = Net(fwd, verbose=True, model_type=model_type).fit(sim_large, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have simulated two different types of source & eeg data and build two neural networks that each was trained on one of these simulations. Lets see how they perform within their own simulation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some new, unseen test data    \n",
    "n_test_samples = 2\n",
    "sim_test_small = Simulation(fwd, info, settings=settings_small).simulate(n_samples=n_test_samples)\n",
    "sim_test_large = Simulation(fwd, info, settings=settings_large).simulate(n_samples=n_test_samples)\n",
    "\n",
    "\n",
    "brain = sim_test_small.source_data[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of small data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "brain = net_small.predict(sim_test_small)[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Small-Net on small data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "\n",
    "brain = sim_test_large.source_data[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of large data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "brain = net_large.predict(sim_test_large)[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Large-Net on large data', 'title',\n",
    "               font_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the large-net to predict the small simulation and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = sim_test_small.source_data[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of small data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "brain = net_large.predict(sim_test_small)[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Large-Net on small data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "\n",
    "brain = sim_test_large.source_data[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth of large data', 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "brain = net_small.predict(sim_test_large)[0].plot(**plot_params)\n",
    "brain.add_text(0.1, 0.9, 'Small-Net on large data', 'title',\n",
    "               font_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find that the Net which was trained on large simulations always tends to find large sources - even when confronted with data in which small sources were active. \n",
    "\n",
    "Conversely, the Net which was trained on simulations that contain small sources finds sparse sources when confronted with data containing large-source activity.\n",
    "\n",
    "This demonstrates that our simulation settings function like priors. Further, it emphasizes the importance to state your priors and to motivate your choice.\n",
    "\n",
    "In many cases we can't make a choice and we want to make as few assumptions into our models as possible. In this case we propose that you use large ranges in your settings to maximize the diversity of your training data.\n",
    "\n",
    "A sample of a diverse setting is given in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'number_of_sources': (1, 20),  # The range of simultaneously active sources.\n",
    "    'extents': (1, 50),  # The range of source diameters in mm \n",
    "    'amplitudes': (1, 100),  # Defines the range of amplitudes (in arbitrary units)\n",
    "    'shapes': 'both',  # Simulate both gaussian-shaped and flat sources\n",
    "    'beta': (0, 3),  # Defines the distribution of the noise in terms of 1/f**beta\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
