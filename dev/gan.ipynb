{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.7s remaining:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.8s remaining:    1.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))\n",
    "# laplace_operator = laplace_operator @ laplace_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source from g:  KerasTensor(type_spec=TensorSpec(shape=(None, 1284), dtype=tf.float32, name=None), name='Generator/activation_639/Tanh:0', description=\"created by layer 'Generator'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1284])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, Activation, Dropout, ActivityRegularization, TimeDistributed, Reshape, Permute, GaussianNoise\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def weightedLoss(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"DISC\", true, pred)\n",
    "        \n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error / tf.linalg.norm(true, axis=-1)\n",
    "        return error\n",
    "    return loss\n",
    "\n",
    "def weightedLossGan(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"GAN\", true, pred)\n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error * tf.linalg.norm(true)\n",
    "        return -error\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_gan_loss(y_ones, y_both):\n",
    "    y_true = y_both[0]\n",
    "    y_pred = y_both[1]\n",
    "    # print(y_ones, y_true, y_pred)\n",
    "    \n",
    "    error = -tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    # error = -tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    # blur =  tf.cast(tf.size(y_true), dtype=tf.float32) / tf.math.count_nonzero(y_true, dtype=tf.float32)\n",
    "    # blur = K.mean(K.abs(y_true))\n",
    "    # error = K.square(y_true -y_pred)\n",
    "    # error = error / tf.linalg.norm(y_true, axis=-1)\n",
    "    # error = -K.mean(error)\n",
    "    # normalize for number of active sites\n",
    "    # error = error / tf.linalg.norm(y_true)\n",
    "    # norm = tf.linalg.norm(y_true)\n",
    "\n",
    "    # Maximize variance of norms\n",
    "    # norm_variance = 1/tf.math.reduce_std(tf.linalg.norm(y_true, axis=-1))\n",
    "\n",
    "    return error #+ norm\n",
    "\n",
    "def square(x):\n",
    "    return K.square(x) * K.sign(x)\n",
    "    \n",
    "def scale_act(x):\n",
    "    return tf.transpose(tf.transpose(x) / tf.math.reduce_max(K.abs(x), axis=-1))\n",
    "        \n",
    "\n",
    "class CustomDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training: #you can remove this line,, so that you can use dropout on inference time\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "def define_models(latent_dim, hidden_units=200, reg=1, n_dense_layers=4, activation=\"tanh\", lr=0.001):\n",
    "    n_chans, n_dipoles = leadfield.shape\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    laplace_operator_ = tf.cast(laplace_operator, dtype=tf.float32)\n",
    "    g_activation = tf.keras.layers.LeakyReLU()\n",
    "\n",
    "    # drop_min = 0.95\n",
    "    # drop_max = 0.99\n",
    "    # G MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    fc = Dense(latent_dim, activation=g_activation, name=\"HL_G1\")(inputs)\n",
    "    fc = GaussianNoise(0.5)(fc)\n",
    "\n",
    "    fc = Dense(latent_dim, activation=g_activation, name=\"HL_G2\")(fc)\n",
    "    fc = GaussianNoise(0.5)(fc)\n",
    "    # gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation, activity_regularizer=l1_l2(reg))(fc)\n",
    "    gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation)(fc)\n",
    "    # gen_out = Dropout(0.5)(gen_out)\n",
    "\n",
    "    # gen_out = ActivityRegularization(l2=reg)(gen_out)\n",
    "    \n",
    "    # gen_out = Dropout(0.97)(gen_out, training=True)\n",
    "    # gen_out = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(gen_out, training=True)\n",
    "    # Thresholding/ Sparsification\n",
    "    # gen_out = Activation(scale_act)(gen_out)\n",
    "    # gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)==K.max(K.abs(x)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>0.9, dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>tfp.stats.percentile(K.abs(x), K.random_uniform((1,), drop_min, drop_max)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    \n",
    "    \n",
    "    # Smoothing\n",
    "    gen_out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(None, n_chans))(gen_out)\n",
    "    \n",
    "    # Scaling\n",
    "    # gen_out = LayerNormalization(center=False)(gen_out)\n",
    "    gen_out = Activation(scale_act)(gen_out)\n",
    "    gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # Sparsify\n",
    "    # gen_out = ActivityRegularization(l2=reg)(gen_out)\n",
    "    g_model = tf.keras.Model(inputs=inputs, outputs=gen_out, name='Generator')\n",
    "    # g_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # D MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    input_shape = (n_chans)\n",
    "    inputs2 = tf.keras.Input(shape=input_shape, name='Input_Discriminator')\n",
    "    \n",
    "    fc2 = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs2)\n",
    "    for i in range(n_dense_layers-1):\n",
    "        fc2 = Dense(hidden_units, activation=activation, name=f\"HL_D{i+2}\")(fc2)\n",
    "        \n",
    "    # out = Dense(n_dipoles, activation=activation, activity_regularizer=l2(reg), name=\"Output_Final\")(fc2)\n",
    "    out = Dense(n_dipoles, activation=activation, name=\"Output_Final\")(fc2)\n",
    "    # out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(n_dipoles))(out)\n",
    "    out = Activation(scale_act)(out)\n",
    "    out = Activation(\"tanh\")(out)\n",
    "    # out = ActivityRegularization(l2=reg)(out)\n",
    "    d_model = tf.keras.Model(inputs=inputs2, outputs=out, name='Discriminator')\n",
    "    # d_model.add_loss(l1_sparsity(out))\n",
    "    d_model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=tf.keras.optimizers.Adam(lr=lr))\n",
    "    # d_model.add_loss( tf.keras.losses.CosineSimilarity()(tf.linalg.matmul(leadfield_, tf.transpose(out)), tf.transpose(inputs2)) )\n",
    "    # d_model.add_loss(norm_ratio_ineq(out))\n",
    "    # d_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # GAN MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    d_model.trainable = False\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    output_1 = g_model(inputs)\n",
    "    # output_1 = Dropout(0.97)(output_1, training=True)\n",
    "    # output_1 = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(output_1, training=True)\n",
    "    print(\"source from g: \", output_1)\n",
    "    eeg = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(output_1)\n",
    "    # print(\"eeg from source from g: \", eeg)\n",
    "    eeg_normed = LayerNormalization()(eeg)\n",
    "    # print(\"eeg_normed from source from g: \", eeg_normed)\n",
    "    \n",
    "    # eeg_noise = tf.keras.layers.GaussianNoise(0.2)(eeg_normed)\n",
    "\n",
    "    output_3 = d_model(eeg_normed)\n",
    "    gan_model = Model(inputs, [output_1, output_3])\n",
    "\n",
    "    # g_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    \n",
    "    # d_model.compile(loss=weightedLoss(10), optimizer=\"adam\")\n",
    "    \n",
    "    # Construct your custom loss as a tensor\n",
    "    # loss = tf.math.log(-tf.keras.losses.CosineSimilarity()(output_1, output_3)) + K.square(norm_ratio_ineq(output_1))/5\n",
    "    loss = K.abs(tf.keras.losses.CosineSimilarity()(output_1, output_3)) + l1_sparsity(output_1) * 3\n",
    "    # loss = batch_diversity(output_1) + l1_sparsity(output_1)\n",
    "    \n",
    "    # loss = norm_ratio_ineq(output_1)\n",
    "    # loss = weightedLossGan(10)(output_1, output_3)\n",
    "    \n",
    "    # Add loss to model\n",
    "    gan_model.add_loss(loss)\n",
    "\n",
    "    gan_model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr))\n",
    "    # gan_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    \n",
    "    return g_model, d_model, gan_model\n",
    "\n",
    "def norm_ratio_ineq(x):\n",
    "    l1 = K.mean(K.abs(x)) \n",
    "    l2 = K.mean(K.square(x)) \n",
    "    return l1 / l2 \n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "\n",
    "# def batch_diversity(x):\n",
    "#     avg_source = K.mean(K.abs(x), axis=0)\n",
    "#     l1 = K.mean(K.abs(avg_source)) \n",
    "#     l2 = K.mean(K.square(avg_source)) \n",
    "#     return l1 / l2\n",
    "\n",
    "def batch_diversity(x):\n",
    "    diversity = K.std(K.mean(K.abs(x), axis=0))\n",
    "    return diversity\n",
    "\n",
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y\n",
    "    \n",
    "def prep_data_sim(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "\n",
    "def generate_samples(g_model, batch_size, latent_dim):\n",
    "    x_input = np.random.randn(batch_size, latent_dim)\n",
    "    sources = g_model.predict(x_input)\n",
    "    return sources\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 32\n",
    "batch_number = 10\n",
    "latent_dim = 16\n",
    "g, d, gan = define_models(latent_dim, hidden_units=latent_dim)\n",
    "g(np.random.randn(batch_size, latent_dim)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 288.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 33235.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 680.29it/s]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(duration_of_trial=0.01, extents=(20, 30), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=100)\n",
    "X_test = np.stack([eeg.average().data[:, 0] for eeg in sim.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data[:, 0] for source in sim.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source from g:  KerasTensor(type_spec=TensorSpec(shape=(None, 1284), dtype=tf.float32, name=None), name='Generator/activation_643/Tanh:0', description=\"created by layer 'Generator'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fea301a63b4482f89995754352e121f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.01, disc-loss: -0.01, gan-loss: 0.88\n",
      "Train Disc\n",
      "test_loss: -0.02, disc-loss: -0.02, gan-loss: 0.88\n",
      "Train Disc\n",
      "test_loss: -0.03, disc-loss: -0.07, gan-loss: 0.88\n",
      "Train Disc\n",
      "test_loss: -0.03, disc-loss: -0.18, gan-loss: 0.88\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.03, disc-loss: -0.27, gan-loss: 1.07\n",
      "Train Disc\n",
      "test_loss: -0.03, disc-loss: -0.28, gan-loss: 1.07\n",
      "Train Disc\n",
      "test_loss: -0.03, disc-loss: -0.31, gan-loss: 1.07\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.34, gan-loss: 1.07\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.04, disc-loss: -0.37, gan-loss: 1.11\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.35, gan-loss: 1.11\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.34, gan-loss: 1.11\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.34, gan-loss: 1.11\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.04, disc-loss: -0.35, gan-loss: 1.06\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.29, gan-loss: 1.06\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.27, gan-loss: 1.06\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.27, gan-loss: 1.06\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.04, disc-loss: -0.28, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.20, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.17, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.17, gan-loss: 0.99\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.04, disc-loss: -0.18, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.12, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.12, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.15, gan-loss: 0.94\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.04, disc-loss: -0.17, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.16, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.04, disc-loss: -0.16, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.05, disc-loss: -0.18, gan-loss: 0.93\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.06, disc-loss: -0.20, gan-loss: 0.96\n",
      "Train Disc\n",
      "test_loss: -0.06, disc-loss: -0.21, gan-loss: 0.96\n",
      "Train Disc\n",
      "test_loss: -0.06, disc-loss: -0.22, gan-loss: 0.96\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.96\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.24, gan-loss: 0.98\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.98\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.24, gan-loss: 0.98\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.98\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.99\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.99\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.97\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.97\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.20, gan-loss: 0.97\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.97\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.95\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.20, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.20, gan-loss: 0.94\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.95\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.95\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.22, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.22, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.94\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.24, gan-loss: 0.94\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.24, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.24, gan-loss: 0.94\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.94\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.26, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.26, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.27, gan-loss: 0.93\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.28, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.28, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.28, gan-loss: 0.93\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.93\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.30, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.92\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.29, gan-loss: 0.91\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.27, gan-loss: 0.91\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.27, gan-loss: 0.91\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.26, gan-loss: 0.91\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.27, gan-loss: 0.90\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.26, gan-loss: 0.90\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.90\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.90\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.25, gan-loss: 0.89\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.89\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.89\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.89\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.23, gan-loss: 0.87\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.87\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.20, gan-loss: 0.87\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.20, gan-loss: 0.87\n",
      "Train Disc\n",
      "Train Gen\n",
      "test_loss: -0.07, disc-loss: -0.21, gan-loss: 0.87\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.87\n",
      "Train Disc\n",
      "test_loss: -0.07, disc-loss: -0.19, gan-loss: 0.87\n",
      "Train Disc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9344/1552859121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisc_mod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Disc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mleadfield\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9344/3979061300.py\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(g_model, batch_size, latent_dim)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mx_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "n_epochs = 10000\n",
    "batch_size = 1024\n",
    "latent_dim = 200\n",
    "hidden_units = 200\n",
    "reg = 0.01\n",
    "gen_mod = 4\n",
    "disc_mod = 1\n",
    "\n",
    "g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg, lr=0.001)\n",
    "# _, d_model, _ = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "# g_model, _, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg, lr=0.001)\n",
    "\n",
    "X_history = np.zeros((0, n_chans))\n",
    "y_history = np.zeros((0, n_dipoles))\n",
    "gan_losses = np.zeros(n_epochs)\n",
    "d_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "k = 20\n",
    "generated = []\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    if i % disc_mod == 0:\n",
    "        print(\"Train Disc\")\n",
    "        y = generate_samples(g_model, batch_size, latent_dim)\n",
    "        X = (leadfield @ y.T).T\n",
    "        X, y = prep_data(X,y)\n",
    "        generated.append(abs(y).mean(axis=0))\n",
    "        X_history = np.append(X_history, X, axis=0)\n",
    "        y_history = np.append(y_history, y, axis=0)\n",
    "        idc = np.arange(X_history.shape[0])\n",
    "        np.random.shuffle(idc)\n",
    "        X_history = X_history[idc]\n",
    "        y_history = y_history[idc]\n",
    "        X_history = X_history[:batch_size]\n",
    "        y_history = y_history[:batch_size]\n",
    "        d_loss = d_model.train_on_batch(X_history, y_history)\n",
    "        \n",
    "        # d_loss = d_model.train_on_batch(X, y)\n",
    "        test_loss = d_model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_losses[i] = test_loss\n",
    "\n",
    "        # print(\"gan_generator: \", gan_model.layers[1].layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"generator: \", g_model.layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"gan_discriminator: \", gan_model.layers[4].layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"discriminator: \", d_model.layers[5].weights[0][0,0].numpy())\n",
    "\n",
    "    if i % gen_mod == 0:    \n",
    "        print(\"Train Gen\")\n",
    "        x_input = np.random.randn(batch_size, latent_dim)\n",
    "\n",
    "        X = np.ones((batch_size, n_dipoles))\n",
    "        # d_model.trainable = False\n",
    "        # gan_model.layers[4].trainable = False\n",
    "        gan_loss = gan_model.train_on_batch(x_input, X)\n",
    "        # d_model.trainable = True\n",
    "        # gan_model.layers[4].trainable = True\n",
    "        \n",
    "        # print(\"gan_generator: \", gan_model.layers[1].layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"generator: \", g_model.layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"gan_discriminator: \", gan_model.layers[4].layers[5].weights[0][0,0].numpy())\n",
    "        # print(\"discriminator: \", d_model.layers[5].weights[0][0,0].numpy())\n",
    "\n",
    "    print(f'test_loss: {test_loss:.2f}, disc-loss: {d_loss:.2f}, gan-loss: {gan_loss:.2f}')\n",
    "    d_losses[i] = d_loss\n",
    "    \n",
    "    gan_losses[i] = gan_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1e359e42f70>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.16726334 0.17712846 0.85275178]\n"
     ]
    }
   ],
   "source": [
    "stc_ = stc.copy()\n",
    "stc_.data = np.stack(generated, axis=0).T\n",
    "stc_.plot(**plot_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Diversity:  0.041135274 L1:  0.09270973\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(gan_losses, label=\"GAN Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# # g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "y = generate_samples(g_model, 1000, latent_dim)\n",
    "data = abs(y).mean(axis=0)\n",
    "stc_ = stc.copy()\n",
    "stc_.data[:, 0] = data\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = y.T\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y.flatten())\n",
    "print(\"Batch Diversity: \", batch_diversity(y).numpy(), \"L1: \", l1_sparsity(y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17.24it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 503.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.63, cosine=-0.02143716812133789, r=0.08, p=0.0057\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.70694911 0.71876111 0.75866978]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=(1,15), target_snr=1e99, source_number_weighting=False)\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=15, target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.01, extents=25, number_of_sources=1, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim_test.source_data], axis=0)\n",
    "\n",
    "X, y = prep_data(X[0].T, y[0].T)\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat.shape\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth Sim\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "from scipy.stats import pearsonr\n",
    "from esinet.evaluate import eval_auc\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_hat.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_hat.data.T, dtype=tf.float32)).numpy()\n",
    "r,p = pearsonr(stc.data.flatten(), stc_hat.data.flatten())\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.64, r=0.48, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(stc.data.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_mne.data.T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Discriminator with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.53, r=0.27, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.00000000e+00 0.00000000e+00 3.23073185e-08]\n",
      "Using control points [0.71678151 0.72408965 0.75658708]\n",
      "Using control points [0.76244749 0.80534982 0.9834994 ]\n",
      "Using control points [0.72853811 0.77025538 0.98795814]\n",
      "Using control points [0.71960323 0.77341869 0.99793099]\n",
      "Using control points [0.67337268 0.70955763 0.94592068]\n",
      "Using control points [0.69988883 0.73342427 0.97168994]\n",
      "Using control points [0.69988883 0.73342427 0.97168994]\n",
      "Using control points [0.65326065 0.69577952 0.93740261]\n",
      "Using control points [0.77052891 0.81111981 0.96638858]\n",
      "Using control points [0.76653285 0.81531107 0.99634089]\n",
      "Using control points [0.76653285 0.81531107 0.99634089]\n",
      "Using control points [0.78645126 0.84070358 0.99638031]\n",
      "Using control points [0.72083507 0.773191   0.96052652]\n",
      "Using control points [0.72681033 0.78352666 0.95552857]\n",
      "Using control points [0.69347107 0.73425538 0.99775193]\n",
      "Using control points [0.72781926 0.7725812  0.99002095]\n",
      "Using control points [0.69834372 0.74988179 0.9889217 ]\n",
      "Using control points [0.73658948 0.78783124 0.97187997]\n",
      "Using control points [0.72060001 0.76203827 0.9833346 ]\n",
      "Using control points [0.72060001 0.76203827 0.9833346 ]\n",
      "Using control points [0.66901532 0.70513413 0.98999446]\n"
     ]
    }
   ],
   "source": [
    "# generated data:\n",
    "y = generate_samples(g_model, 32, latent_dim)\n",
    "X = (leadfield @ y.T).T\n",
    "X, y = prep_data(X,y)\n",
    "\n",
    "y[np.isnan(y)] = 0\n",
    "stc_hat.data = y.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat[np.isnan(y_hat)] = 0\n",
    "\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "\n",
    "r,p = pearsonr(y.flatten(), y_hat.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y, y_hat)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "\n",
    "evoked = mne.EvokedArray(X[:, 0, :].T, info)\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "stc_mne = solver.apply_inverse_operator(evoked)\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(y[:, 0, :].flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y[:, 0, :].T, stc_mne.data .T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "basic_sources = np.identity(n_dipoles)\n",
    "basic_sources = laplace_operator @ basic_sources\n",
    "basic_sources = np.concatenate([basic_sources, -1*basic_sources], axis=1)\n",
    "basic_eeg = leadfield @ basic_sources\n",
    "print(basic_sources.shape, basic_eeg.shape)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    g_model = tf.keras.Sequential()\n",
    "    input_shape = (None, latent_dim)\n",
    "    g_model.add(InputLayer(input_shape=input_shape))\n",
    "    g_model.add(Dense(latent_dim, name=\"HL1\"))\n",
    "    g_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    # g_model.build()\n",
    "    # g_model.compile(optimizer='adam', loss=\"mse\")\n",
    "    # g_model.summary()\n",
    "    return g_model\n",
    "    \n",
    "def define_discriminator(hidden_units=100):\n",
    "    input_shape = (None, n_chans)\n",
    "    d_model = tf.keras.Sequential()\n",
    "    d_model.add(InputLayer(input_shape=input_shape))\n",
    "    d_model.add(Dense(hidden_units, name=\"HL1\"))\n",
    "    d_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    d_model.build()\n",
    "    d_model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "    # d_model.summary()\n",
    "    return d_model\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    input_shape = (None, latent_dim)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(g_model.output)\n",
    "    print(lam)\n",
    "    discriminator = d_model(lam)\n",
    "    model = tf.keras.Model(inputs=g_model.input, outputs=[d_model.output, g_model.output], name='Contextualizer')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(g_model)\n",
    "    # model.add(Lambda(lambda x: tf.linalg.matmul(leadfield_, x)))\n",
    "    # model.add(d_model)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
