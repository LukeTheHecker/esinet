{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    2.3s remaining:    3.9s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.3s remaining:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))\n",
    "laplace_operator = laplace_operator @ laplace_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1284])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, Activation, Dropout, ActivityRegularization, TimeDistributed, Reshape, Permute\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def weightedLoss(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"DISC\", true, pred)\n",
    "        \n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error / tf.linalg.norm(true, axis=-1)\n",
    "        return error\n",
    "    return loss\n",
    "\n",
    "def weightedLossGan(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"GAN\", true, pred)\n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error * tf.linalg.norm(true)\n",
    "        return -error\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_gan_loss(y_ones, y_both):\n",
    "    y_true = y_both[0]\n",
    "    y_pred = y_both[1]\n",
    "    # print(y_ones, y_true, y_pred)\n",
    "    \n",
    "    error = -tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    # error = -tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    # blur =  tf.cast(tf.size(y_true), dtype=tf.float32) / tf.math.count_nonzero(y_true, dtype=tf.float32)\n",
    "    # blur = K.mean(K.abs(y_true))\n",
    "    # error = K.square(y_true -y_pred)\n",
    "    # error = error / tf.linalg.norm(y_true, axis=-1)\n",
    "    # error = -K.mean(error)\n",
    "    # normalize for number of active sites\n",
    "    # error = error / tf.linalg.norm(y_true)\n",
    "    # norm = tf.linalg.norm(y_true)\n",
    "\n",
    "    # Maximize variance of norms\n",
    "    # norm_variance = 1/tf.math.reduce_std(tf.linalg.norm(y_true, axis=-1))\n",
    "\n",
    "    return error #+ norm\n",
    "\n",
    "def square(x):\n",
    "    return K.square(x) * K.sign(x)\n",
    "    \n",
    "def scale_act(x):\n",
    "    return tf.transpose(tf.transpose(x) / tf.math.reduce_max(K.abs(x), axis=-1))\n",
    "\n",
    "def RelativeCosineSimilarity():\n",
    "    def relative_cosine_similarity(y_true, y_pred):\n",
    "        distance = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        \n",
    "\n",
    "class CustomDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training: #you can remove this line,, so that you can use dropout on inference time\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "def define_models(latent_dim, hidden_units=200, reg=1, n_dense_layers=4, activation=\"tanh\"):\n",
    "    n_chans, n_dipoles = leadfield.shape\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    laplace_operator_ = tf.cast(laplace_operator, dtype=tf.float32)\n",
    "    drop_min = 0.95\n",
    "    drop_max = 0.99\n",
    "    # G MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    fc = Dense(latent_dim, activation=activation, name=\"HL_G1\")(inputs)\n",
    "    fc = Dropout(0.5)(fc)\n",
    "    fc = Dense(latent_dim, activation=activation, name=\"HL_G2\")(fc)\n",
    "    fc = Dropout(0.5)(fc)\n",
    "    # gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation, activity_regularizer=l1_l2(reg))(fc)\n",
    "    gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation)(fc)\n",
    "    # gen_out = ActivityRegularization(l2=reg)(gen_out)\n",
    "    \n",
    "    # gen_out = Dropout(0.97)(gen_out, training=True)\n",
    "    # gen_out = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(gen_out, training=True)\n",
    "    # Thresholding/ Sparsification\n",
    "    # gen_out = Activation(scale_act)(gen_out)\n",
    "    # gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)==K.max(K.abs(x)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>0.9, dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>tfp.stats.percentile(K.abs(x), K.random_uniform((1,), drop_min, drop_max)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    \n",
    "    \n",
    "    # Smoothing\n",
    "    gen_out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(None, n_chans))(gen_out)\n",
    "    \n",
    "    # Scaling\n",
    "    # gen_out = LayerNormalization(center=False)(gen_out)\n",
    "    gen_out = Activation(scale_act)(gen_out)\n",
    "    gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # Sparsify\n",
    "    # gen_out = ActivityRegularization(l2=reg)(gen_out)\n",
    "    g_model = tf.keras.Model(inputs=inputs, outputs=gen_out, name='Generator')\n",
    "    # g_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # D MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    input_shape = (n_chans)\n",
    "    inputs2 = tf.keras.Input(shape=input_shape, name='Input_Discriminator')\n",
    "    \n",
    "    fc2 = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs2)\n",
    "    for i in range(n_dense_layers-1):\n",
    "        fc2 = Dense(hidden_units, activation=activation, name=f\"HL_D{i+2}\")(fc2)\n",
    "        \n",
    "    # out = Dense(n_dipoles, activation=activation, activity_regularizer=l2(reg), name=\"Output_Final\")(fc2)\n",
    "    out_main = Dense(n_dipoles, activation=activation, name=\"Output_Final\")(fc2)\n",
    "    out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(n_dipoles))(out_main)\n",
    "    out = Activation(scale_act)(out)\n",
    "    out = Activation(\"tanh\")(out)\n",
    "    # out = ActivityRegularization(l2=reg)(out)\n",
    "    d_model = tf.keras.Model(inputs=inputs2, outputs=out, name='Discriminator')\n",
    "    # d_model.add_loss(norm_ratio_ineq(out))\n",
    "    # d_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # GAN MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    # d_model.trainable = False\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    output_1 = g_model(inputs)\n",
    "    # output_1 = Dropout(0.97)(output_1, training=True)\n",
    "    # output_1 = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(output_1, training=True)\n",
    "    # print(\"source from g: \", output_1)\n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))\n",
    "    eeg = lam(output_1)\n",
    "    # print(\"eeg from source from g: \", eeg)\n",
    "    eeg_normed = LayerNormalization()(eeg)\n",
    "    # print(\"eeg_normed from source from g: \", eeg_normed)\n",
    "    \n",
    "    # eeg_noise = tf.keras.layers.GaussianNoise(0.2)(eeg_normed)\n",
    "\n",
    "    output_3 = d_model(eeg_normed)\n",
    "    gan_model = Model(inputs, [output_1, output_3])\n",
    "\n",
    "    # g_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    d_model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\", metrics=[tf.keras.losses.CosineSimilarity(),])\n",
    "    # d_model.compile(loss=weightedLoss(10), optimizer=\"adam\")\n",
    "    \n",
    "    # Construct your custom loss as a tensor\n",
    "    # loss = tf.math.log(-tf.keras.losses.CosineSimilarity()(output_1, output_3)) + K.square(norm_ratio_ineq(output_1))/5\n",
    "    loss = -tf.keras.losses.CosineSimilarity()(output_1, output_3) + l1_sparsity(output_1)*10\n",
    "    \n",
    "    # loss = norm_ratio_ineq(output_1)\n",
    "    \n",
    "    \n",
    "    # loss = weightedLossGan(10)(output_1, output_3)\n",
    "    \n",
    "    # Add loss to model\n",
    "    gan_model.add_loss(loss)\n",
    "\n",
    "    gan_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    # gan_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    \n",
    "    return g_model, d_model, gan_model\n",
    "\n",
    "def norm_ratio_ineq(x):\n",
    "    l1 = K.mean(K.abs(x)) \n",
    "    l2 = K.mean(K.square(x)) \n",
    "    return l1 / l2 \n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "\n",
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y\n",
    "    \n",
    "def prep_data_sim(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "\n",
    "def generate_samples(g_model, batch_size, latent_dim):\n",
    "    x_input = np.random.randn(batch_size, latent_dim)\n",
    "    sources = g_model.predict(x_input)\n",
    "    return sources\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 32\n",
    "batch_number = 10\n",
    "latent_dim = 16\n",
    "g, d, gan = define_models(latent_dim, hidden_units=latent_dim)\n",
    "g(np.random.randn(batch_size, latent_dim)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 185.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 20037.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 482.50it/s]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(duration_of_trial=0.01, extents=(20, 30), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=100)\n",
    "X_test = np.stack([eeg.average().data[:, 0] for eeg in sim.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data[:, 0] for source in sim.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd5db07e5dc4c158ef7e757e34700cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: -0.10, disc-loss: 0.00, gan-loss: 2.67\n",
      "test_loss: -0.07, disc-loss: -0.09, gan-loss: 2.63\n",
      "test_loss: -0.05, disc-loss: -0.06, gan-loss: 2.58\n",
      "test_loss: -0.04, disc-loss: -0.04, gan-loss: 2.55\n",
      "test_loss: -0.03, disc-loss: -0.03, gan-loss: 2.53\n",
      "test_loss: -0.02, disc-loss: -0.01, gan-loss: 2.52\n",
      "test_loss: -0.01, disc-loss: -0.00, gan-loss: 2.49\n",
      "test_loss: -0.01, disc-loss: 0.01, gan-loss: 2.47\n",
      "test_loss: 0.00, disc-loss: 0.02, gan-loss: 2.45\n",
      "test_loss: 0.01, disc-loss: 0.03, gan-loss: 2.43\n",
      "test_loss: 0.01, disc-loss: 0.04, gan-loss: 2.41\n",
      "test_loss: 0.02, disc-loss: 0.05, gan-loss: 2.39\n",
      "test_loss: 0.03, disc-loss: 0.05, gan-loss: 2.37\n",
      "test_loss: 0.03, disc-loss: 0.06, gan-loss: 2.35\n",
      "test_loss: 0.04, disc-loss: 0.07, gan-loss: 2.32\n",
      "test_loss: 0.04, disc-loss: 0.08, gan-loss: 2.31\n",
      "test_loss: 0.05, disc-loss: 0.09, gan-loss: 2.28\n",
      "test_loss: 0.05, disc-loss: 0.10, gan-loss: 2.26\n",
      "test_loss: 0.05, disc-loss: 0.10, gan-loss: 2.24\n",
      "test_loss: 0.05, disc-loss: 0.11, gan-loss: 2.21\n",
      "test_loss: 0.05, disc-loss: 0.12, gan-loss: 2.18\n",
      "test_loss: 0.06, disc-loss: 0.12, gan-loss: 2.16\n",
      "test_loss: 0.06, disc-loss: 0.12, gan-loss: 2.13\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 2.13\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 2.13\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 2.11\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 2.08\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 2.04\n",
      "test_loss: 0.05, disc-loss: 0.14, gan-loss: 1.98\n",
      "test_loss: 0.05, disc-loss: 0.14, gan-loss: 1.93\n",
      "test_loss: 0.05, disc-loss: 0.14, gan-loss: 1.90\n",
      "test_loss: 0.05, disc-loss: 0.14, gan-loss: 1.88\n",
      "test_loss: 0.05, disc-loss: 0.13, gan-loss: 1.86\n",
      "test_loss: 0.04, disc-loss: 0.12, gan-loss: 1.87\n",
      "test_loss: 0.03, disc-loss: 0.11, gan-loss: 1.84\n",
      "test_loss: 0.03, disc-loss: 0.10, gan-loss: 1.81\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.80\n",
      "test_loss: 0.02, disc-loss: 0.09, gan-loss: 1.76\n",
      "test_loss: 0.02, disc-loss: 0.09, gan-loss: 1.72\n",
      "test_loss: 0.02, disc-loss: 0.09, gan-loss: 1.69\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.61\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.56\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.52\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.48\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.45\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.43\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.39\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.37\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.34\n",
      "test_loss: 0.01, disc-loss: 0.11, gan-loss: 1.32\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.29\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.28\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.26\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.23\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.20\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.19\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.17\n",
      "test_loss: 0.02, disc-loss: 0.10, gan-loss: 1.15\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.13\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.12\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.08\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.07\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.07\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.05\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.04\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.02\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 1.00\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.99\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.98\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.96\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.95\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.94\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.93\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.91\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.91\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.90\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.89\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.88\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.86\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.85\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.84\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.83\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.81\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.81\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.80\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.79\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.77\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.77\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.77\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.75\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.75\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.73\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.72\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.71\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.71\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.70\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.69\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.68\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.68\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.67\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.66\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.66\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.65\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.63\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.64\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.63\n",
      "test_loss: 0.02, disc-loss: 0.11, gan-loss: 0.62\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.61\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.61\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.60\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.59\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.60\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.58\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.57\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.56\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.56\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.56\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.55\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.55\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.54\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.54\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.54\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.52\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.51\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.51\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.50\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.50\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.49\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.50\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.48\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.48\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.47\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.47\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.47\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.46\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.45\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.45\n",
      "test_loss: 0.02, disc-loss: 0.12, gan-loss: 0.45\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.44\n",
      "test_loss: 0.02, disc-loss: 0.13, gan-loss: 0.43\n",
      "test_loss: 0.03, disc-loss: 0.13, gan-loss: 0.44\n",
      "test_loss: 0.03, disc-loss: 0.13, gan-loss: 0.43\n",
      "test_loss: 0.03, disc-loss: 0.13, gan-loss: 0.43\n",
      "test_loss: 0.03, disc-loss: 0.13, gan-loss: 0.43\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.43\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.42\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.41\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.40\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.41\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.40\n",
      "test_loss: 0.03, disc-loss: 0.14, gan-loss: 0.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2240/3101590433.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# d_loss = d_model.train_on_batch(np.append(X_history, X_test, axis=0) , np.append(y_history, y_test, axis=0) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgen_mod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1452\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m         \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m         data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1455\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1923\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   1924\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 1925\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1926\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4486\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4487\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[1;32m-> 4488\u001b[1;33m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[0;32m   4489\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4490\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name)\u001b[0m\n\u001b[0;32m   3167\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3169\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3170\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3171\u001b[0m         \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "n_epochs = 1000\n",
    "batch_size = 1024\n",
    "latent_dim = 500\n",
    "hidden_units = 500\n",
    "reg = 0.01\n",
    "gen_mod = 1\n",
    "disc_mod = 1\n",
    "g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "X_history = np.zeros((0, n_chans))\n",
    "y_history = np.zeros((0, n_dipoles))\n",
    "gan_losses = np.zeros(n_epochs)\n",
    "d_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "k = 20\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    if i % disc_mod == 0:\n",
    "        y = generate_samples(g_model, batch_size, latent_dim)\n",
    "        X = (leadfield @ y.T).T\n",
    "        X, y = prep_data(X,y)\n",
    "        X_history = np.append(X_history, X, axis=0)\n",
    "        y_history = np.append(y_history, y, axis=0)\n",
    "        idc = np.arange(X_history.shape[0])\n",
    "        np.random.shuffle(idc)\n",
    "        X_history = X_history[idc]\n",
    "        y_history = y_history[idc]\n",
    "        X_history = X_history[:batch_size]\n",
    "        y_history = y_history[:batch_size]\n",
    "\n",
    "        # d_model.trainable = True\n",
    "        # d_model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "        d_loss = d_model.train_on_batch(X_history, y_history)[1]\n",
    "        # d_loss = d_model.train_on_batch(np.append(X_history, X_test, axis=0) , np.append(y_history, y_test, axis=0) )\n",
    "        \n",
    "        test_loss = d_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "        test_losses[i] = test_loss\n",
    "    if i % gen_mod == 0:    \n",
    "        x_input = np.random.randn(batch_size, latent_dim)\n",
    "        X = np.ones((batch_size, n_dipoles))\n",
    "        d_model.trainable = False\n",
    "        gan_loss = gan_model.train_on_batch(x_input, X)\n",
    "\n",
    "    print(f'test_loss: {test_loss:.2f}, disc-loss: {d_loss:.2f}, gan-loss: {gan_loss:.2f}')\n",
    "    d_losses[i] = d_loss\n",
    "    \n",
    "    gan_losses[i] = gan_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  761.,   616.,   962.,  3643., 57758., 58807.,  3663.,   893.,\n",
       "          632.,   665.]),\n",
       " array([-0.7615942 , -0.60927534, -0.4569565 , -0.30463767, -0.15231884,\n",
       "         0.        ,  0.15231884,  0.30463767,  0.4569565 ,  0.60927534,\n",
       "         0.7615942 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.26009514 0.39334973 0.76121824]\n",
      "Using control points [0.25965381 0.41853858 0.74863843]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(gan_losses, label=\"GAN Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "y = generate_samples(g_model, 100, latent_dim)\n",
    "data = abs(y).mean(axis=0)\n",
    "stc_ = stc.copy()\n",
    "stc_.data[:, 0] = data\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = y.T\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.48456874>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(y[0])\n",
    "l1_sparsity(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 244.38it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 392.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.25, cosine=0.006478124298155308, r=-0.03, p=0.3583\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.00000000e+00 0.00000000e+00 6.75949907e-08]\n",
      "Using control points [0.00000000e+00 0.00000000e+00 6.75949907e-08]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=(1,15), target_snr=1e99, source_number_weighting=False)\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=15, target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.01, extents=25, number_of_sources=1, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim_test.source_data], axis=0)\n",
    "\n",
    "X, y = prep_data(X[0].T, y[0].T)\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat.shape\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth Sim\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "from scipy.stats import pearsonr\n",
    "from esinet.evaluate import eval_auc\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_hat.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_hat.data.T, dtype=tf.float32)).numpy()\n",
    "r,p = pearsonr(stc.data.flatten(), stc_hat.data.flatten())\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.66, r=0.54, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(stc.data.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_mne.data.T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Discriminator with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.507000e+03, 2.830100e+04, 1.034900e+04, 7.477900e+04,\n",
       "        6.305901e+06, 6.296174e+06, 7.593000e+04, 1.007600e+04,\n",
       "        2.595100e+04, 6.032000e+03]),\n",
       " array([-0.7615942 , -0.60927534, -0.4569565 , -0.30463767, -0.15231884,\n",
       "         0.        ,  0.15231884,  0.30463767,  0.4569565 ,  0.60927534,\n",
       "         0.7615942 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "y = generate_samples(g_model, 10000, latent_dim)\n",
    "y.shape\n",
    "data = abs(y).mean(axis=0)\n",
    "stc_ = stc.copy()\n",
    "stc_.data[:, 0] = data\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.58, r=-0.71, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# generated data:\n",
    "y = generate_samples(g_model, 32, latent_dim).T\n",
    "X = (leadfield @ y).T\n",
    "X, y = prep_data(X,y)\n",
    "y = y\n",
    "X = X\n",
    "y[np.isnan(y)] = 0\n",
    "stc_hat.data = y\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat[np.isnan(y_hat)] = 0\n",
    "\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "\n",
    "r,p = pearsonr(y.flatten(), y_hat.T.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y.T, y_hat)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.50, r=0.03, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [1. 1. 1.]\n",
      "Using control points [1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "\n",
    "evoked = mne.EvokedArray(X[:, 0, :].T, info)\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "stc_mne = solver.apply_inverse_operator(evoked)\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(y[:, 0, :].flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y[:, 0, :].T, stc_mne.data .T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "basic_sources = np.identity(n_dipoles)\n",
    "basic_sources = laplace_operator @ basic_sources\n",
    "basic_sources = np.concatenate([basic_sources, -1*basic_sources], axis=1)\n",
    "basic_eeg = leadfield @ basic_sources\n",
    "print(basic_sources.shape, basic_eeg.shape)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    g_model = tf.keras.Sequential()\n",
    "    input_shape = (None, latent_dim)\n",
    "    g_model.add(InputLayer(input_shape=input_shape))\n",
    "    g_model.add(Dense(latent_dim, name=\"HL1\"))\n",
    "    g_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    # g_model.build()\n",
    "    # g_model.compile(optimizer='adam', loss=\"mse\")\n",
    "    # g_model.summary()\n",
    "    return g_model\n",
    "    \n",
    "def define_discriminator(hidden_units=100):\n",
    "    input_shape = (None, n_chans)\n",
    "    d_model = tf.keras.Sequential()\n",
    "    d_model.add(InputLayer(input_shape=input_shape))\n",
    "    d_model.add(Dense(hidden_units, name=\"HL1\"))\n",
    "    d_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    d_model.build()\n",
    "    d_model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "    # d_model.summary()\n",
    "    return d_model\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    input_shape = (None, latent_dim)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(g_model.output)\n",
    "    print(lam)\n",
    "    discriminator = d_model(lam)\n",
    "    model = tf.keras.Model(inputs=g_model.input, outputs=[d_model.output, g_model.output], name='Contextualizer')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(g_model)\n",
    "    # model.add(Lambda(lambda x: tf.linalg.matmul(leadfield_, x)))\n",
    "    # model.add(d_model)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
