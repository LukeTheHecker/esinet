{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))\n",
    "# laplace_operator = laplace_operator @ laplace_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (None, 61).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, Activation, Dropout, ActivityRegularization\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "\n",
    "def weightedLoss(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"DISC\", true, pred)\n",
    "        \n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error / tf.linalg.norm(true, axis=-1)\n",
    "        return error\n",
    "    return loss\n",
    "\n",
    "def weightedLossGan(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"GAN\", true, pred)\n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error * tf.linalg.norm(true)\n",
    "        return -error\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_gan_loss(y_ones, y_both):\n",
    "    y_true = y_both[0]\n",
    "    y_pred = y_both[1]\n",
    "    # print(y_ones, y_true, y_pred)\n",
    "    \n",
    "    error = -tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    # error = -tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    # blur =  tf.cast(tf.size(y_true), dtype=tf.float32) / tf.math.count_nonzero(y_true, dtype=tf.float32)\n",
    "    # blur = K.mean(K.abs(y_true))\n",
    "    # error = K.square(y_true -y_pred)\n",
    "    # error = error / tf.linalg.norm(y_true, axis=-1)\n",
    "    # error = -K.mean(error)\n",
    "    # normalize for number of active sites\n",
    "    # error = error / tf.linalg.norm(y_true)\n",
    "    # norm = tf.linalg.norm(y_true)\n",
    "\n",
    "    # Maximize variance of norms\n",
    "    # norm_variance = 1/tf.math.reduce_std(tf.linalg.norm(y_true, axis=-1))\n",
    "\n",
    "    return error #+ norm\n",
    "\n",
    "def square(x):\n",
    "    return K.square(x) * K.sign(x)\n",
    "    \n",
    "def scale_act(x):\n",
    "    return tf.transpose(tf.transpose(x) / tf.math.reduce_max(K.abs(x), axis=-1))\n",
    "\n",
    "def RelativeCosineSimilarity():\n",
    "    def relative_cosine_similarity(y_true, y_pred):\n",
    "        distance = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        \n",
    "\n",
    "class CustomDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training: #you can remove this line,, so that you can use dropout on inference time\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "def define_models(latent_dim, hidden_units=200, reg=1, n_dense_layers=4, activation=\"tanh\"):\n",
    "    n_chans, n_dipoles = leadfield.shape\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    laplace_operator_ = tf.cast(laplace_operator, dtype=tf.float32)\n",
    "    drop_min = 0.95\n",
    "    drop_max = 0.99\n",
    "    # G MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    fc = Dense(latent_dim, activation=activation, name=\"HL_G1\")(inputs)\n",
    "    # fc = Dropout(0.5)(fc)\n",
    "    # fc = Dense(latent_dim, activation=activation, name=\"HL_G2\")(fc)\n",
    "    # fc = Dropout(0.5)(fc)\n",
    "    # gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation, activity_regularizer=l1_l2(reg))(fc)\n",
    "    gen_out = Dense(n_dipoles, name=\"Output_Generator\", activation=activation)(fc)\n",
    "    # gen_out = Dropout(0.97)(gen_out, training=True)\n",
    "    # gen_out = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(gen_out, training=True)\n",
    "    # Thresholding/ Sparsification\n",
    "    # gen_out = Activation(scale_act)(gen_out)\n",
    "    # gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)==K.max(K.abs(x)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>tfp.stats.percentile(K.abs(x), 97), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    # gen_out = Lambda(lambda x: tf.cast(K.abs(x)>tfp.stats.percentile(K.abs(x), K.random_uniform((1,), drop_min, drop_max)), dtype=x.dtype) * x, output_shape=(None, n_dipoles))(gen_out)\n",
    "    \n",
    "    \n",
    "    # Smoothing\n",
    "    gen_out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(None, n_chans))(gen_out)\n",
    "    \n",
    "    # Scaling\n",
    "    # gen_out = LayerNormalization(center=False)(gen_out)\n",
    "    gen_out = Activation(scale_act)(gen_out)\n",
    "    gen_out = Activation(\"tanh\")(gen_out)\n",
    "    # Sparsify\n",
    "    # gen_out = ActivityRegularization(l2=reg)(gen_out)\n",
    "    g_model = tf.keras.Model(inputs=inputs, outputs=gen_out, name='Generator')\n",
    "    # g_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # D MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    input_shape = (None, n_chans)\n",
    "    inputs2 = tf.keras.Input(shape=input_shape, name='Input_Discriminator')\n",
    "    \n",
    "    fc2 = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs2)\n",
    "    for i in range(n_dense_layers-1):\n",
    "        fc2 = Dense(hidden_units, activation=activation, name=f\"HL_D{i+2}\")(fc2)\n",
    "        \n",
    "    # out = Dense(n_dipoles, activation=activation, activity_regularizer=l1(0.1), name=\"Output_Final\")(fc2)\n",
    "    out = Dense(n_dipoles, activation=activation, name=\"Output_Final\")(fc2)\n",
    "    \n",
    "    d_model = tf.keras.Model(inputs=inputs2, outputs=out, name='Discriminator')\n",
    "    # d_model.build(input_shape=(latent_dim))\n",
    "    \n",
    "    # GAN MODEL\n",
    "    # ------------------------------------------------------------------------\n",
    "    d_model.traineble = False\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    output_1 = g_model(inputs)\n",
    "    # output_1 = Dropout(0.97)(output_1, training=True)\n",
    "    # output_1 = Dropout(K.random_uniform((1,), drop_min, drop_max)[0])(output_1, training=True)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))\n",
    "    eeg = lam(output_1)\n",
    "    eeg_normed = LayerNormalization()(eeg)\n",
    "    # eeg_noise = tf.keras.layers.GaussianNoise(0.2)(eeg_normed)\n",
    "\n",
    "    output_3 = d_model(eeg_normed)\n",
    "    gan_model = Model(inputs, [output_1, output_3])\n",
    "\n",
    "    # g_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    d_model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "    # d_model.compile(loss=weightedLoss(10), optimizer=\"adam\")\n",
    "    \n",
    "    # Construct your custom loss as a tensor\n",
    "    loss = tf.math.log(-tf.keras.losses.CosineSimilarity()(output_1, output_3))\n",
    "    # loss = weightedLossGan(10)(output_1, output_3)\n",
    "    \n",
    "    # Add loss to model\n",
    "    gan_model.add_loss(loss)\n",
    "\n",
    "    gan_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    # gan_model.compile(loss=custom_gan_loss, optimizer=\"adam\")\n",
    "    \n",
    "    return g_model, d_model, gan_model\n",
    "\n",
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "    \n",
    "def prep_data_sim(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "\n",
    "def generate_samples(g_model, batch_size, latent_dim):\n",
    "    x_input = np.random.randn(batch_size, latent_dim)\n",
    "    sources = g_model.predict(x_input)\n",
    "    return sources\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 32\n",
    "batch_number = 10\n",
    "latent_dim = 1000\n",
    "g_model, d_model, gan_model = define_models(latent_dim, hidden_units=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (None, 61).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038f27ecbdc84cb5b1b0bf58572de299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (1024, 61).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (1024, 61).\n",
      "disc-loss: -0.00, gan-loss: -3.04\n",
      "disc-loss: 0.03, gan-loss: -3.04\n",
      "disc-loss: -0.00, gan-loss: -3.04\n",
      "disc-loss: -0.03, gan-loss: -3.04\n",
      "disc-loss: -0.06, gan-loss: -3.04\n",
      "disc-loss: -0.08, gan-loss: -3.04\n",
      "disc-loss: -0.10, gan-loss: -3.04\n",
      "disc-loss: -0.12, gan-loss: -3.04\n",
      "disc-loss: -0.14, gan-loss: -3.04\n",
      "disc-loss: -0.15, gan-loss: -3.04\n",
      "disc-loss: -0.17, gan-loss: -3.04\n",
      "disc-loss: -0.19, gan-loss: -3.04\n",
      "disc-loss: -0.20, gan-loss: -3.04\n",
      "disc-loss: -0.21, gan-loss: -3.04\n",
      "disc-loss: -0.22, gan-loss: -3.04\n",
      "disc-loss: -0.23, gan-loss: -3.04\n",
      "disc-loss: -0.24, gan-loss: -3.04\n",
      "disc-loss: -0.25, gan-loss: -3.04\n",
      "disc-loss: -0.26, gan-loss: -3.04\n",
      "disc-loss: -0.27, gan-loss: -3.04\n",
      "disc-loss: -0.28, gan-loss: -3.04\n",
      "disc-loss: -0.28, gan-loss: -3.04\n",
      "disc-loss: -0.29, gan-loss: -3.04\n",
      "disc-loss: -0.30, gan-loss: -3.04\n",
      "disc-loss: -0.30, gan-loss: -3.04\n",
      "disc-loss: -0.31, gan-loss: -1.14\n",
      "disc-loss: -0.29, gan-loss: -1.14\n",
      "disc-loss: -0.30, gan-loss: -1.14\n",
      "disc-loss: -0.31, gan-loss: -1.14\n",
      "disc-loss: -0.31, gan-loss: -1.14\n",
      "disc-loss: -0.32, gan-loss: -1.14\n",
      "disc-loss: -0.32, gan-loss: -1.14\n",
      "disc-loss: -0.33, gan-loss: -1.14\n",
      "disc-loss: -0.34, gan-loss: -1.14\n",
      "disc-loss: -0.34, gan-loss: -1.14\n",
      "disc-loss: -0.34, gan-loss: -1.14\n",
      "disc-loss: -0.35, gan-loss: -1.14\n",
      "disc-loss: -0.36, gan-loss: -1.14\n",
      "disc-loss: -0.36, gan-loss: -1.14\n",
      "disc-loss: -0.36, gan-loss: -1.14\n",
      "disc-loss: -0.37, gan-loss: -1.14\n",
      "disc-loss: -0.37, gan-loss: -1.14\n",
      "disc-loss: -0.37, gan-loss: -1.14\n",
      "disc-loss: -0.38, gan-loss: -1.14\n",
      "disc-loss: -0.38, gan-loss: -1.14\n",
      "disc-loss: -0.39, gan-loss: -1.14\n",
      "disc-loss: -0.39, gan-loss: -1.14\n",
      "disc-loss: -0.40, gan-loss: -1.14\n",
      "disc-loss: -0.40, gan-loss: -1.14\n",
      "disc-loss: -0.40, gan-loss: -1.14\n",
      "disc-loss: -0.41, gan-loss: -0.90\n",
      "disc-loss: -0.37, gan-loss: -0.90\n",
      "disc-loss: -0.39, gan-loss: -0.90\n",
      "disc-loss: -0.38, gan-loss: -0.90\n",
      "disc-loss: -0.39, gan-loss: -0.90\n",
      "disc-loss: -0.40, gan-loss: -0.90\n",
      "disc-loss: -0.40, gan-loss: -0.90\n",
      "disc-loss: -0.41, gan-loss: -0.90\n",
      "disc-loss: -0.41, gan-loss: -0.90\n",
      "disc-loss: -0.41, gan-loss: -0.90\n",
      "disc-loss: -0.42, gan-loss: -0.90\n",
      "disc-loss: -0.42, gan-loss: -0.90\n",
      "disc-loss: -0.42, gan-loss: -0.90\n",
      "disc-loss: -0.42, gan-loss: -0.90\n",
      "disc-loss: -0.43, gan-loss: -0.90\n",
      "disc-loss: -0.44, gan-loss: -0.90\n",
      "disc-loss: -0.44, gan-loss: -0.90\n",
      "disc-loss: -0.44, gan-loss: -0.90\n",
      "disc-loss: -0.44, gan-loss: -0.90\n",
      "disc-loss: -0.45, gan-loss: -0.90\n",
      "disc-loss: -0.45, gan-loss: -0.90\n",
      "disc-loss: -0.45, gan-loss: -0.90\n",
      "disc-loss: -0.45, gan-loss: -0.90\n",
      "disc-loss: -0.46, gan-loss: -0.90\n",
      "disc-loss: -0.46, gan-loss: -0.90\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.39, gan-loss: -0.77\n",
      "disc-loss: -0.44, gan-loss: -0.77\n",
      "disc-loss: -0.42, gan-loss: -0.77\n",
      "disc-loss: -0.43, gan-loss: -0.77\n",
      "disc-loss: -0.44, gan-loss: -0.77\n",
      "disc-loss: -0.44, gan-loss: -0.77\n",
      "disc-loss: -0.44, gan-loss: -0.77\n",
      "disc-loss: -0.45, gan-loss: -0.77\n",
      "disc-loss: -0.45, gan-loss: -0.77\n",
      "disc-loss: -0.45, gan-loss: -0.77\n",
      "disc-loss: -0.45, gan-loss: -0.77\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.46, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.47, gan-loss: -0.77\n",
      "disc-loss: -0.48, gan-loss: -0.77\n",
      "disc-loss: -0.48, gan-loss: -0.77\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.36, gan-loss: -0.73\n",
      "disc-loss: -0.45, gan-loss: -0.73\n",
      "disc-loss: -0.41, gan-loss: -0.73\n",
      "disc-loss: -0.41, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.45, gan-loss: -0.73\n",
      "disc-loss: -0.45, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.49, gan-loss: -0.73\n",
      "disc-loss: -0.49, gan-loss: -0.72\n",
      "disc-loss: -0.38, gan-loss: -0.72\n",
      "disc-loss: -0.46, gan-loss: -0.72\n",
      "disc-loss: -0.43, gan-loss: -0.72\n",
      "disc-loss: -0.44, gan-loss: -0.72\n",
      "disc-loss: -0.44, gan-loss: -0.72\n",
      "disc-loss: -0.45, gan-loss: -0.72\n",
      "disc-loss: -0.46, gan-loss: -0.72\n",
      "disc-loss: -0.45, gan-loss: -0.72\n",
      "disc-loss: -0.46, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.46, gan-loss: -0.72\n",
      "disc-loss: -0.46, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.47, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.72\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.36, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.41, gan-loss: -0.73\n",
      "disc-loss: -0.41, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.44, gan-loss: -0.73\n",
      "disc-loss: -0.45, gan-loss: -0.73\n",
      "disc-loss: -0.45, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.46, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.73\n",
      "disc-loss: -0.48, gan-loss: -0.73\n",
      "disc-loss: -0.47, gan-loss: -0.75\n",
      "disc-loss: -0.36, gan-loss: -0.75\n",
      "disc-loss: -0.44, gan-loss: -0.75\n",
      "disc-loss: -0.40, gan-loss: -0.75\n",
      "disc-loss: -0.41, gan-loss: -0.75\n",
      "disc-loss: -0.43, gan-loss: -0.75\n",
      "disc-loss: -0.43, gan-loss: -0.75\n",
      "disc-loss: -0.44, gan-loss: -0.75\n",
      "disc-loss: -0.44, gan-loss: -0.75\n",
      "disc-loss: -0.44, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.45, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.75\n",
      "disc-loss: -0.47, gan-loss: -0.75\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.37, gan-loss: -0.76\n",
      "disc-loss: -0.43, gan-loss: -0.76\n",
      "disc-loss: -0.41, gan-loss: -0.76\n",
      "disc-loss: -0.42, gan-loss: -0.76\n",
      "disc-loss: -0.42, gan-loss: -0.76\n",
      "disc-loss: -0.43, gan-loss: -0.76\n",
      "disc-loss: -0.43, gan-loss: -0.76\n",
      "disc-loss: -0.44, gan-loss: -0.76\n",
      "disc-loss: -0.44, gan-loss: -0.76\n",
      "disc-loss: -0.44, gan-loss: -0.76\n",
      "disc-loss: -0.44, gan-loss: -0.76\n",
      "disc-loss: -0.44, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.45, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.76\n",
      "disc-loss: -0.46, gan-loss: -0.78\n",
      "disc-loss: -0.33, gan-loss: -0.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11944/3785429247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisc_mod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mleadfield\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11944/2639337942.py\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(g_model, batch_size, latent_dim)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mx_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1980\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "n_epochs = 1000\n",
    "batch_size = 1024\n",
    "latent_dim = 300\n",
    "hidden_units = 300\n",
    "reg = 0.0\n",
    "gen_mod = 25\n",
    "disc_mod = 1\n",
    "g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "gan_losses = np.zeros(n_epochs)\n",
    "d_losses = np.zeros(n_epochs)\n",
    "k = 20\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    if i % disc_mod == 0:\n",
    "        y = generate_samples(g_model, batch_size, latent_dim)\n",
    "        X = (leadfield @ y.T).T\n",
    "        X, y = prep_data(X,y)\n",
    "        d_model.trainable = True\n",
    "        d_loss = d_model.train_on_batch(X, y)\n",
    "    if i % gen_mod == 0:    \n",
    "        x_input = np.random.randn(batch_size, latent_dim)\n",
    "        X = np.ones((batch_size, n_dipoles))\n",
    "        d_model.trainable = False\n",
    "        gan_loss = gan_model.train_on_batch(x_input, X)\n",
    "        \n",
    "    print(f'disc-loss: {d_loss:.2f}, gan-loss: {gan_loss:.2f}')\n",
    "    d_losses[i] = d_loss\n",
    "    \n",
    "    gan_losses[i] = gan_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ab58697df0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(gan_losses, label=\"GAN Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.05it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 401.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.95, cosine=-0.02236238867044449, r=0.17, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=(1,15), target_snr=1e99, source_number_weighting=False)\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=15, target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.01, extents=20, number_of_sources=1, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim_test.source_data], axis=0)\n",
    "\n",
    "X, y = prep_data(X, y)\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat.shape\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth Sim\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat[0].T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "from scipy.stats import pearsonr\n",
    "from esinet.evaluate import eval_auc\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_hat.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T[0], dtype=tf.float32), tf.cast(stc_hat.data.T[0], dtype=tf.float32)).numpy()\n",
    "r,p = pearsonr(stc.data.flatten(), stc_hat.data.flatten())\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.59, r=0.37, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.12393189 0.13201884 0.21535906]\n",
      "Using control points [0.         0.         0.60265697]\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "# solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(stc.data.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_mne.data.T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Discriminator with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x2ad279f2e50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "y = generate_samples(g_model, 10000, latent_dim)\n",
    "y.shape\n",
    "data = abs(y).mean(axis=0)\n",
    "stc_ = stc.copy()\n",
    "stc_.data[:, 0] = data\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = y.T\n",
    "stc_.plot(**plot_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.53, r=0.32, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.99032785 1.         1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# generated data:\n",
    "y = generate_samples(g_model, 32, latent_dim).T\n",
    "X = (leadfield @ y).T\n",
    "X, y = prep_data(X,y)\n",
    "y[np.isnan(y)] = 0\n",
    "stc_hat.data = y[:, 0, :]\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat[np.isnan(y_hat)] = 0\n",
    "\n",
    "stc_hat.data = y_hat[:, 0, :].T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "\n",
    "r,p = pearsonr(y[:, 0, :].flatten(), y_hat[:, 0, :].T.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y[:, 0, :].T, y_hat[:, 0, :])])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.50, r=0.03, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [1. 1. 1.]\n",
      "Using control points [1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\utils.py\", line 60, in safe_event\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 731, in _clean\n",
      "    self.clear_glyphs()\n",
      "  File \"c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\_brain\\_brain.py\", line 1629, in clear_glyphs\n",
      "    assert sum(len(v) for v in self.picked_points.values()) == 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "\n",
    "evoked = mne.EvokedArray(X[:, 0, :].T, info)\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "stc_mne = solver.apply_inverse_operator(evoked)\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(y[:, 0, :].flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y[:, 0, :].T, stc_mne.data .T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "basic_sources = np.identity(n_dipoles)\n",
    "basic_sources = laplace_operator @ basic_sources\n",
    "basic_sources = np.concatenate([basic_sources, -1*basic_sources], axis=1)\n",
    "basic_eeg = leadfield @ basic_sources\n",
    "print(basic_sources.shape, basic_eeg.shape)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    g_model = tf.keras.Sequential()\n",
    "    input_shape = (None, latent_dim)\n",
    "    g_model.add(InputLayer(input_shape=input_shape))\n",
    "    g_model.add(Dense(latent_dim, name=\"HL1\"))\n",
    "    g_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    # g_model.build()\n",
    "    # g_model.compile(optimizer='adam', loss=\"mse\")\n",
    "    # g_model.summary()\n",
    "    return g_model\n",
    "    \n",
    "def define_discriminator(hidden_units=100):\n",
    "    input_shape = (None, n_chans)\n",
    "    d_model = tf.keras.Sequential()\n",
    "    d_model.add(InputLayer(input_shape=input_shape))\n",
    "    d_model.add(Dense(hidden_units, name=\"HL1\"))\n",
    "    d_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    d_model.build()\n",
    "    d_model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "    # d_model.summary()\n",
    "    return d_model\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    input_shape = (None, latent_dim)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(g_model.output)\n",
    "    print(lam)\n",
    "    discriminator = d_model(lam)\n",
    "    model = tf.keras.Model(inputs=g_model.input, outputs=[d_model.output, g_model.output], name='Contextualizer')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(g_model)\n",
    "    # model.add(Lambda(lambda x: tf.linalg.matmul(leadfield_, x)))\n",
    "    # model.add(d_model)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
