{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 58.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x2ab6c8af910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(yy / np.max(abs(yy))) for yy in y], axis=0)\n",
    "    return X, y\n",
    "\n",
    "# settings = dict(duration_of_trial=0.25, method=\"noise\", target_snr=1e99)\n",
    "# settings = dict(duration_of_trial=0.25, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.25, extents=1, number_of_sources=2, target_snr=1e99)\n",
    "\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=2)\n",
    "X = np.stack([eeg.average().data for eeg in sim.eeg_data], axis=0)\n",
    "y = np.stack([source.data for source in sim.source_data], axis=0)\n",
    "X, y = prep_data(X, y)\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "y = np.swapaxes(y, 1, 2)\n",
    "sim.source_data[0].plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, GRU, multiply, Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dropout, ActivityRegularization, TimeDistributed, Reshape, Permute, GaussianNoise, add\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "def data_loss(leadfield, lam_0=0.1):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    def batch_data_loss(y_true, y_est):\n",
    "        def d_loss(y_true, y_est):\n",
    "            y_true_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_true)))\n",
    "            y_est_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "            # print(\"y_true \", y_true)\n",
    "            # print(\"y_est \", y_est)\n",
    "            \n",
    "            # return K.mean(K.square(y_est - y_true))\n",
    "            # error_source = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est, y_true)\n",
    "            # error_eeg = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            error_source =  K.mean(K.square(y_est - y_true))\n",
    "            error_eeg =  K.mean(K.square(y_est_eeg - y_true_eeg))\n",
    "            \n",
    "            return (error_source*lam_0 + error_eeg) / (1 + lam_0)\n",
    "        \n",
    "        \n",
    "\n",
    "        batched_losses = tf.map_fn(lambda x:\n",
    "            d_loss(x[0], x[1]), \n",
    "            (y_true, y_est), dtype=tf.float32)\n",
    "        return K.mean(batched_losses)\n",
    "\n",
    "\n",
    "    return batch_data_loss\n",
    "\n",
    "\n",
    "def consistency(x):\n",
    "    # error = K.mean(tf.experimental.numpy.diff(K.abs(x), axis=1))\n",
    "    # error = K.mean(K.abs(x[:,1:])-K.abs(x[:,:-1]))\n",
    "    error = K.mean(K.abs(x[:,1:]-x[:,:-1]))\n",
    "    # print(\"error: \", error)\n",
    "    return error\n",
    "\n",
    "def c_loss(sources):\n",
    "    matrix = 1-compute_cosine_distances(K.abs(sources), K.abs(sources))\n",
    "    return K.mean(matrix)\n",
    "\n",
    "\n",
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,-1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,-1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "    # return K.mean(K.pow(K.abs(x), 0.5))\n",
    "    \n",
    "def l1l2(x):\n",
    "    return K.mean((K.abs(x))) / K.mean((K.square(x)))\n",
    "\n",
    "\n",
    "def get_model(name=\"Model\", n_dense_layers=2, n_lstm_units=64, hidden_units=300, learning_rate=0.001, lam_0=0., lam_1=1, lam_2=0.001):\n",
    "    input_shape = (None, n_chans)\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=name)\n",
    "    fc = TimeDistributed(Dense(hidden_units, activation=\"tanh\", name=\"FC1\"))(inputs)\n",
    "    source = TimeDistributed(Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\"))(fc)\n",
    "\n",
    "    gru = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='GRU_Discriminator'))(inputs)\n",
    "    source_time = TimeDistributed(Dense(n_dipoles, activation=\"sigmoid\", name=\"Mask\"))(gru)    \n",
    "\n",
    "    out = multiply([source_time, source])\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name=name)\n",
    "    \n",
    "    # Data Loss\n",
    "    # Spatial L1 Regularization\n",
    "    model.add_loss(l1_sparsity(out)*lam_1)\n",
    "    model.add_loss(l1l2(out)*lam_1)\n",
    "    \n",
    "    # Temporal L2 Regularization\n",
    "    model.add_loss(consistency(out)*lam_2)\n",
    "    \n",
    "    model.compile(loss=data_loss(leadfield, lam_0=lam_0), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "lams_0 = [0.0, ]  # source-based\n",
    "lams_1 = [0, ]  # sparsity\n",
    "lams_2 = [0.1, ]  # temporal context\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "\n",
    "y_ = np.zeros(y.shape[1:])\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=epochs/10, min_delta=0.01, monitor=\"loss\", restore_best_weights=True)]\n",
    "models = []\n",
    "hists = []\n",
    "for lam_0, lam_1, lam_2 in zip(lams_0, lams_1, lams_2):\n",
    "    model = get_model(lam_0=lam_0, lam_1=lam_1, lam_2=lam_2, learning_rate=learning_rate, name=f\"Lam_{lam_1}\")\n",
    "    hist = model.fit(X[0][np.newaxis], y[0][np.newaxis], epochs=epochs, batch_size=8, callbacks=callbacks, verbose=0)\n",
    "    models.append(model)\n",
    "    hists.append(hist)\n",
    "\n",
    "%matplotlib qt\n",
    "plt.figure()\n",
    "for hist in hists:\n",
    "    plt.loglog(hist.history[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "AUC:  0.09459999999999999  Corrs:  0.00022137156543629695  nMSE:  9.468034898431503e-05 Spatial Sparsity:  tf.Tensor(0.001018151, shape=(), dtype=float32) Temporal Sparsity:  tf.Tensor(0.0019766116, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.00361926 0.00871878 0.17455739]\n",
      "Using control points [0.00000000e+00 0.00000000e+00 2.02772345e-08]\n"
     ]
    }
   ],
   "source": [
    "from esinet.evaluate import eval_auc\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "idx = 0\n",
    "times = [0, 0.10, 0.20]\n",
    "stc = sim.source_data[idx].copy()\n",
    "stc.plot(**plot_params)\n",
    "evoked = sim.eeg_data[idx].average()\n",
    "evoked.plot_joint(title=\"True\", times=times)\n",
    "for mod in models:\n",
    "    src_hat = mod.predict(X)[idx]\n",
    "    stc_hat = stc.copy()\n",
    "    stc_hat.data = src_hat.T\n",
    "    stc_hat.plot(**plot_params, brain_kwargs=dict(title=mod.name))\n",
    "\n",
    "    evoked_ = util.get_eeg_from_source(stc_hat, fwd, info, tmin=evoked.tmin)\n",
    "    evoked_.plot_joint(title=f\"Predicted {mod.name}\", times=times)\n",
    "\n",
    "    auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    spatial_sparsity = l1_sparsity(stc_hat.data[np.newaxis])\n",
    "    temporal_sparsity = consistency(stc_hat.data[np.newaxis])\n",
    "    print(\"AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses, \"Spatial Sparsity: \", spatial_sparsity, \"Temporal Sparsity: \", temporal_sparsity)\n",
    "    mat = compute_cosine_distances(abs(stc_hat.data.T), abs(stc_hat.data.T)).numpy()\n",
    "    plt.figure()\n",
    "    plt.imshow(mat)\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"{mat.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'esinet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9832/3080999128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../../invert/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0minvert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum_norm_estimates\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSolverMinimumL1Norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSolverMinimumL1Norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_inverse_operator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstc_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_inverse_operator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevoked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\Dokumente\\projects\\esinet\\dev\\../../invert\\invert\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lukas\\Dokumente\\projects\\esinet\\dev\\../../invert\\invert\\invert.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmne\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_from_forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mesinet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'esinet'"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, \"../../invert/\")\n",
    "from invert.solvers.minimum_norm_estimates import SolverMinimumL1Norm\n",
    "solver = SolverMinimumL1Norm()\n",
    "solver.make_inverse_operator(fwd)\n",
    "stc_ = solver.apply_inverse_operator(evoked)\n",
    "stc_.plot(**plot_params, brain_kwargs=dict(title=\"L1-FISTA\"))\n",
    "\n",
    "auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_.data.T)])\n",
    "corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_.data.T)])\n",
    "nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_.data.T)])\n",
    "spatial_sparsity = l1_sparsity(stc_.data[np.newaxis])\n",
    "temporal_sparsity = consistency(stc_.data[np.newaxis])\n",
    "print(\"AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses, \"Spatial Sparsity: \", spatial_sparsity, \"Temporal Sparsity: \", temporal_sparsity)\n",
    "mat = compute_cosine_distances(abs(stc_.data.T), abs(stc_.data.T)).numpy()\n",
    "plt.figure()\n",
    "plt.imshow(mat)\n",
    "plt.colorbar()\n",
    "plt.title(f\"{mat.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "plt.figure()\n",
    "plt.imshow(compute_cosine_distances(abs(stc_hat.data.T), abs(stc_hat.data.T)).numpy())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(compute_cosine_distances(abs(stc.data.T), abs(stc.data.T)).numpy())\n",
    "plt.colorbar()\n",
    "\n",
    "# %%\n",
    "plt.figure()\n",
    "plt.hist(compute_cosine_distances(abs(stc_hat.data.T), abs(stc_hat.data.T)).numpy().flatten())\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(compute_cosine_distances(abs(stc.data.T), abs(stc.data.T)).numpy().flatten())\n",
    "\n",
    "# %%\n",
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping, SolverMinimumNorm\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "# solver = SolverMinimumNorm().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "\n",
    "print(model.name, \" AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
