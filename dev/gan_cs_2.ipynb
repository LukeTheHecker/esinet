{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    2.1s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.1s remaining:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "pos_left = fwd[\"src\"][0][\"rr\"][fwd[\"src\"][0][\"vertno\"], :]\n",
    "pos_right = fwd[\"src\"][1][\"rr\"][fwd[\"src\"][1][\"vertno\"], :]\n",
    "pos = np.concatenate((pos_left, pos_right), axis=0)\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization\n",
    "from tensorflow.keras.layers import Activation, Dropout, ActivityRegularization, TimeDistributed\n",
    "from tensorflow.keras.layers import Reshape, Permute, GaussianNoise, add, BatchNormalization, multiply\n",
    "from esinet.losses import chamfer2\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def weightedLoss(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"DISC\", true, pred)\n",
    "        \n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error / tf.linalg.norm(true, axis=-1)\n",
    "        return error\n",
    "    return loss\n",
    "\n",
    "def weightedLossGan(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"GAN\", true, pred)\n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error * tf.linalg.norm(true)\n",
    "        return -error\n",
    "    return loss\n",
    "\n",
    "\n",
    "def square(x, exponent=4):\n",
    "    return K.pow(x, exponent) * K.sign(x)\n",
    "    \n",
    "def scale_act(x):\n",
    "    return tf.transpose(tf.transpose(x) / tf.math.reduce_max(K.abs(x), axis=-1))\n",
    "        \n",
    "def data_loss(leadfield, lam_0=0.1):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    def batch_data_loss(y_true, y_est):\n",
    "        def d_loss(y_true, y_est):\n",
    "            y_true_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_true)))\n",
    "            y_est_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "            # print(\"y_true \", y_true)\n",
    "            # print(\"y_est \", y_est)\n",
    "            \n",
    "            # return K.mean(K.square(y_est - y_true))\n",
    "            error_source = tf.keras.losses.CosineSimilarity(name=\"Source_Data_Cosine_Loss\")(y_est, y_true)\n",
    "            error_eeg = tf.keras.losses.CosineSimilarity(name=\"EEG_Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            return error_source*lam_0 + error_eeg\n",
    "        \n",
    "        \n",
    "\n",
    "        batched_losses = tf.map_fn(lambda x:\n",
    "            d_loss(x[0], x[1]), \n",
    "            (y_true, y_est), dtype=tf.float32)\n",
    "        return K.mean(batched_losses)\n",
    "\n",
    "\n",
    "    return batch_data_loss\n",
    "\n",
    "def c_loss(x):\n",
    "        matrix = compute_cosine_distances(K.abs(x), K.abs(x))\n",
    "        return K.mean(matrix)\n",
    "\n",
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim, lam_0=0.1, lam_1=1, lam_2=0.001, learning_rate=0.0001):\n",
    "    n_chans, n_dipoles = leadfield.shape\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    output_1 = g_model(inputs)\n",
    "    eeg = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(output_1)\n",
    "    \n",
    "    # print(\"eeg from source from g: \", eeg)\n",
    "    eeg_normed = LayerNormalization()(eeg)\n",
    "    \n",
    "    # print(\"eeg_normed from source from g: \", eeg_normed)\n",
    "    output_3 = d_model(eeg_normed)\n",
    "    model = Model(inputs, [output_1, output_3])\n",
    "\n",
    "    # Add loss to model\n",
    "    eeg_hat = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(output_3)\n",
    "    source_loss = K.abs(tf.keras.losses.CosineSimilarity()(output_1, output_3)) \n",
    "    eeg_loss = K.abs(tf.keras.losses.CosineSimilarity()(eeg, eeg_hat)) \n",
    "    loss = source_loss + eeg_loss * lam_0\n",
    "    model.add_loss(loss)\n",
    "    model.add_loss(-l1_sparsity(output_3) * lam_1)\n",
    "    # temporal consistency\n",
    "    model.add_loss(K.abs(c_loss(output_3) * lam_2))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def define_discriminator(n_dense_layers=2, hidden_units=200, learning_rate=0.0001, lam_0=0.1, lam_1=1, lam_2=0.001):\n",
    "    input_shape = (None, n_chans)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name='Input_Discriminator')\n",
    "    activation = \"tanh\"\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "\n",
    "    # fc = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs)\n",
    "    # for i in range(n_dense_layers-1):\n",
    "    #     fc = Dense(hidden_units, activation=activation, name=f\"HL_D{i+2}\")(fc)\n",
    "    # out = Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\")(fc)\n",
    "\n",
    "    fc1 = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs)\n",
    "    fc2 = Dense(hidden_units, activation=activation, name=\"HL_D2\")(fc1)\n",
    "    adder = add([fc1, fc2])\n",
    "\n",
    "    fc3 = Dense(hidden_units, activation=activation, name=\"HL_D3\")(adder)\n",
    "    fc4 = Dense(hidden_units, activation=activation, name=\"HL_D4\")(fc3)\n",
    "    adder = add([fc3, fc4])\n",
    "    out = Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\")(adder)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name='Discriminator')\n",
    "    # sparsity\n",
    "    model.add_loss(l1_sparsity(out) * lam_1)\n",
    "    # temporal consistency\n",
    "    model.add_loss(c_loss(out) * lam_2)\n",
    "    model.compile(loss=data_loss(leadfield, lam_0=lam_0), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    \n",
    "    # model.compile(loss=chamfer2(pos), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "  \n",
    "\n",
    "def define_generator(latent_dim=100, sparsity=1):\n",
    "    laplace_operator_ = tf.cast(laplace_operator, dtype=tf.float32)\n",
    "\n",
    "    lrelu = tf.keras.layers.LeakyReLU()\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    fc = Dense(latent_dim, activation=lrelu, name=\"HL_G1\")(inputs)\n",
    "    gen_out_sparse = Dense(n_dipoles, name=\"Output_Generator\", activation=\"tanh\")(fc)\n",
    "    \n",
    "    \n",
    "    # fc2 = Dense(hidden_units, activation=lrelu, name=\"HL_D2\")(fc1)\n",
    "    # adder = add([fc1, fc2])\n",
    "\n",
    "    # fc3 = Dense(hidden_units, activation=lrelu, name=\"HL_D3\")(adder)\n",
    "    # fc4 = Dense(hidden_units, activation=lrelu, name=\"HL_D4\")(fc3)\n",
    "    # adder = add([fc3, fc4])\n",
    "    # gen_out_sparse = Dense(n_dipoles, name=\"Output_Generator\", activation=\"tanh\")(adder)\n",
    "    gen_out_sparse = Activation(square)(gen_out_sparse)\n",
    "    gen_out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(None, n_chans))(gen_out_sparse)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=gen_out, name='Generator')\n",
    "    model.add_loss(l1_sparsity( gen_out_sparse )*sparsity)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "\n",
    "def batch_diversity(x):\n",
    "    diversity = K.std(K.mean(K.abs(x), axis=0))\n",
    "    return diversity\n",
    "\n",
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y\n",
    "    \n",
    "def prep_data_sim(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "\n",
    "def generate_samples(g_model, batch_size, latent_dim):\n",
    "    x_input = np.random.randn(batch_size, latent_dim)\n",
    "    sources = g_model.predict(x_input)\n",
    "    return sources\n",
    "\n",
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 32\n",
    "batch_number = 10\n",
    "latent_dim = 16\n",
    "# define_discriminator2()\n",
    "# g, d, gan = define_models(latent_dim, hidden_units=latent_dim)\n",
    "# gan(np.random.randn(batch_size, latent_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 238.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 24989.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 606.04it/s]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(duration_of_trial=0.01, extents=(1, 40), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=100)\n",
    "X_test = np.stack([eeg.average().data[:, 0] for eeg in sim.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data[:, 0] for source in sim.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 64), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (None, 64).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b0f0f10774ac3a505ccc59bfb5319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Discriminator\n",
      "Train Generator\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 64), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (256, 64).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 64), dtype=tf.float32, name='Input_Discriminator'), name='Input_Discriminator', description=\"created by layer 'Input_Discriminator'\"), but it was called on an input with incompatible shape (256, 64).\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.59\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.41\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.23\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.15\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.11\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.10\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.06\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.08\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.05\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.09\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.99, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.17, disc-loss: -0.98, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.98, gan-loss: 0.07\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.98, gan-loss: 0.05\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.98, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.97, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.97, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.97, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.18, disc-loss: -0.98, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.98, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.98, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.05\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -0.99, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.19, disc-loss: -1.00, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.00, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.00, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.00, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.04\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.20, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.03\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.01, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.21, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.02, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.02\n",
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.22, disc-loss: -1.03, gan-loss: 0.01\n",
      "Train Discriminator\n",
      "Train Generator\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1644/4267162364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mgan_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;31m# print(f\"\\tintermediate gan_loss: {gan_loss}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1823\u001b[0m                                                     class_weight)\n\u001b[0;32m   1824\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1825\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "n_epochs = 10000\n",
    "batch_size = 256\n",
    "latent_dim = 100\n",
    "hidden_units = 400\n",
    "disc_mod = 1\n",
    "gen_mod = 1\n",
    "reps = 5\n",
    "learning_rate = 0.001\n",
    "sparsity = 100\n",
    "\n",
    "lam_0 = 0.4 # source_loss\n",
    "lam_1 = 1 # sparsity\n",
    "lam_2 = 0.0 # context\n",
    "history_size = batch_size*5\n",
    "\n",
    "# g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg, learning_rate=0.01)\n",
    "\n",
    "# d_model = define_discriminator(learning_rate=learning_rate, hidden_units=hidden_units, lam_0=lam_0, lam_1=lam_1, lam_2=lam_2)\n",
    "g_model = define_generator(latent_dim=latent_dim, sparsity=sparsity)\n",
    "gan_model = define_gan(g_model, d_model, latent_dim, learning_rate=learning_rate, lam_0=lam_0, lam_1=lam_1, lam_2=lam_2)\n",
    "\n",
    "n_time = 1\n",
    "X_history = np.zeros((0, n_time, n_chans))\n",
    "y_history = np.zeros((0, n_time, n_dipoles))\n",
    "gan_losses = np.zeros(n_epochs)\n",
    "d_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "generated = []\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    if i % disc_mod == 0:\n",
    "        print(\"Train Discriminator\")\n",
    "        y = generate_samples(g_model, batch_size, latent_dim)\n",
    "        X = (leadfield @ y.T).T\n",
    "        X, _ = prep_data(X,y)\n",
    "        # Add temporal dimension\n",
    "        X = X[:, np.newaxis]\n",
    "        y = y[:, np.newaxis]\n",
    "\n",
    "        generated.append(abs(y).mean(axis=0))\n",
    "        X_history = np.append(X_history, X, axis=0)\n",
    "        y_history = np.append(y_history, y, axis=0)\n",
    "        idc = np.arange(X_history.shape[0])\n",
    "        np.random.shuffle(idc)\n",
    "        X_history = X_history[idc]\n",
    "        y_history = y_history[idc]\n",
    "        \n",
    "        if X_history.shape[0]>history_size:\n",
    "            X_history = X_history[:history_size]\n",
    "            y_history = y_history[:history_size]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for _ in range(1):\n",
    "            d_loss = d_model.train_on_batch(X_history[:batch_size], y_history[:batch_size])\n",
    "        \n",
    "        # d_loss = d_model.train_on_batch(X, y)\n",
    "        d_model.trainable = True\n",
    "        # test_loss = d_model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_loss = tf.keras.losses.CosineSimilarity()(tf.cast(y_test, dtype=tf.float32), tf.cast(d_model.predict(X_test), dtype=tf.float32)).numpy()\n",
    "        test_losses[i] = test_loss\n",
    "\n",
    "\n",
    "\n",
    "    if i % gen_mod == 0:# and d_loss<-0.15:\n",
    "        g_model = define_generator(latent_dim=latent_dim, sparsity=sparsity)\n",
    "        gan_model.layers[1] = g_model\n",
    "        print(\"Train Generator\")\n",
    "        x_input = np.random.randn(batch_size, latent_dim)\n",
    "\n",
    "        X = np.ones((batch_size, n_dipoles))\n",
    "        d_model.trainable = False\n",
    "        gan_model.layers[4].trainable = False\n",
    "        for _ in range(15):\n",
    "            gan_loss = gan_model.train_on_batch(x_input, X)\n",
    "            # print(f\"\\tintermediate gan_loss: {gan_loss}\")\n",
    "        d_model.trainable = True\n",
    "        gan_model.layers[4].trainable = True\n",
    "      \n",
    "    print(f'test_loss: {test_loss:.2f}, disc-loss: {d_loss:.2f}, gan-loss: {gan_loss:.2f}')\n",
    "    d_losses[i] = d_loss\n",
    "    \n",
    "    gan_losses[i] = gan_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.046016186 Cos:  tf.Tensor(-0.09968847, shape=(), dtype=float32)\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\evoked.py:521: UserWarning: Attempting to set identical left == right == 0.0 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(xlim)\n",
      "c:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\mne\\viz\\evoked.py:521: UserWarning: Attempting to set identical left == right == 0.0 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(xlim)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x302 with 5 Axes>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.43759244 0.53018656 1.92419425]\n"
     ]
    }
   ],
   "source": [
    "stc_ = sim.source_data[0].copy()\n",
    "noise = np.random.randn(1, latent_dim)\n",
    "src = g_model(noise).numpy().T\n",
    "eeg = leadfield @ src\n",
    "eeg -= eeg.mean()\n",
    "eeg /= eeg.std()\n",
    "src_hat = d_model(eeg.T[np.newaxis]).numpy()[0].T\n",
    "\n",
    "print(\"MSE: \", np.mean((src-src_hat)**2), \"Cos: \", tf.keras.losses.CosineSimilarity()(src, src_hat))\n",
    "\n",
    "\n",
    "stc_.data = src\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = src_hat\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "evoked = mne.EvokedArray(eeg, info)\n",
    "evoked.plot_joint(title=\"True\")\n",
    "\n",
    "evoked_hat = util.get_eeg_from_source(stc_, fwd, info, tmin=evoked.tmin)\n",
    "evoked_hat.plot_joint(title=\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x22bfbb41ac0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stc = sim.source_data[0].copy()\n",
    "stc_ = stc.copy()\n",
    "stc_.data = np.stack(generated, axis=0).T\n",
    "stc_.plot(**plot_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Diversity:  0.02836658 L1:  0.104076944\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(gan_losses, label=\"GAN Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# # g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "y = generate_samples(g_model, 1000, latent_dim)\n",
    "data = abs(y).mean(axis=0)\n",
    "stc_ = stc.copy()\n",
    "stc_.data[:, 0] = data\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = y.T\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y.flatten())\n",
    "print(\"Batch Diversity: \", batch_diversity(y).numpy(), \"L1: \", l1_sparsity(y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 16.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1986.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 181.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.71, cosine=-0.35133638978004456, r=0.37, p=0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.        0.0310662 0.218538 ]\n",
      "Using control points [0.08220464 0.09563892 0.16821336]\n",
      "Using control points [0.        0.0310662 0.218538 ]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=(1,15), target_snr=1e99, source_number_weighting=False)\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,25), number_of_sources=15, target_snr=1e99)\n",
    "# settings = dict(duration_of_trial=0.01, extents=30, number_of_sources=1, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim_test.source_data], axis=0)\n",
    "\n",
    "X, y = prep_data(X[0].T, y[0].T)\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat.shape\n",
    "stc = sim_test.source_data[0]\n",
    "stc.data /= np.max(abs(stc.data))\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth Sim\"), clim=dict(kind='percent', pos_lims=(0, 50, 100)))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "from scipy.stats import pearsonr\n",
    "from esinet.evaluate import eval_auc\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_hat.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_hat.data.T, dtype=tf.float32)).numpy()\n",
    "r,p = pearsonr(stc.data.flatten(), stc_hat.data.flatten())\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.89, cosine=-0.4212958812713623, r=0.43, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping, SolverMinimumNorm\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "# solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "solver = SolverMinimumNorm().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(stc.data.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_mne.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_mne.data.T, dtype=tf.float32)).numpy()\n",
    "\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Discriminator with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.91, r=0.84, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generated data:\n",
    "y = generate_samples(g_model, 32, latent_dim)\n",
    "X = (leadfield @ y.T).T\n",
    "X, y = prep_data(X,y)\n",
    "\n",
    "y[np.isnan(y)] = 0\n",
    "stc_hat.data = y.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat[np.isnan(y_hat)] = 0\n",
    "\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "r,p = pearsonr(y.flatten(), y_hat.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y, y_hat)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.51, r=-0.01, p=0.0427\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.02927872 0.03322611 0.06774781]\n",
      "Using control points [0.42319688 0.4607384  0.93013235]\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "evoked = mne.EvokedArray(X.T, info)\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "# solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(evoked)\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(y.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y, stc_mne.data.T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "basic_sources = np.identity(n_dipoles)\n",
    "basic_sources = laplace_operator @ basic_sources\n",
    "basic_sources = np.concatenate([basic_sources, -1*basic_sources], axis=1)\n",
    "basic_eeg = leadfield @ basic_sources\n",
    "print(basic_sources.shape, basic_eeg.shape)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    g_model = tf.keras.Sequential()\n",
    "    input_shape = (None, latent_dim)\n",
    "    g_model.add(InputLayer(input_shape=input_shape))\n",
    "    g_model.add(Dense(latent_dim, name=\"HL1\"))\n",
    "    g_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    # g_model.build()\n",
    "    # g_model.compile(optimizer='adam', loss=\"mse\")\n",
    "    # g_model.summary()\n",
    "    return g_model\n",
    "    \n",
    "def define_discriminator(hidden_units=100):\n",
    "    input_shape = (None, n_chans)\n",
    "    d_model = tf.keras.Sequential()\n",
    "    d_model.add(InputLayer(input_shape=input_shape))\n",
    "    d_model.add(Dense(hidden_units, name=\"HL1\"))\n",
    "    d_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    d_model.build()\n",
    "    d_model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "    # d_model.summary()\n",
    "    return d_model\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    input_shape = (None, latent_dim)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(g_model.output)\n",
    "    print(lam)\n",
    "    discriminator = d_model(lam)\n",
    "    model = tf.keras.Model(inputs=g_model.input, outputs=[d_model.output, g_model.output], name='Contextualizer')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(g_model)\n",
    "    # model.add(Lambda(lambda x: tf.linalg.matmul(leadfield_, x)))\n",
    "    # model.add(d_model)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
