{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "sys.path.insert(1, \"../../invert\")\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)\n",
    "from invert.solvers.empirical_bayes import SolverChampagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.1s remaining:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 137.61it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 12137.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 20) (1284, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 296.64it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,20), target_snr=99999)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc Champagnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fb183e435944c4a84c751951d01c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "solver = SolverChampagne()\n",
    "solver.make_inverse_operator(fwd)\n",
    "stcs_champagne = [solver.apply_inverse_operator(eeg.average()) for eeg in tqdm(sim.eeg_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20, 61) (1000, 20, 1284)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "y = np.squeeze(np.stack([src.data for src in stcs_champagne]))\n",
    "y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "X = np.swapaxes(X, 1,2)\n",
    "y = np.swapaxes(y, 1,2)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        loss1 = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        loss2 = sparsity(None, y_pred)\n",
    "        return loss1 + loss2 * 1e-3\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, None, 150)    9300        ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, None, 256)    146688      ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 150)    0           ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 150)    38550       ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 150)    0           ['dropout[0][0]',                \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, None, 1284)   193884      ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 388,422\n",
      "Trainable params: 388,422\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 5s 38ms/step - loss: -0.0183 - val_loss: -0.0385\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 1s 19ms/step - loss: -0.0643 - val_loss: -0.0571\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.0887 - val_loss: -0.0678\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 16ms/step - loss: -0.1067 - val_loss: -0.0769\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 1s 20ms/step - loss: -0.1213 - val_loss: -0.0835\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.1328 - val_loss: -0.0890\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.1439 - val_loss: -0.0933\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.1540 - val_loss: -0.0967\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.1636 - val_loss: -0.0994\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.1734 - val_loss: -0.1025\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.1827 - val_loss: -0.1048\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 1s 21ms/step - loss: -0.1922 - val_loss: -0.1072\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2011 - val_loss: -0.1091\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2100 - val_loss: -0.1106\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2183 - val_loss: -0.1126\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2267 - val_loss: -0.1130\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: -0.2346 - val_loss: -0.1141\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2419 - val_loss: -0.1155\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2489 - val_loss: -0.1153\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2555 - val_loss: -0.1156\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: -0.2618 - val_loss: -0.1163\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 1s 34ms/step - loss: -0.2675 - val_loss: -0.1159\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 16ms/step - loss: -0.2732 - val_loss: -0.1168\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2781 - val_loss: -0.1158\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2822 - val_loss: -0.1156\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2868 - val_loss: -0.1161\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.2912 - val_loss: -0.1154\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: -0.2946 - val_loss: -0.1163\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.2978 - val_loss: -0.1169\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3012 - val_loss: -0.1155\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3047 - val_loss: -0.1160\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3071 - val_loss: -0.1156\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3094 - val_loss: -0.1160\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3124 - val_loss: -0.1151\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3144 - val_loss: -0.1153\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3169 - val_loss: -0.1137\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3188 - val_loss: -0.1144\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3211 - val_loss: -0.1141\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3230 - val_loss: -0.1136\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3240 - val_loss: -0.1139\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3259 - val_loss: -0.1133\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3276 - val_loss: -0.1125\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3292 - val_loss: -0.1129\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3305 - val_loss: -0.1129\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3319 - val_loss: -0.1129\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3332 - val_loss: -0.1115\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3347 - val_loss: -0.1127\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3359 - val_loss: -0.1125\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3370 - val_loss: -0.1110\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3382 - val_loss: -0.1116\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3398 - val_loss: -0.1110\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3409 - val_loss: -0.1103\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3416 - val_loss: -0.1109\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3427 - val_loss: -0.1103\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: -0.3433 - val_loss: -0.1099\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 0s 17ms/step - loss: -0.3446 - val_loss: -0.1103\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3458 - val_loss: -0.1104\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 1s 20ms/step - loss: -0.3461 - val_loss: -0.1104\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 0s 17ms/step - loss: -0.3471 - val_loss: -0.1103\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 1s 19ms/step - loss: -0.3482 - val_loss: -0.1098\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 0s 17ms/step - loss: -0.3491 - val_loss: -0.1091\n",
      "Epoch 62/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3498 - val_loss: -0.1095\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3509 - val_loss: -0.1088\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3514 - val_loss: -0.1091\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3516 - val_loss: -0.1085\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3531 - val_loss: -0.1090\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3539 - val_loss: -0.1081\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3543 - val_loss: -0.1072\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3547 - val_loss: -0.1077\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3556 - val_loss: -0.1076\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3561 - val_loss: -0.1081\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3569 - val_loss: -0.1071\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3578 - val_loss: -0.1074\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3580 - val_loss: -0.1078\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3589 - val_loss: -0.1067\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3598 - val_loss: -0.1075\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3595 - val_loss: -0.1077\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3607 - val_loss: -0.1070\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3604 - val_loss: -0.1065\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 1s 20ms/step - loss: -0.3617 - val_loss: -0.1066\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3625 - val_loss: -0.1064\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3629 - val_loss: -0.1063\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3631 - val_loss: -0.1063\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 0s 17ms/step - loss: -0.3637 - val_loss: -0.1057\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3641 - val_loss: -0.1061\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: -0.3644 - val_loss: -0.1055\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3650 - val_loss: -0.1052\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3662 - val_loss: -0.1050\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3658 - val_loss: -0.1056\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 0s 16ms/step - loss: -0.3664 - val_loss: -0.1049\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3674 - val_loss: -0.1040\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3673 - val_loss: -0.1046\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 0s 18ms/step - loss: -0.3676 - val_loss: -0.1043\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3681 - val_loss: -0.1045\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3688 - val_loss: -0.1045\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3697 - val_loss: -0.1037\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3695 - val_loss: -0.1035\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: -0.3700 - val_loss: -0.1046\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3707 - val_loss: -0.1035\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: -0.3711 - val_loss: -0.1038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a68e1bfb20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 150\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "dropout = 0.1\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.3, dtype=tf.float32)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(inputs)\n",
    "mask = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "# mask = Activation(threshold_activation)(mask)\n",
    "\n",
    "multi = multiply([fc1, mask], name=\"multiply\")\n",
    "final_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(multi)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=final_out, name='Contextualizer')\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "# model.compile(loss=custom_loss(), optimizer=\"adam\", metrics=[tf.keras.losses.CosineSimilarity(), sparsity])\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[tf.keras.losses.CosineSimilarity(), sparsity])\n",
    "\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, None, 61)]        0         \n",
      "                                                                 \n",
      " FC1 (TimeDistributed)       (None, None, 300)         18600     \n",
      "                                                                 \n",
      " FC2 (TimeDistributed)       (None, None, 1284)        386484    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 405,084\n",
      "Trainable params: 405,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "266/266 [==============================] - 3s 9ms/step - loss: -0.0923 - val_loss: -0.1197\n",
      "Epoch 2/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1322 - val_loss: -0.1336\n",
      "Epoch 3/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1426 - val_loss: -0.1389\n",
      "Epoch 4/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1476 - val_loss: -0.1408\n",
      "Epoch 5/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1499 - val_loss: -0.1422\n",
      "Epoch 6/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1514 - val_loss: -0.1432\n",
      "Epoch 7/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1524 - val_loss: -0.1436\n",
      "Epoch 8/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1532 - val_loss: -0.1444\n",
      "Epoch 9/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1539 - val_loss: -0.1446\n",
      "Epoch 10/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1544 - val_loss: -0.1452\n",
      "Epoch 11/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1552 - val_loss: -0.1451\n",
      "Epoch 12/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1558 - val_loss: -0.1458\n",
      "Epoch 13/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1565 - val_loss: -0.1465\n",
      "Epoch 14/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1570 - val_loss: -0.1466\n",
      "Epoch 15/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1578 - val_loss: -0.1471\n",
      "Epoch 16/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1586 - val_loss: -0.1476\n",
      "Epoch 17/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1594 - val_loss: -0.1485\n",
      "Epoch 18/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1603 - val_loss: -0.1493\n",
      "Epoch 19/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1610 - val_loss: -0.1499\n",
      "Epoch 20/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1619 - val_loss: -0.1505\n",
      "Epoch 21/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1628 - val_loss: -0.1511\n",
      "Epoch 22/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1637 - val_loss: -0.1520\n",
      "Epoch 23/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1647 - val_loss: -0.1527\n",
      "Epoch 24/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1657 - val_loss: -0.1533\n",
      "Epoch 25/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1667 - val_loss: -0.1542\n",
      "Epoch 26/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1678 - val_loss: -0.1552\n",
      "Epoch 27/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1687 - val_loss: -0.1560\n",
      "Epoch 28/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1698 - val_loss: -0.1569\n",
      "Epoch 29/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1707 - val_loss: -0.1576\n",
      "Epoch 30/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1717 - val_loss: -0.1581\n",
      "Epoch 31/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1728 - val_loss: -0.1592\n",
      "Epoch 32/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1738 - val_loss: -0.1596\n",
      "Epoch 33/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1747 - val_loss: -0.1606\n",
      "Epoch 34/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1758 - val_loss: -0.1614\n",
      "Epoch 35/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1766 - val_loss: -0.1620\n",
      "Epoch 36/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1776 - val_loss: -0.1623\n",
      "Epoch 37/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1784 - val_loss: -0.1636\n",
      "Epoch 38/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1793 - val_loss: -0.1638\n",
      "Epoch 39/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1803 - val_loss: -0.1648\n",
      "Epoch 40/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1811 - val_loss: -0.1653\n",
      "Epoch 41/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1820 - val_loss: -0.1662\n",
      "Epoch 42/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1829 - val_loss: -0.1666\n",
      "Epoch 43/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1837 - val_loss: -0.1670\n",
      "Epoch 44/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1846 - val_loss: -0.1680\n",
      "Epoch 45/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1855 - val_loss: -0.1684\n",
      "Epoch 46/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1862 - val_loss: -0.1688\n",
      "Epoch 47/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1870 - val_loss: -0.1699\n",
      "Epoch 48/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1879 - val_loss: -0.1707\n",
      "Epoch 49/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1886 - val_loss: -0.1709\n",
      "Epoch 50/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1895 - val_loss: -0.1714\n",
      "Epoch 51/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1902 - val_loss: -0.1721\n",
      "Epoch 52/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1909 - val_loss: -0.1726\n",
      "Epoch 53/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1917 - val_loss: -0.1732\n",
      "Epoch 54/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1925 - val_loss: -0.1739\n",
      "Epoch 55/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1933 - val_loss: -0.1743\n",
      "Epoch 56/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1940 - val_loss: -0.1746\n",
      "Epoch 57/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1947 - val_loss: -0.1756\n",
      "Epoch 58/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1954 - val_loss: -0.1758\n",
      "Epoch 59/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.1961 - val_loss: -0.1767\n",
      "Epoch 60/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1968 - val_loss: -0.1766\n",
      "Epoch 61/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1976 - val_loss: -0.1780\n",
      "Epoch 62/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1982 - val_loss: -0.1781\n",
      "Epoch 63/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.1990 - val_loss: -0.1789\n",
      "Epoch 64/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.1994 - val_loss: -0.1795\n",
      "Epoch 65/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2002 - val_loss: -0.1794\n",
      "Epoch 66/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2008 - val_loss: -0.1800\n",
      "Epoch 67/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2014 - val_loss: -0.1807\n",
      "Epoch 68/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2020 - val_loss: -0.1812\n",
      "Epoch 69/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2027 - val_loss: -0.1816\n",
      "Epoch 70/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2032 - val_loss: -0.1820\n",
      "Epoch 71/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2038 - val_loss: -0.1828\n",
      "Epoch 72/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2045 - val_loss: -0.1829\n",
      "Epoch 73/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2050 - val_loss: -0.1835\n",
      "Epoch 74/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2056 - val_loss: -0.1837\n",
      "Epoch 75/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2060 - val_loss: -0.1843\n",
      "Epoch 76/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2066 - val_loss: -0.1847\n",
      "Epoch 77/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2073 - val_loss: -0.1850\n",
      "Epoch 78/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2077 - val_loss: -0.1850\n",
      "Epoch 79/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2083 - val_loss: -0.1856\n",
      "Epoch 80/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2088 - val_loss: -0.1860\n",
      "Epoch 81/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2093 - val_loss: -0.1861\n",
      "Epoch 82/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2098 - val_loss: -0.1868\n",
      "Epoch 83/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2101 - val_loss: -0.1875\n",
      "Epoch 84/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2107 - val_loss: -0.1875\n",
      "Epoch 85/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2112 - val_loss: -0.1883\n",
      "Epoch 86/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2118 - val_loss: -0.1884\n",
      "Epoch 87/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2121 - val_loss: -0.1891\n",
      "Epoch 88/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2126 - val_loss: -0.1887\n",
      "Epoch 89/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2131 - val_loss: -0.1897\n",
      "Epoch 90/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2135 - val_loss: -0.1899\n",
      "Epoch 91/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2139 - val_loss: -0.1901\n",
      "Epoch 92/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2144 - val_loss: -0.1907\n",
      "Epoch 93/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2148 - val_loss: -0.1907\n",
      "Epoch 94/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2152 - val_loss: -0.1913\n",
      "Epoch 95/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2157 - val_loss: -0.1918\n",
      "Epoch 96/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2162 - val_loss: -0.1917\n",
      "Epoch 97/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: -0.2164 - val_loss: -0.1928\n",
      "Epoch 98/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2170 - val_loss: -0.1922\n",
      "Epoch 99/100\n",
      "266/266 [==============================] - 1s 6ms/step - loss: -0.2173 - val_loss: -0.1925\n",
      "Epoch 100/100\n",
      "266/266 [==============================] - 2s 6ms/step - loss: -0.2177 - val_loss: -0.1932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25708698d00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "dropout = 0.1\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.3, dtype=tf.float32)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='FC')\n",
    "\n",
    "\n",
    "model2.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "# model2.compile(loss=custom_loss(), optimizer=\"adam\", metrics=[tf.keras.losses.CosineSimilarity(), sparsity])\n",
    "# model2.compile(loss=\"mae\", optimizer=\"adam\", metrics=[tf.keras.losses.CosineSimilarity(), sparsity])\n",
    "\n",
    "model2.summary()\n",
    "model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.87it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 25) (1284, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 334.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25, 61) (1, 25, 1284)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1a899e903d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.25, number_of_sources=3, extents=(1,20))\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "evoked = sim_test.eeg_data[0].average()\n",
    "\n",
    "X_test = evoked.data\n",
    "X_test = (X_test- X_test.mean()) / X_test.std()\n",
    "y_test = sim_test.source_data[0].data\n",
    "y_test /= np.max(np.abs(y_test))\n",
    "\n",
    "X_test = X_test[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "\n",
    "X_test = np.swapaxes(X_test, 1,2)\n",
    "y_test = np.swapaxes(y_test, 1,2)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "y_hat = model.predict(X_test)[0]\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params)\n",
    "\n",
    "\n",
    "stc_hat = solver.apply_inverse_operator(evoked)\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
