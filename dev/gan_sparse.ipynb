{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "import numpy as np\n",
    "# from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "pos_left = fwd[\"src\"][0][\"rr\"][fwd[\"src\"][0][\"vertno\"], :]\n",
    "pos_right = fwd[\"src\"][1][\"rr\"][fwd[\"src\"][1][\"vertno\"], :]\n",
    "pos = np.concatenate((pos_left, pos_right), axis=0)\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))\n",
    "# laplace_operator = laplace_operator @ laplace_operator\n",
    "# laplace_operator = np.identity(n_dipoles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization\n",
    "from tensorflow.keras.layers import Activation, Dropout, ActivityRegularization, TimeDistributed\n",
    "from tensorflow.keras.layers import Reshape, Permute, GaussianNoise, add, BatchNormalization, multiply\n",
    "from esinet.losses import chamfer2\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def weightedLoss(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"DISC\", true, pred)\n",
    "        \n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error / tf.linalg.norm(true, axis=-1)\n",
    "        return error\n",
    "    return loss\n",
    "\n",
    "def weightedLossGan(w):\n",
    "    def loss(true, pred):\n",
    "        print(\"GAN\", true, pred)\n",
    "        error = K.square(true - pred)\n",
    "        # error = K.abs(true - pred)\n",
    "        error = K.switch(K.equal(true, 0), w * error , error)\n",
    "        # normalize for number of active sites\n",
    "        error = error * tf.linalg.norm(true)\n",
    "        return -error\n",
    "    return loss\n",
    "\n",
    "\n",
    "def square(x, exponent=4):\n",
    "    return K.pow(x, exponent) * K.sign(x)\n",
    "\n",
    "def clipped_linear(x, a_min=-1, a_max=1):\n",
    "    return K.clip(x, a_min, a_max)\n",
    "\n",
    "def thresholding(X, min_val, max_val):\n",
    "\n",
    "    def thresholding_sample(xx, K):\n",
    "        K = tf.random.uniform([1], minval=min_val, maxval=max_val, \n",
    "                          dtype=tf.dtypes.int32)\n",
    "        # print(\"See: \\n\")\n",
    "        # print(x, K)\n",
    "        # thresh = tf.sort(tf.abs(x))[-K]\n",
    "        idx = tf.shape(xx) - K\n",
    "        thresh = tf.gather(tf.sort(tf.abs(xx)), idx)\n",
    "        # print(thresh)\n",
    "        \n",
    "        xx = xx * tf.cast(tf.abs(xx)>=thresh, dtype=tf.float32)\n",
    "        return xx\n",
    "    batched_losses = tf.map_fn(lambda x: thresholding_sample(x, K), X, dtype=tf.float32)\n",
    "    return batched_losses\n",
    "\n",
    "\n",
    "\n",
    "def scale_act(x):\n",
    "    return tf.transpose(tf.transpose(x) / tf.math.reduce_max(K.abs(x), axis=-1))\n",
    "    \n",
    "def l1l2_ratio(x):\n",
    "    return K.mean(K.square(x)) / K.mean(K.abs(x))\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim, learning_rate=0.001):\n",
    "    n_chans, n_dipoles = leadfield.shape\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    output_1 = g_model(inputs)\n",
    "    eeg = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(output_1)\n",
    "    \n",
    "    # eeg_normed = LayerNormalization()(eeg)\n",
    "    scale = Lambda(lambda x: (x-tf.reduce_mean(x, 0)) /tf.math.reduce_std(x, 0) )\n",
    "    eeg_normed = scale(eeg)\n",
    "    output_3 = d_model(eeg_normed)\n",
    "    model = Model(inputs, [output_1, output_3])\n",
    "\n",
    "    # Add loss to model\n",
    "    loss = K.abs(tf.keras.losses.CosineSimilarity()(output_1, output_3))\n",
    "    # loss = -tf.keras.losses.Huber()(output_1, output_3)\n",
    "    \n",
    "    model.add_loss(loss)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def define_discriminator(n_dense_layers=2, hidden_units=300, learning_rate=0.001):\n",
    "    input_shape = (n_chans)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name='Input_Discriminator')\n",
    "    activation = \"tanh\"\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "\n",
    "    fc = Dense(hidden_units, activation=activation, name=\"HL_D1\")(inputs)\n",
    "    for i in range(n_dense_layers-1):\n",
    "        fc = Dense(hidden_units, activation=activation, name=f\"HL_D{i+2}\")(fc)\n",
    "    out = Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\")(fc)\n",
    "    \n",
    "    # sparsifier branch\n",
    "    # mask = Dense(n_dipoles, activation=\"sigmoid\", name=\"sparsifier\")(out)\n",
    "    # multi = multiply([out, mask])\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name='Discriminator')\n",
    "    # model.compile(loss=\"mae\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    # model.compile(loss=\"huber\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    # model.add_loss(l1l2_ratio(out))\n",
    "    \n",
    "    return model\n",
    "  \n",
    "\n",
    "\n",
    "def define_generator(latent_dim=100, hidden_units=300, sparsity=[1, 10]):\n",
    "    laplace_operator_ = tf.cast(laplace_operator, dtype=tf.float32)\n",
    "\n",
    "    lrelu = \"tanh\"  # tf.keras.layers.LeakyReLU()\n",
    "    \n",
    "\n",
    "    inputs = tf.keras.Input(shape=(latent_dim), name='Input_Generator')\n",
    "    fc = Dense(latent_dim, activation=lrelu, name=\"HL_G1\")(inputs)\n",
    "    fc = Dense(hidden_units, activation=lrelu, name=\"HL_G2\")(fc)\n",
    "    \n",
    "    # gen_out_sparse = Dense(n_dipoles, name=\"Output_Generator\", activation=\"tanh\")(fc)\n",
    "    \n",
    "    fc = Dense(n_dipoles, name=\"Output_Generator\", activation=clipped_linear)(fc)\n",
    "    out = Activation(lambda x: thresholding(x, *sparsity))(fc)\n",
    "    # gen_out_sparse = Activation(square)(gen_out_sparse)\n",
    "    # gen_out_sparse = Lambda(lambda x: tf.transpose(tf.linalg.matmul(laplace_operator_, tf.transpose(x))), output_shape=(None, n_chans))(gen_out_sparse)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name='Generator')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "\n",
    "def batch_diversity(x):\n",
    "    diversity = K.std(K.mean(K.abs(x), axis=0))\n",
    "    return diversity\n",
    "\n",
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y\n",
    "    \n",
    "def prep_data_sim(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    return X, y\n",
    "\n",
    "def generate_samples(g_model, batch_size, latent_dim):\n",
    "    x_input = np.random.randn(batch_size, latent_dim)\n",
    "    sources = g_model.predict(x_input)\n",
    "    return sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 259.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 25073.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 463.11it/s]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(duration_of_trial=0.01, extents=(1, 40), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=100)\n",
    "X_test = np.stack([eeg.average().data[:, 0] for eeg in sim.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data[:, 0] for source in sim.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8713d6ae581417f809f7c8a7d22598d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Discriminator\n",
      "Train Generator\n",
      "test_loss: -0.06, disc-loss: -0.11, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.08, disc-loss: -0.12, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.09, disc-loss: -0.14, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.16, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.18, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.10, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.11, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.11, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.12, disc-loss: -0.18, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.13, disc-loss: -0.17, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.17, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.14, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.15, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.15, disc-loss: -0.18, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.18, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.16, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.19, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.17, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.18, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.20, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.19, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.20, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.21, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.22, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.21, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.22, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.23, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.25, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.26, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.27, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.24, disc-loss: -0.24, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n",
      "test_loss: -0.25, disc-loss: -0.23, gan-loss: 0.00\n",
      "Train Discriminator\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7360/3233611042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisc_mod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Discriminator\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mleadfield\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7360/3639097518.py\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(g_model, batch_size, latent_dim)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mx_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1980\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "n_iter = 1000\n",
    "epochs = 5\n",
    "batch_size = 1024\n",
    "max_samples = int(10 * batch_size)\n",
    "latent_dim = 100\n",
    "hidden_units = 600\n",
    "n_dense_layers = 3\n",
    "disc_mod = 1\n",
    "gen_mod = 1e99\n",
    "spawn_mod = 10\n",
    "reps = 1\n",
    "learning_rate = 0.001\n",
    "sparsity_generator = [1, int(n_chans/2)]\n",
    "\n",
    "# g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg, learning_rate=0.01)\n",
    "\n",
    "d_model = define_discriminator(learning_rate=learning_rate, hidden_units=hidden_units, n_dense_layers=n_dense_layers)\n",
    "g_model = define_generator(latent_dim=latent_dim, hidden_units=hidden_units, sparsity=sparsity_generator)\n",
    "gan_model = define_gan(g_model, d_model, latent_dim, learning_rate=learning_rate)\n",
    "\n",
    "X_history = np.zeros((0, n_chans))\n",
    "y_history = np.zeros((0, n_dipoles))\n",
    "gan_losses = np.zeros(n_iter)\n",
    "d_losses = np.zeros(n_iter)\n",
    "test_losses = np.zeros(n_iter)\n",
    "generated = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    if i % disc_mod == 0:\n",
    "        print(\"Train Discriminator\")\n",
    "        y = generate_samples(g_model, batch_size, latent_dim)\n",
    "        X = (leadfield @ y.T).T\n",
    "        X, _ = prep_data(X,y)\n",
    "        generated.append(abs(y).mean(axis=0))\n",
    "        X_history = np.append(X_history, X, axis=0)\n",
    "        y_history = np.append(y_history, y, axis=0)\n",
    "        idc = np.arange(X_history.shape[0])\n",
    "        np.random.shuffle(idc)\n",
    "        X_history = X_history[idc]\n",
    "        y_history = y_history[idc]\n",
    "        X_history = X_history[:batch_size]\n",
    "        y_history = y_history[:batch_size]\n",
    "        d_model.trainable = True\n",
    "        for _ in range(epochs):\n",
    "            d_loss = d_model.train_on_batch(X_history[:batch_size], y_history[:batch_size])\n",
    "            # d_loss = d_model.train_on_batch(X, y)\n",
    "        if X_history.shape[0] > max_samples:\n",
    "            X_history = X_history[:max_samples]\n",
    "            y_history = y_history[:max_samples]\n",
    "        gan_model.layers[4] = d_model\n",
    "        # test_loss = d_model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_loss = tf.keras.losses.CosineSimilarity()(tf.cast(y_test, dtype=tf.float32), tf.cast(d_model.predict(X_test), dtype=tf.float32)).numpy()\n",
    "        test_losses[i] = test_loss\n",
    "\n",
    "\n",
    "    if i % spawn_mod == 0:\n",
    "        g_model = define_generator(latent_dim=latent_dim, sparsity=sparsity_generator)\n",
    "        gan_model.layers[1] = g_model\n",
    "        \n",
    "    if i % gen_mod == 0:\n",
    "        print(\"Train Generator\")\n",
    "        x_input = np.random.randn(batch_size, latent_dim)\n",
    "\n",
    "        X = np.ones((batch_size, n_dipoles))\n",
    "        d_model.trainable = False\n",
    "        gan_model.layers[4].trainable = False\n",
    "        for _ in range(epochs):\n",
    "            gan_loss = gan_model.train_on_batch(x_input, X)\n",
    "            # print(f\"\\tintermediate gan_loss: {gan_loss}\")\n",
    "        d_model.trainable = True\n",
    "        gan_model.layers[4].trainable = True\n",
    "        g_model = gan_model.layers[1]\n",
    "      \n",
    "    print(f'test_loss: {test_loss:.2f}, disc-loss: {d_loss:.2f}, gan-loss: {gan_loss:.2f}')\n",
    "    d_losses[i] = d_loss\n",
    "    gan_losses[i] = gan_loss\n",
    "\n",
    "    # break\n",
    "    # print(gan_model.layers[1].layers[1].weights[0].numpy()[0,0])\n",
    "    # print(g_model.layers[1].weights[0].numpy()[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  samples\n",
      "MSE:  0.5042365 Cos:  tf.Tensor(-0.004672897, shape=(), dtype=float32)\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\evoked.py:521: UserWarning: Attempting to set identical left == right == 0.0 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(xlim)\n",
      "c:\\Users\\Lukas\\Envs\\esienv\\lib\\site-packages\\mne\\viz\\evoked.py:521: UserWarning: Attempting to set identical left == right == 0.0 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(xlim)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x302 with 5 Axes>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stc_ = sim.source_data[0].copy()\n",
    "noise = np.random.randn(1, latent_dim)\n",
    "src = g_model(noise).numpy().T\n",
    "print((src[:, 0]!=0).sum(), \" samples\")\n",
    "eeg = leadfield @ src\n",
    "eeg -= eeg.mean()\n",
    "eeg /= eeg.std()\n",
    "src_hat = d_model(eeg[np.newaxis]).numpy().T\n",
    "\n",
    "print(\"MSE: \", np.mean((src-src_hat)**2), \"Cos: \", tf.keras.losses.CosineSimilarity()(src, src_hat))\n",
    "\n",
    "\n",
    "stc_.data = src\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "stc_.data = src_hat\n",
    "stc_.plot(**plot_params)\n",
    "\n",
    "evoked = mne.EvokedArray(eeg, info)\n",
    "evoked.plot_joint()\n",
    "\n",
    "eeg = leadfield @ src_hat\n",
    "eeg -= eeg.mean()\n",
    "eeg /= eeg.std()\n",
    "evoked = mne.EvokedArray(eeg, info)\n",
    "evoked.plot_joint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25b975eddf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(gan_losses, label=\"GAN Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# # g_model, d_model, gan_model = define_models(latent_dim, hidden_units=hidden_units, reg=reg)\n",
    "\n",
    "# y = generate_samples(g_model, 1000, latent_dim)\n",
    "# data = abs(y).mean(axis=0)\n",
    "# stc_ = stc.copy()\n",
    "# stc_.data[:, 0] = data\n",
    "# stc_.plot(**plot_params)\n",
    "\n",
    "# stc_.data = y.T\n",
    "# stc_.plot(**plot_params)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(y.flatten())\n",
    "# print(\"Batch Diversity: \", batch_diversity(y).numpy(), \"L1: \", l1_sparsity(y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17.90it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 401.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.82, cosine=-0.173993319272995, r=0.17, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "# settings = dict(duration_of_trial=0.1, extents=(1,40), number_of_sources=(1,15), target_snr=1e99, source_number_weighting=False)\n",
    "# settings = dict(duration_of_trial=0.01, method=\"noise\", target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.01, extents=(1, 2), number_of_sources=3, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim_test.source_data], axis=0)\n",
    "\n",
    "X, y = prep_data(X[0].T, y[0].T)\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat.shape\n",
    "stc = sim_test.source_data[0]\n",
    "stc.data /= np.max(abs(stc.data))\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth Sim\"), clim=dict(kind='percent', pos_lims=(0, 50, 100)))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"))\n",
    "from scipy.stats import pearsonr\n",
    "from esinet.evaluate import eval_auc\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_hat.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_hat.data.T, dtype=tf.float32)).numpy()\n",
    "r,p = pearsonr(stc.data.flatten(), stc_hat.data.flatten())\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.94, cosine=-0.19464705884456635, r=0.20, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert import Solver\n",
    "\n",
    "solver = Solver(\"MNE\").make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(stc.data.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(y_true, y_pred, pos)) for y_true, y_pred in zip(stc.data.T, stc_mne.data.T)])\n",
    "cosine = tf.keras.losses.CosineSimilarity()(tf.cast(stc.data.T, dtype=tf.float32), tf.cast(stc_mne.data.T, dtype=tf.float32)).numpy()\n",
    "\n",
    "print(f'auc={auc:.2f}, cosine={cosine}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Discriminator with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.91, r=0.84, p=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generated data:\n",
    "y = generate_samples(g_model, 32, latent_dim)\n",
    "X = (leadfield @ y.T).T\n",
    "X, y = prep_data(X,y)\n",
    "\n",
    "y[np.isnan(y)] = 0\n",
    "stc_hat.data = y.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "y_hat = d_model.predict(X)\n",
    "y_hat[np.isnan(y_hat)] = 0\n",
    "\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=\"GAN\"), clim=dict(kind='value', pos_lims=(0, 0.5, 1)))\n",
    "\n",
    "r,p = pearsonr(y.flatten(), y_hat.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y, y_hat)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.51, r=-0.01, p=0.0427\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.02927872 0.03322611 0.06774781]\n",
      "Using control points [0.42319688 0.4607384  0.93013235]\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "evoked = mne.EvokedArray(X.T, info)\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "# solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(evoked)\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "r,p = pearsonr(y.flatten(), stc_mne.data.flatten())\n",
    "auc = np.mean([np.mean(eval_auc(yy_true, yy_pred, pos)) for yy_true, yy_pred in zip(y, stc_mne.data.T)])\n",
    "\n",
    "print(f'auc={auc:.2f}, r={r:.2f}, p={p:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "basic_sources = np.identity(n_dipoles)\n",
    "basic_sources = laplace_operator @ basic_sources\n",
    "basic_sources = np.concatenate([basic_sources, -1*basic_sources], axis=1)\n",
    "basic_eeg = leadfield @ basic_sources\n",
    "print(basic_sources.shape, basic_eeg.shape)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    g_model = tf.keras.Sequential()\n",
    "    input_shape = (None, latent_dim)\n",
    "    g_model.add(InputLayer(input_shape=input_shape))\n",
    "    g_model.add(Dense(latent_dim, name=\"HL1\"))\n",
    "    g_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    # g_model.build()\n",
    "    # g_model.compile(optimizer='adam', loss=\"mse\")\n",
    "    # g_model.summary()\n",
    "    return g_model\n",
    "    \n",
    "def define_discriminator(hidden_units=100):\n",
    "    input_shape = (None, n_chans)\n",
    "    d_model = tf.keras.Sequential()\n",
    "    d_model.add(InputLayer(input_shape=input_shape))\n",
    "    d_model.add(Dense(hidden_units, name=\"HL1\"))\n",
    "    d_model.add(Dense(n_dipoles, name=\"Output\"))\n",
    "    d_model.build()\n",
    "    d_model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "    # d_model.summary()\n",
    "    return d_model\n",
    "\n",
    "def define_gan(g_model, d_model, latent_dim):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    input_shape = (None, latent_dim)\n",
    "    \n",
    "    lam = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(g_model.output)\n",
    "    print(lam)\n",
    "    discriminator = d_model(lam)\n",
    "    model = tf.keras.Model(inputs=g_model.input, outputs=[d_model.output, g_model.output], name='Contextualizer')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(g_model)\n",
    "    # model.add(Lambda(lambda x: tf.linalg.matmul(leadfield_, x)))\n",
    "    # model.add(d_model)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
