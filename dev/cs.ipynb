{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = dict(duration_of_trial=0.25, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=2000)\n",
    "X = np.stack([eeg.average().data for eeg in sim.eeg_data], axis=0)\n",
    "y = np.stack([source.data for source in sim.source_data], axis=0)\n",
    "X, y = prep_data(X, y)\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "y = np.swapaxes(y, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, GRU, multiply, Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dropout, ActivityRegularization, TimeDistributed, Reshape, Permute, GaussianNoise, add\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "def data_loss(leadfield, lam_0=0.1):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    def batch_data_loss(y_true, y_est):\n",
    "        def d_loss(y_true, y_est):\n",
    "            y_true_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_true)))\n",
    "            y_est_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "            # print(\"y_true \", y_true)\n",
    "            # print(\"y_est \", y_est)\n",
    "            \n",
    "            # return K.mean(K.square(y_est - y_true))\n",
    "            error_source = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est, y_true)\n",
    "            error_eeg = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            return (error_source*lam_0 + error_eeg) / (1 + lam_0)\n",
    "        \n",
    "        \n",
    "\n",
    "        batched_losses = tf.map_fn(lambda x:\n",
    "            d_loss(x[0], x[1]), \n",
    "            (y_true, y_est), dtype=tf.float32)\n",
    "        return K.mean(batched_losses)\n",
    "\n",
    "\n",
    "    return batch_data_loss\n",
    "\n",
    "\n",
    "def consistency(x):\n",
    "    return K.mean(K.std(K.abs(x), axis=1))\n",
    "\n",
    "# def consistency(source):\n",
    "#     def c_loss(x):\n",
    "#         matrix = compute_cosine_distances(K.abs(x), K.abs(x))\n",
    "#         return K.mean(matrix)\n",
    "\n",
    "\n",
    "#     batched_losses = tf.map_fn(lambda x:\n",
    "#             c_loss(x), \n",
    "#             source, dtype=tf.float32)\n",
    "#     return K.mean(batched_losses)\n",
    "\n",
    "def c_loss(sources):\n",
    "    matrix = K.abs(compute_cosine_distances(K.abs(sources), K.abs(sources)))\n",
    "    return K.mean(matrix)\n",
    "\n",
    "\n",
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,-1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,-1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "    # return K.mean(K.pow(K.abs(x), 0.5))\n",
    "\n",
    "\n",
    "def get_model(name=\"Model\", n_dense_layers=2, n_lstm_units=128, hidden_units=200, learning_rate=0.001, lam_0=0.1, lam_1=1, lam_2=0.001, add_consistency=True):\n",
    "    input_shape = (None, n_chans)\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=name)\n",
    "    fc = TimeDistributed(Dense(hidden_units, activation=\"linear\", name=\"FC1\"))(inputs)\n",
    "\n",
    "    gru = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='GRU_Discriminator'))(inputs)\n",
    "    source_time = TimeDistributed(Dense(n_dipoles, activation=\"sigmoid\", name=\"Mask\"))(gru)\n",
    "\n",
    "    source = TimeDistributed(Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\"))(fc)\n",
    "\n",
    "    out = multiply([source_time, source])\n",
    "    # out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(source)\n",
    "    # # out = TimeDistributed(Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans)))(source)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name='CS_Net')\n",
    "    \n",
    "    # Data Loss\n",
    "    # L1 Loss\n",
    "    model.add_loss(l1_sparsity(out)*lam_1)\n",
    "    \n",
    "    # Data consistency loss\n",
    "    if add_consistency:\n",
    "        model.add_loss(c_loss(out)*lam_2)\n",
    "    \n",
    "    model.compile(loss=data_loss(leadfield, lam_0=lam_0), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams_0 = [0.01, ]  # source-based\n",
    "lams_1 = [1, ]  # sparsity\n",
    "lams_2 = [0.0, ]  # context\n",
    "\n",
    "models = []\n",
    "for lam_0, lam_1, lam_2 in zip(lams_0, lams_1, lams_2):\n",
    "    model = get_model(lam_0=lam_0, lam_1=lam_1, lam_2=lam_2, name=f\"Lam {lam_2}\")\n",
    "    model.fit(X, y, epochs=20, batch_size=8, validation_split=0.1)\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_auc\n",
    "from scipy.stats import pearsonr\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=2)\n",
    "idx = 0\n",
    "stc = sim_test.source_data[idx].copy()\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "for mod in models:\n",
    "    src_hat = mod.predict(X)[idx]\n",
    "    stc_hat = stc.copy()\n",
    "    stc_hat.data = src_hat.T\n",
    "    stc_hat.plot(**plot_params)\n",
    "    auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    \n",
    "    print(\"AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_auc\n",
    "from scipy.stats import pearsonr\n",
    "# settings = dict(duration_of_trial=0.25, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "settings = dict(duration_of_trial=0.25, extents=20, number_of_sources=6, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=2)\n",
    "\n",
    "X_test = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data for source in sim_test.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "y_test = np.swapaxes(y_test, 1, 2)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "stc = sim_test.source_data[idx].copy()\n",
    "stc.data /= np.max(abs(stc.data))\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "for model in models:\n",
    "    src_hat = model.predict(X_test)[idx]\n",
    "    stc_hat = stc.copy()\n",
    "    stc_hat.data = src_hat.T\n",
    "    stc_hat.data /= np.max(abs(stc_hat.data))\n",
    "    stc_hat.plot(**plot_params)\n",
    "    auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    \n",
    "    print(model.name, \" AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(compute_cosine_distances(abs(stc_hat.data.T), abs(stc_hat.data.T)).numpy().flatten())\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(compute_cosine_distances(abs(stc.data.T), abs(stc.data.T)).numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.minimum_norm_estimates import SolverDynamicStatisticalParametricMapping, SolverMinimumNorm\n",
    "from invert.solvers.wrop import SolverLAURA\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "# solver = SolverLAURA().make_inverse_operator(fwd)\n",
    "solver = SolverChampagne().make_inverse_operator(fwd)\n",
    "# solver = SolverMinimumNorm().make_inverse_operator(fwd)\n",
    "\n",
    "stc_mne = solver.apply_inverse_operator(sim_test.eeg_data[0].average())\n",
    "stc_mne.data = stc_mne.data / np.max(abs(stc_mne.data))\n",
    "stc_mne.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_mne.data.T)])\n",
    "\n",
    "print(model.name, \" AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.flip([[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.75808678, -0.96662011, -0.70010118],\n",
      "       [-0.61501162, -2.32679078,  1.70737184],\n",
      "       [-0.80966745, -1.78914168,  1.18965504],\n",
      "       [-0.39464271, -1.77448911,  1.40676046],\n",
      "       [ 0.44694803,  0.52756118, -0.4517301 ]]), array([[-0.09941177],\n",
      "       [ 2.04270356],\n",
      "       [-1.26083924],\n",
      "       [ 0.44744613],\n",
      "       [ 0.30982689]])]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.70010118, -0.96662011, -0.75808678],\n",
       "        [ 1.70737184, -2.32679078, -0.61501162],\n",
       "        [ 1.18965504, -1.78914168, -0.80966745],\n",
       "        [ 1.40676046, -1.77448911, -0.39464271],\n",
       "        [-0.4517301 ,  0.52756118,  0.44694803]]),\n",
       " array([[-0.09941177],\n",
       "        [ 2.04270356],\n",
       "        [-1.26083924],\n",
       "        [ 0.44744613],\n",
       "        [ 0.30982689]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [np.random.randn(5, 3), np.random.randn(5, 1)]\n",
    "print(a)\n",
    "print()\n",
    "[np.flip(aa, axis=1) for aa in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
