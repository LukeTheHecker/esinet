{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.1s remaining:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100, kind=\"biosemi64\")\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)\n",
    "_, pos = util.unpack_fwd(fwd)[1:3]\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "import mne\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "adjacency = mne.spatial_src_adjacency(fwd['src']).toarray()\n",
    "laplace_operator = abs(laplacian(adjacency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(X, y):\n",
    "    X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "    y = np.stack([(x / np.max(abs(x))) for x in y], axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:07<00:00, 283.85it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 22992.63it/s]\n",
      "100%|██████████| 2000/2000 [00:26<00:00, 75.05it/s]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(duration_of_trial=0.25, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=2000)\n",
    "X = np.stack([eeg.average().data for eeg in sim.eeg_data], axis=0)\n",
    "y = np.stack([source.data for source in sim.source_data], axis=0)\n",
    "X, y = prep_data(X, y)\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "y = np.swapaxes(y, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Input, Lambda, LayerNormalization, GRU, multiply\n",
    "from tensorflow.keras.layers import Activation, Dropout, ActivityRegularization, TimeDistributed, Reshape, Permute, GaussianNoise, add\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# def data_loss(leadfield):\n",
    "#     leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "#     def batch_data_loss(y_true, y_est):\n",
    "#         def d_loss(y_true, y_est):\n",
    "#             y_est = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "#             # print(\"y_true \", y_true)\n",
    "#             # print(\"y_est \", y_est)\n",
    "            \n",
    "#             # return K.mean(K.square(y_est - y_true))\n",
    "#             return tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est, y_true)\n",
    "        \n",
    "        \n",
    "\n",
    "#         batched_losses = tf.map_fn(lambda x:\n",
    "#             d_loss(x[0], x[1]), \n",
    "#             (y_true, y_est), dtype=tf.float32)\n",
    "#         return K.mean(batched_losses)\n",
    "\n",
    "\n",
    "#     return batch_data_loss\n",
    "\n",
    "def data_loss(leadfield, lam_0=0.1):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    def batch_data_loss(y_true, y_est):\n",
    "        def d_loss(y_true, y_est):\n",
    "            y_true_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_true)))\n",
    "            y_est_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "            # print(\"y_true \", y_true)\n",
    "            # print(\"y_est \", y_est)\n",
    "            \n",
    "            # return K.mean(K.square(y_est - y_true))\n",
    "            error_source = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est, y_true)\n",
    "            error_eeg = tf.keras.losses.CosineSimilarity(name=\"Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            return error_source*lam_0 + error_eeg\n",
    "        \n",
    "        \n",
    "\n",
    "        batched_losses = tf.map_fn(lambda x:\n",
    "            d_loss(x[0], x[1]), \n",
    "            (y_true, y_est), dtype=tf.float32)\n",
    "        return K.mean(batched_losses)\n",
    "\n",
    "\n",
    "    return batch_data_loss\n",
    "\n",
    "\n",
    "def consistency(x):\n",
    "    return K.mean(K.std(K.abs(x), axis=1))\n",
    "\n",
    "# def consistency(source):\n",
    "#     def c_loss(x):\n",
    "#         matrix = compute_cosine_distances(K.abs(x), K.abs(x))\n",
    "#         return K.mean(matrix)\n",
    "\n",
    "\n",
    "#     batched_losses = tf.map_fn(lambda x:\n",
    "#             c_loss(x), \n",
    "#             source, dtype=tf.float32)\n",
    "#     return K.mean(batched_losses)\n",
    "\n",
    "def c_loss(x):\n",
    "        matrix = compute_cosine_distances(K.abs(x), K.abs(x))\n",
    "        return K.mean(matrix)\n",
    "\n",
    "\n",
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def l1_sparsity(x):\n",
    "    return K.mean(K.abs(x)) \n",
    "\n",
    "def get_model(name=\"Model\", n_dense_layers=2, hidden_units=200, learning_rate=0.001, lam_0=0.1, lam_1=1, lam_2=0.001, add_consistency=True):\n",
    "    input_shape = (None, n_chans)\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=name)\n",
    "    fc = TimeDistributed(Dense(hidden_units, activation=\"linear\", name=\"FC1\"))(inputs)\n",
    "\n",
    "    gru = GRU(64, return_sequences=True, name='GRU_Discriminator')(inputs)\n",
    "    source_time = TimeDistributed(Dense(n_dipoles, activation=\"sigmoid\", name=\"Mask\"))(gru)\n",
    "\n",
    "    source = TimeDistributed(Dense(n_dipoles, activation=\"linear\", name=\"Output_Final\"))(fc)\n",
    "\n",
    "    out = multiply([source_time, source])\n",
    "    # print(source)\n",
    "    # out = Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans))(source)\n",
    "    # # out = TimeDistributed(Lambda(lambda x: tf.transpose(tf.linalg.matmul(leadfield_, tf.transpose(x))), output_shape=(None, n_chans)))(source)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out, name='CS_Net')\n",
    "    \n",
    "    # Data Loss\n",
    "    # L1 Loss\n",
    "    model.add_loss(l1_sparsity(source)*lam_1)\n",
    "    \n",
    "    # Data consistency loss\n",
    "    if add_consistency:\n",
    "        model.add_loss(c_loss(source)*lam_2)\n",
    "    \n",
    "    model.compile(loss=data_loss(leadfield, lam_0=lam_0), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "225/225 [==============================] - 12s 46ms/step - loss: -0.8990 - val_loss: -0.9696\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 9s 42ms/step - loss: -0.9872 - val_loss: -1.0030\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 10s 42ms/step - loss: -1.0026 - val_loss: -0.9996\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 9s 41ms/step - loss: -1.0107 - val_loss: -1.0122\n",
      "Epoch 5/20\n",
      "225/225 [==============================] - 9s 40ms/step - loss: -1.0188 - val_loss: -1.0219\n",
      "Epoch 6/20\n",
      "225/225 [==============================] - 9s 42ms/step - loss: -1.0212 - val_loss: -1.0217\n",
      "Epoch 7/20\n",
      "225/225 [==============================] - 10s 45ms/step - loss: -1.0249 - val_loss: -1.0229\n",
      "Epoch 8/20\n",
      "225/225 [==============================] - 10s 45ms/step - loss: -1.0297 - val_loss: -1.0166\n",
      "Epoch 9/20\n",
      "225/225 [==============================] - 10s 45ms/step - loss: -1.0310 - val_loss: -1.0287\n",
      "Epoch 10/20\n",
      "225/225 [==============================] - 10s 45ms/step - loss: -1.0311 - val_loss: -1.0223\n",
      "Epoch 11/20\n",
      "225/225 [==============================] - 9s 40ms/step - loss: -1.0341 - val_loss: -1.0338\n",
      "Epoch 12/20\n",
      "225/225 [==============================] - 9s 42ms/step - loss: -1.0349 - val_loss: -1.0202\n",
      "Epoch 13/20\n",
      "225/225 [==============================] - 10s 46ms/step - loss: -1.0355 - val_loss: -1.0283\n",
      "Epoch 14/20\n",
      "225/225 [==============================] - 11s 50ms/step - loss: -1.0398 - val_loss: -1.0308\n",
      "Epoch 15/20\n",
      "225/225 [==============================] - 11s 49ms/step - loss: -1.0407 - val_loss: -1.0325\n",
      "Epoch 16/20\n",
      "225/225 [==============================] - 10s 46ms/step - loss: -1.0410 - val_loss: -1.0306\n",
      "Epoch 17/20\n",
      "225/225 [==============================] - 10s 44ms/step - loss: -1.0419 - val_loss: -1.0275\n",
      "Epoch 18/20\n",
      "225/225 [==============================] - 9s 41ms/step - loss: -1.0433 - val_loss: -1.0292\n",
      "Epoch 19/20\n",
      "225/225 [==============================] - 9s 42ms/step - loss: -1.0434 - val_loss: -1.0285\n",
      "Epoch 20/20\n",
      "225/225 [==============================] - 9s 41ms/step - loss: -1.0437 - val_loss: -1.0279\n"
     ]
    }
   ],
   "source": [
    "lams = [0.001, ]\n",
    "models = []\n",
    "for lam in lams:\n",
    "    model = get_model(lam_2=lam, name=f\"Lam {lam}\")\n",
    "    model.fit(X, y, epochs=20, batch_size=8, validation_split=0.1)\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esinet.evaluate import eval_auc\n",
    "from scipy.stats import pearsonr\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=2)\n",
    "idx = 0\n",
    "stc = sim_test.source_data[idx].copy()\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "for mod in models:\n",
    "    src_hat = mod.predict(X)[idx]\n",
    "    stc_hat = stc.copy()\n",
    "    stc_hat.data = src_hat.T\n",
    "    stc_hat.plot(**plot_params)\n",
    "    auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    \n",
    "    print(\"AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 16.20it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS_Net  AUC:  0.8531631145033631  Corrs:  0.47838636132969276  nMSE:  0.00013167695252763248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.05064677 0.06514393 0.13835773]\n",
      "Using control points [0.05064677 0.06514393 0.13835773]\n"
     ]
    }
   ],
   "source": [
    "from esinet.evaluate import eval_auc\n",
    "from scipy.stats import pearsonr\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=2)\n",
    "\n",
    "X_test = np.stack([eeg.average().data for eeg in sim_test.eeg_data], axis=0)\n",
    "y_test = np.stack([source.data for source in sim_test.source_data], axis=0)\n",
    "X_test, y_test = prep_data(X_test, y_test)\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "y_test = np.swapaxes(y_test, 1, 2)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "stc = sim_test.source_data[idx].copy()\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "for model in models:\n",
    "    src_hat = model.predict(X_test)[idx]\n",
    "    stc_hat = stc.copy()\n",
    "    stc_hat.data = src_hat.T\n",
    "    stc_hat.plot(**plot_params)\n",
    "    auc = np.mean([np.mean(eval_auc(y_true_sample, y_pred_sample, pos)) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    corrs = np.mean([np.mean(pearsonr(y_true_sample, y_pred_sample)[0]) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    nmses = np.mean([np.mean((y_true_sample - y_pred_sample)**2) for y_true_sample, y_pred_sample in zip(stc.data.T, stc_hat.data.T)])\n",
    "    \n",
    "    print(model.name, \" AUC: \", auc, \" Corrs: \", corrs, \" nMSE: \", nmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
