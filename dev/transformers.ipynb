{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import get_info, create_forward_model\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Dense, MultiHeadAttention, Bidirectional, TimeDistributed, \n",
    "    LSTM, GRU, InputLayer, Attention, BatchNormalization, RepeatVector, Input, Activation, dot, \n",
    "    concatenate)\n",
    "\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.4s remaining:    2.4s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.4s remaining:    0.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:23<00:00, 210.30it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 26253.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (324, 100) (324, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:43<00:00, 114.54it/s]\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(info=info, sampling='ico2')\n",
    "sim = Simulation(fwd, info).simulate(n_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([eeg.get_data()[0] for eeg in sim.eeg_data], axis=0)\n",
    "y = np.stack([src.data for src in sim.source_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.swapaxes(X, 1,2)\n",
    "y = np.swapaxes(y, 1,2)\n",
    "\n",
    "for n in range(X.shape[0]):\n",
    "    for t in range(X.shape[1]):\n",
    "        X[n,t,:] -= X[n,t,:].mean()\n",
    "        X[n,t,:] /= X[n,t,:].std()\n",
    "        \n",
    "        # y[n,t,:] -= y[n,t,:].mean()\n",
    "        y[n,t,:] /= np.abs(y[n,t,:]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Attention\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, None, 150)         82200     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 150)         135600    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, None, 324)         48924     \n",
      "=================================================================\n",
      "Total params: 266,724\n",
      "Trainable params: 266,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 31s 195ms/step - loss: -0.1392 - val_loss: -0.2094\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 31s 221ms/step - loss: -0.2576 - val_loss: -0.2803\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 31s 218ms/step - loss: -0.2936 - val_loss: -0.2978\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 30s 215ms/step - loss: -0.3061 - val_loss: -0.3066\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 30s 215ms/step - loss: -0.3134 - val_loss: -0.3117\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 30s 215ms/step - loss: -0.3182 - val_loss: -0.3156\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3213 - val_loss: -0.3177\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3238 - val_loss: -0.3200\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 30s 215ms/step - loss: -0.3260 - val_loss: -0.3213\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3278 - val_loss: -0.3230\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3300 - val_loss: -0.3260\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3321 - val_loss: -0.3280\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3342 - val_loss: -0.3300\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3366 - val_loss: -0.3327\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3388 - val_loss: -0.3340\n",
      "Epoch 16/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3408 - val_loss: -0.3358\n",
      "Epoch 17/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3428 - val_loss: -0.3376\n",
      "Epoch 18/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3450 - val_loss: -0.3399\n",
      "Epoch 19/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3471 - val_loss: -0.3429\n",
      "Epoch 20/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3496 - val_loss: -0.3442\n",
      "Epoch 21/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3513 - val_loss: -0.3462\n",
      "Epoch 22/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3538 - val_loss: -0.3480\n",
      "Epoch 23/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3558 - val_loss: -0.3495\n",
      "Epoch 24/100\n",
      "141/141 [==============================] - 30s 209ms/step - loss: -0.3577 - val_loss: -0.3523\n",
      "Epoch 25/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3599 - val_loss: -0.3541\n",
      "Epoch 26/100\n",
      "141/141 [==============================] - 30s 209ms/step - loss: -0.3623 - val_loss: -0.3551\n",
      "Epoch 27/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3639 - val_loss: -0.3572\n",
      "Epoch 28/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3659 - val_loss: -0.3593\n",
      "Epoch 29/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3678 - val_loss: -0.3613\n",
      "Epoch 30/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3694 - val_loss: -0.3621\n",
      "Epoch 31/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3716 - val_loss: -0.3638\n",
      "Epoch 32/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3733 - val_loss: -0.3656\n",
      "Epoch 33/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3750 - val_loss: -0.3663\n",
      "Epoch 34/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3768 - val_loss: -0.3684\n",
      "Epoch 35/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3787 - val_loss: -0.3693\n",
      "Epoch 36/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3805 - val_loss: -0.3709\n",
      "Epoch 37/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3820 - val_loss: -0.3728\n",
      "Epoch 38/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3841 - val_loss: -0.3736\n",
      "Epoch 39/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3855 - val_loss: -0.3759\n",
      "Epoch 40/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3875 - val_loss: -0.3778\n",
      "Epoch 41/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3892 - val_loss: -0.3793\n",
      "Epoch 42/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3910 - val_loss: -0.3799\n",
      "Epoch 43/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3927 - val_loss: -0.3822\n",
      "Epoch 44/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.3945 - val_loss: -0.3844\n",
      "Epoch 45/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3963 - val_loss: -0.3849\n",
      "Epoch 46/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3977 - val_loss: -0.3869\n",
      "Epoch 47/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3995 - val_loss: -0.3881\n",
      "Epoch 48/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4012 - val_loss: -0.3894\n",
      "Epoch 49/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.4028 - val_loss: -0.3909\n",
      "Epoch 50/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4044 - val_loss: -0.3919\n",
      "Epoch 51/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4058 - val_loss: -0.3934\n",
      "Epoch 52/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4076 - val_loss: -0.3944\n",
      "Epoch 53/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4089 - val_loss: -0.3955\n",
      "Epoch 54/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.4102 - val_loss: -0.3975\n",
      "Epoch 55/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4119 - val_loss: -0.3994\n",
      "Epoch 56/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4131 - val_loss: -0.3994\n",
      "Epoch 57/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4148 - val_loss: -0.4010\n",
      "Epoch 58/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4157 - val_loss: -0.4027\n",
      "Epoch 59/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4172 - val_loss: -0.4026\n",
      "Epoch 60/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4183 - val_loss: -0.4051\n",
      "Epoch 61/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4199 - val_loss: -0.4062\n",
      "Epoch 62/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4208 - val_loss: -0.4067\n",
      "Epoch 63/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4219 - val_loss: -0.4071\n",
      "Epoch 64/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4230 - val_loss: -0.4089\n",
      "Epoch 65/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4241 - val_loss: -0.4098\n",
      "Epoch 66/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4254 - val_loss: -0.4112\n",
      "Epoch 67/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4262 - val_loss: -0.4120\n",
      "Epoch 68/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4273 - val_loss: -0.4125\n",
      "Epoch 69/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4283 - val_loss: -0.4132\n",
      "Epoch 70/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4292 - val_loss: -0.4132\n",
      "Epoch 71/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4301 - val_loss: -0.4146\n",
      "Epoch 72/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4310 - val_loss: -0.4144\n",
      "Epoch 73/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4318 - val_loss: -0.4162\n",
      "Epoch 74/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4327 - val_loss: -0.4161\n",
      "Epoch 75/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4333 - val_loss: -0.4178\n",
      "Epoch 76/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4342 - val_loss: -0.4188\n",
      "Epoch 77/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4352 - val_loss: -0.4198\n",
      "Epoch 78/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4357 - val_loss: -0.4201\n",
      "Epoch 79/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.4364 - val_loss: -0.4210\n",
      "Epoch 80/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4373 - val_loss: -0.4201\n",
      "Epoch 81/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4378 - val_loss: -0.4211\n",
      "Epoch 82/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4385 - val_loss: -0.4223\n",
      "Epoch 83/100\n",
      "141/141 [==============================] - 30s 212ms/step - loss: -0.4392 - val_loss: -0.4221\n",
      "Epoch 84/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4399 - val_loss: -0.4237\n",
      "Epoch 85/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4406 - val_loss: -0.4237\n",
      "Epoch 86/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4413 - val_loss: -0.4245\n",
      "Epoch 87/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4419 - val_loss: -0.4244\n",
      "Epoch 88/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4424 - val_loss: -0.4245\n",
      "Epoch 89/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4430 - val_loss: -0.4256\n",
      "Epoch 90/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4434 - val_loss: -0.4267\n",
      "Epoch 91/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4440 - val_loss: -0.4272\n",
      "Epoch 92/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4447 - val_loss: -0.4268\n",
      "Epoch 93/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4454 - val_loss: -0.4277\n",
      "Epoch 94/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4455 - val_loss: -0.4291\n",
      "Epoch 95/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4463 - val_loss: -0.4280\n",
      "Epoch 96/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.4467 - val_loss: -0.4291\n",
      "Epoch 97/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.4472 - val_loss: -0.4295\n",
      "Epoch 98/100\n",
      "141/141 [==============================] - 30s 209ms/step - loss: -0.4477 - val_loss: -0.4305\n",
      "Epoch 99/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.4483 - val_loss: -0.4298\n",
      "Epoch 100/100\n",
      "141/141 [==============================] - 30s 209ms/step - loss: -0.4488 - val_loss: -0.4310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265d7640100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape= (None, X.shape[2])\n",
    "output_shape = (None, y.shape[2])\n",
    "\n",
    "model = tf.keras.models.Sequential(name='Attention')\n",
    "\n",
    "model.add(InputLayer(input_shape=input_shape))\n",
    "\n",
    "model.add(Bidirectional(LSTM(75, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(75, return_sequences=True)))\n",
    "\n",
    "model.add(TimeDistributed(Dense(y.shape[2])))\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "model.fit(X, y, validation_split=0.1, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 100, 300), dtype=tf.float32, name=None), name='concatenate_25/concat:0', description=\"created by layer 'concatenate_25'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 100, 324), dtype=tf.float32, name=None), name='time_distributed_7/Reshape_1:0', description=\"created by layer 'time_distributed_7'\")\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Total params: 407,124\n",
      "Trainable params: 406,224\n",
      "Non-trainable params: 900\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 33s 212ms/step - loss: -0.0160 - val_loss: -0.0410\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 27s 192ms/step - loss: -0.0474 - val_loss: -0.0431\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 28s 201ms/step - loss: -0.0523 - val_loss: -0.0540\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 29s 203ms/step - loss: -0.0613 - val_loss: -0.0544\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.0667 - val_loss: -0.0631\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 26s 188ms/step - loss: -0.0785 - val_loss: -0.1043\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 26s 188ms/step - loss: -0.1229 - val_loss: -0.1591\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 26s 188ms/step - loss: -0.1692 - val_loss: -0.1818\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 26s 188ms/step - loss: -0.1912 - val_loss: -0.2052\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - 28s 197ms/step - loss: -0.2108 - val_loss: -0.1873\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2165 - val_loss: -0.2236\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - 26s 186ms/step - loss: -0.2307 - val_loss: -0.2368\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2403 - val_loss: -0.2393\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2469 - val_loss: -0.2492\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2512 - val_loss: -0.2531\n",
      "Epoch 16/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2533 - val_loss: -0.2586\n",
      "Epoch 17/100\n",
      "141/141 [==============================] - 27s 189ms/step - loss: -0.2585 - val_loss: -0.2490\n",
      "Epoch 18/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2605 - val_loss: -0.2606\n",
      "Epoch 19/100\n",
      "141/141 [==============================] - 26s 187ms/step - loss: -0.2662 - val_loss: -0.2622\n",
      "Epoch 20/100\n",
      "141/141 [==============================] - 28s 196ms/step - loss: -0.2644 - val_loss: -0.2688\n",
      "Epoch 21/100\n",
      "141/141 [==============================] - 28s 196ms/step - loss: -0.2706 - val_loss: -0.2627\n",
      "Epoch 22/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.2700 - val_loss: -0.2723\n",
      "Epoch 23/100\n",
      "141/141 [==============================] - 29s 205ms/step - loss: -0.2793 - val_loss: -0.2787\n",
      "Epoch 24/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.2802 - val_loss: -0.2774\n",
      "Epoch 25/100\n",
      "141/141 [==============================] - 28s 199ms/step - loss: -0.2693 - val_loss: -0.2774\n",
      "Epoch 26/100\n",
      "141/141 [==============================] - 28s 196ms/step - loss: -0.2828 - val_loss: -0.2836\n",
      "Epoch 27/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.2848 - val_loss: -0.2830\n",
      "Epoch 28/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.2824 - val_loss: -0.2867\n",
      "Epoch 29/100\n",
      "141/141 [==============================] - 27s 195ms/step - loss: -0.2887 - val_loss: -0.2790\n",
      "Epoch 30/100\n",
      "141/141 [==============================] - 28s 197ms/step - loss: -0.2878 - val_loss: -0.2904\n",
      "Epoch 31/100\n",
      "141/141 [==============================] - 28s 195ms/step - loss: -0.2907 - val_loss: -0.2914\n",
      "Epoch 32/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.2914 - val_loss: -0.2916\n",
      "Epoch 33/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.2951 - val_loss: -0.2857\n",
      "Epoch 34/100\n",
      "141/141 [==============================] - 28s 195ms/step - loss: -0.2965 - val_loss: -0.2942\n",
      "Epoch 35/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.2996 - val_loss: -0.2987\n",
      "Epoch 36/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.3011 - val_loss: -0.2982\n",
      "Epoch 37/100\n",
      "141/141 [==============================] - 28s 200ms/step - loss: -0.3032 - val_loss: -0.2895\n",
      "Epoch 38/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.3008 - val_loss: -0.3026\n",
      "Epoch 39/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.3064 - val_loss: -0.3034\n",
      "Epoch 40/100\n",
      "141/141 [==============================] - 27s 194ms/step - loss: -0.3001 - val_loss: -0.3040\n",
      "Epoch 41/100\n",
      "141/141 [==============================] - 28s 201ms/step - loss: -0.3079 - val_loss: -0.2949\n",
      "Epoch 42/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3013 - val_loss: -0.3068\n",
      "Epoch 43/100\n",
      "141/141 [==============================] - 28s 200ms/step - loss: -0.3098 - val_loss: -0.3089\n",
      "Epoch 44/100\n",
      "141/141 [==============================] - 29s 207ms/step - loss: -0.3104 - val_loss: -0.2816\n",
      "Epoch 45/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.2924 - val_loss: -0.2847\n",
      "Epoch 46/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3050 - val_loss: -0.3063\n",
      "Epoch 47/100\n",
      "141/141 [==============================] - 29s 207ms/step - loss: -0.3079 - val_loss: -0.3090\n",
      "Epoch 48/100\n",
      "141/141 [==============================] - 28s 202ms/step - loss: -0.3108 - val_loss: -0.3108\n",
      "Epoch 49/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.3136 - val_loss: -0.3083\n",
      "Epoch 50/100\n",
      "141/141 [==============================] - 32s 226ms/step - loss: -0.3122 - val_loss: -0.3126\n",
      "Epoch 51/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3137 - val_loss: -0.3122\n",
      "Epoch 52/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.3185 - val_loss: -0.3157\n",
      "Epoch 53/100\n",
      "141/141 [==============================] - 27s 195ms/step - loss: -0.3164 - val_loss: -0.2895\n",
      "Epoch 54/100\n",
      "141/141 [==============================] - 29s 205ms/step - loss: -0.3150 - val_loss: -0.3175\n",
      "Epoch 55/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3213 - val_loss: -0.3118\n",
      "Epoch 56/100\n",
      "141/141 [==============================] - 28s 197ms/step - loss: -0.3220 - val_loss: -0.3054\n",
      "Epoch 57/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3129 - val_loss: -0.3221\n",
      "Epoch 58/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3270 - val_loss: -0.3215\n",
      "Epoch 59/100\n",
      "141/141 [==============================] - 29s 203ms/step - loss: -0.3272 - val_loss: -0.3134\n",
      "Epoch 60/100\n",
      "141/141 [==============================] - 28s 201ms/step - loss: -0.3180 - val_loss: -0.3218\n",
      "Epoch 61/100\n",
      "141/141 [==============================] - 30s 216ms/step - loss: -0.3288 - val_loss: -0.3132\n",
      "Epoch 62/100\n",
      "141/141 [==============================] - 29s 209ms/step - loss: -0.3230 - val_loss: -0.3201\n",
      "Epoch 63/100\n",
      "141/141 [==============================] - 30s 211ms/step - loss: -0.3257 - val_loss: -0.3300\n",
      "Epoch 64/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3371 - val_loss: -0.3296\n",
      "Epoch 65/100\n",
      "141/141 [==============================] - 28s 199ms/step - loss: -0.3319 - val_loss: -0.3312\n",
      "Epoch 66/100\n",
      "141/141 [==============================] - 28s 198ms/step - loss: -0.3306 - val_loss: -0.3318\n",
      "Epoch 67/100\n",
      "141/141 [==============================] - 27s 193ms/step - loss: -0.3336 - val_loss: -0.3341\n",
      "Epoch 68/100\n",
      "141/141 [==============================] - 28s 197ms/step - loss: -0.3313 - val_loss: -0.3321\n",
      "Epoch 69/100\n",
      "141/141 [==============================] - 30s 213ms/step - loss: -0.3398 - val_loss: -0.3329\n",
      "Epoch 70/100\n",
      "141/141 [==============================] - 28s 200ms/step - loss: -0.3396 - val_loss: -0.3356\n",
      "Epoch 71/100\n",
      "141/141 [==============================] - 28s 202ms/step - loss: -0.3424 - val_loss: -0.3364\n",
      "Epoch 72/100\n",
      "141/141 [==============================] - 30s 210ms/step - loss: -0.3407 - val_loss: -0.3383\n",
      "Epoch 73/100\n",
      "141/141 [==============================] - 28s 202ms/step - loss: -0.3371 - val_loss: -0.3339\n",
      "Epoch 74/100\n",
      "141/141 [==============================] - 29s 204ms/step - loss: -0.3398 - val_loss: -0.3391\n",
      "Epoch 75/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3440 - val_loss: -0.3395\n",
      "Epoch 76/100\n",
      "141/141 [==============================] - 33s 233ms/step - loss: -0.3454 - val_loss: -0.3341\n",
      "Epoch 77/100\n",
      "141/141 [==============================] - 37s 261ms/step - loss: -0.3333 - val_loss: -0.3392\n",
      "Epoch 78/100\n",
      "141/141 [==============================] - 31s 219ms/step - loss: -0.3463 - val_loss: -0.3422\n",
      "Epoch 79/100\n",
      "141/141 [==============================] - 29s 207ms/step - loss: -0.3483 - val_loss: -0.3422\n",
      "Epoch 80/100\n",
      "141/141 [==============================] - 30s 215ms/step - loss: -0.3481 - val_loss: -0.3372\n",
      "Epoch 81/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3471 - val_loss: -0.3437\n",
      "Epoch 82/100\n",
      "141/141 [==============================] - 29s 208ms/step - loss: -0.3497 - val_loss: -0.3442\n",
      "Epoch 83/100\n",
      "141/141 [==============================] - 27s 194ms/step - loss: -0.3508 - val_loss: -0.3443\n",
      "Epoch 84/100\n",
      "141/141 [==============================] - 27s 190ms/step - loss: -0.3491 - val_loss: -0.2925\n",
      "Epoch 85/100\n",
      "141/141 [==============================] - 27s 190ms/step - loss: -0.3128 - val_loss: -0.3214\n",
      "Epoch 86/100\n",
      "141/141 [==============================] - 27s 191ms/step - loss: -0.3301 - val_loss: -0.3358\n",
      "Epoch 87/100\n",
      "141/141 [==============================] - 30s 214ms/step - loss: -0.3451 - val_loss: -0.3428\n",
      "Epoch 88/100\n",
      "141/141 [==============================] - 1591s 11s/step - loss: -0.3453 - val_loss: -0.3437\n",
      "Epoch 89/100\n",
      " 16/141 [==>...........................] - ETA: 28s - loss: -0.3497"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6484/3084253327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCosineSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "input_shape= (X.shape[1], X.shape[2])\n",
    "output_shape = (y.shape[1], y.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "n_hidden = 150\n",
    "n_blocks = 1\n",
    "input_train = Input(shape=input_shape)\n",
    "output_train = Input(shape=output_shape)\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    if i == 0:\n",
    "        first_layer = input_train\n",
    "    else:\n",
    "        first_layer = decoder_combined_context\n",
    "\n",
    "    encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
    "        n_hidden, return_state=True, return_sequences=True)(first_layer)\n",
    "    # print(encoder_stack_h)\n",
    "    # print(encoder_last_h)\n",
    "    # print(encoder_last_c)\n",
    "\n",
    "    encoder_last_h = BatchNormalization(momentum=0.6)(encoder_last_h)\n",
    "    encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)\n",
    "    decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n",
    "    # print(decoder_input)\n",
    "\n",
    "    decoder_stack_h = LSTM(n_hidden, return_state=False, return_sequences=True)(\n",
    "    decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
    "    # print(decoder_stack_h)\n",
    "\n",
    "\n",
    "    attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
    "    attention = Activation('sigmoid')(attention)\n",
    "    # print(attention)\n",
    "\n",
    "    context = dot([attention, encoder_stack_h], axes=[2,1])\n",
    "    context = BatchNormalization(momentum=0.6)(context)\n",
    "    # print(context)\n",
    "\n",
    "\n",
    "    decoder_combined_context = concatenate([context, decoder_stack_h])\n",
    "    print(decoder_combined_context)\n",
    "\n",
    "\n",
    "out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n",
    "print(out)\n",
    "\n",
    "model = Model(inputs=input_train, outputs=out)\n",
    "model.build(input_shape=input_shape)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CosineSimilarity())\n",
    "model.summary()\n",
    "model.fit(X, y, validation_split=0.1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8292a7c1b71beb25883e5d3de4479593a27229e31834907607dc8a0d6e7b1899"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
