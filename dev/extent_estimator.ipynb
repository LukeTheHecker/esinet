{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    2.0s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.0s remaining:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.5s remaining:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico4\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:38<00:00, 262.89it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 27248.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 1) (1284, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1075.47it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "settings = dict(duration_of_trial=0., number_of_sources=1)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.40710349,  2.22236401, 11.1196467 , ..., 10.09542751,\n",
       "       34.24898695,  2.2508002 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "y = np.array([extent[0] for extent in sim.simulation_info.extents.values])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, None, 300)         18600     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, None, 1)           301       \n",
      "=================================================================\n",
      "Total params: 18,901\n",
      "Trainable params: 18,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "232/266 [=========================>....] - ETA: 0s - loss: 625.2062WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "266/266 [==============================] - 1s 2ms/step - loss: 577.3128 - val_loss: 237.5743\n",
      "Epoch 2/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 205.0219 - val_loss: 214.0326\n",
      "Epoch 3/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 196.0475 - val_loss: 200.2537\n",
      "Epoch 4/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 192.2560 - val_loss: 199.5821\n",
      "Epoch 5/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 190.9534 - val_loss: 194.6966\n",
      "Epoch 6/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 189.7705 - val_loss: 195.0513\n",
      "Epoch 7/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 189.1053 - val_loss: 195.0654\n",
      "Epoch 8/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 187.6180 - val_loss: 190.7591\n",
      "Epoch 9/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 185.8484 - val_loss: 195.5975\n",
      "Epoch 10/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 186.1694 - val_loss: 191.0378\n",
      "Epoch 11/100\n",
      "266/266 [==============================] - 0s 996us/step - loss: 184.3083 - val_loss: 190.1154\n",
      "Epoch 12/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 181.7246 - val_loss: 188.1086\n",
      "Epoch 13/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 181.3654 - val_loss: 185.8084\n",
      "Epoch 14/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 179.6353 - val_loss: 185.1559\n",
      "Epoch 15/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 178.1401 - val_loss: 184.1660\n",
      "Epoch 16/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 176.7909 - val_loss: 180.5488\n",
      "Epoch 17/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 175.0140 - val_loss: 182.2039\n",
      "Epoch 18/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 172.7304 - val_loss: 180.1262\n",
      "Epoch 19/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 171.2193 - val_loss: 181.2050\n",
      "Epoch 20/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 169.1844 - val_loss: 182.8853\n",
      "Epoch 21/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 168.4496 - val_loss: 180.2906\n",
      "Epoch 22/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 166.4749 - val_loss: 174.5484\n",
      "Epoch 23/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 165.4704 - val_loss: 172.5938\n",
      "Epoch 24/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 164.4845 - val_loss: 173.3899\n",
      "Epoch 25/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 161.7754 - val_loss: 175.8435\n",
      "Epoch 26/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 161.2240 - val_loss: 168.7955\n",
      "Epoch 27/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 158.7782 - val_loss: 172.9509\n",
      "Epoch 28/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 158.1048 - val_loss: 167.4571\n",
      "Epoch 29/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 156.8696 - val_loss: 168.1172\n",
      "Epoch 30/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 155.8308 - val_loss: 166.6676\n",
      "Epoch 31/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 153.7361 - val_loss: 165.8956\n",
      "Epoch 32/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 152.2617 - val_loss: 165.6083\n",
      "Epoch 33/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 150.9946 - val_loss: 165.5144\n",
      "Epoch 34/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 149.6476 - val_loss: 165.0653\n",
      "Epoch 35/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 148.5879 - val_loss: 164.9556\n",
      "Epoch 36/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 147.4007 - val_loss: 163.2747\n",
      "Epoch 37/100\n",
      "266/266 [==============================] - 0s 1000us/step - loss: 146.7859 - val_loss: 160.7294\n",
      "Epoch 38/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 145.2814 - val_loss: 161.7685\n",
      "Epoch 39/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 143.6770 - val_loss: 162.8570\n",
      "Epoch 40/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 143.5984 - val_loss: 160.1650\n",
      "Epoch 41/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 142.3523 - val_loss: 162.8129\n",
      "Epoch 42/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 140.7820 - val_loss: 164.2195\n",
      "Epoch 43/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 139.6789 - val_loss: 160.0594\n",
      "Epoch 44/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 138.7407 - val_loss: 164.6287\n",
      "Epoch 45/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 138.0244 - val_loss: 161.4127\n",
      "Epoch 46/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 136.5200 - val_loss: 162.5842\n",
      "Epoch 47/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 135.5107 - val_loss: 157.5866\n",
      "Epoch 48/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 134.4219 - val_loss: 162.6150\n",
      "Epoch 49/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 133.7536 - val_loss: 158.7353\n",
      "Epoch 50/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 132.0081 - val_loss: 157.5062\n",
      "Epoch 51/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 131.1944 - val_loss: 156.4414\n",
      "Epoch 52/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 130.2018 - val_loss: 164.3909\n",
      "Epoch 53/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 129.6015 - val_loss: 154.6409\n",
      "Epoch 54/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 129.0993 - val_loss: 153.4438\n",
      "Epoch 55/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 127.3122 - val_loss: 153.8606\n",
      "Epoch 56/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 126.0430 - val_loss: 154.8736\n",
      "Epoch 57/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 125.4505 - val_loss: 155.9010\n",
      "Epoch 58/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 124.5345 - val_loss: 153.1828\n",
      "Epoch 59/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 124.2815 - val_loss: 153.4882\n",
      "Epoch 60/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 122.6129 - val_loss: 155.4629\n",
      "Epoch 61/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 121.4672 - val_loss: 153.5360\n",
      "Epoch 62/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 120.6429 - val_loss: 155.4191\n",
      "Epoch 63/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 119.9840 - val_loss: 153.4582\n",
      "Epoch 64/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 118.7605 - val_loss: 152.8624\n",
      "Epoch 65/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 117.6080 - val_loss: 153.7731\n",
      "Epoch 66/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 116.3815 - val_loss: 152.2404\n",
      "Epoch 67/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 115.9574 - val_loss: 152.4031\n",
      "Epoch 68/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 114.6248 - val_loss: 150.5980\n",
      "Epoch 69/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 113.9562 - val_loss: 156.0627\n",
      "Epoch 70/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 112.5725 - val_loss: 151.8035\n",
      "Epoch 71/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 112.3702 - val_loss: 154.9393\n",
      "Epoch 72/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 110.5428 - val_loss: 151.7711\n",
      "Epoch 73/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 109.4934 - val_loss: 150.8429\n",
      "Epoch 74/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 108.9777 - val_loss: 150.2682\n",
      "Epoch 75/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 107.6128 - val_loss: 151.5188\n",
      "Epoch 76/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 107.0792 - val_loss: 150.4430\n",
      "Epoch 77/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 105.9262 - val_loss: 149.3815\n",
      "Epoch 78/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 105.0016 - val_loss: 151.8107\n",
      "Epoch 79/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 104.4326 - val_loss: 150.8232\n",
      "Epoch 80/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 103.5520 - val_loss: 148.1430\n",
      "Epoch 81/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 102.2602 - val_loss: 147.5270\n",
      "Epoch 82/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 101.2963 - val_loss: 152.3262\n",
      "Epoch 83/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 100.7610 - val_loss: 146.5791\n",
      "Epoch 84/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 99.2945 - val_loss: 148.4374\n",
      "Epoch 85/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 98.3709 - val_loss: 148.2392\n",
      "Epoch 86/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 97.3421 - val_loss: 147.5572\n",
      "Epoch 87/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 97.0060 - val_loss: 148.0577\n",
      "Epoch 88/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 97.0830 - val_loss: 145.5140\n",
      "Epoch 89/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 95.1133 - val_loss: 147.0444\n",
      "Epoch 90/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 95.1237 - val_loss: 147.0717\n",
      "Epoch 91/100\n",
      "266/266 [==============================] - 0s 928us/step - loss: 93.2992 - val_loss: 147.1461\n",
      "Epoch 92/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 92.9035 - val_loss: 146.0488\n",
      "Epoch 93/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 91.9061 - val_loss: 145.9823\n",
      "Epoch 94/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 91.0553 - val_loss: 143.8244\n",
      "Epoch 95/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 90.2804 - val_loss: 145.9543\n",
      "Epoch 96/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 89.6765 - val_loss: 144.2394\n",
      "Epoch 97/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 88.4156 - val_loss: 145.8498\n",
      "Epoch 98/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 87.9110 - val_loss: 144.5969\n",
      "Epoch 99/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 87.3618 - val_loss: 144.3835\n",
      "Epoch 100/100\n",
      "266/266 [==============================] - 0s 1ms/step - loss: 86.2193 - val_loss: 144.2265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e8fc56eee0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "# model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "# model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Build model with input layer\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 308.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 34456.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 1) (1284, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1129.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'r=0.564959964322163')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "settings = dict(duration_of_trial=0., number_of_sources=1)\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X_test = np.squeeze(np.stack([eeg.average().data for eeg in sim_test.eeg_data]))\n",
    "X_test = np.stack([(x - np.mean(x)) / np.std(x) for x in X_test], axis=0)\n",
    "y_test = np.array([extent[0] for extent in sim_test.simulation_info.extents.values])\n",
    "\n",
    "y_pred = model.predict(X_test)[:, 0]\n",
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.ylim(0,51)\n",
    "plt.xlim(0,51)\n",
    "r, _ = pearsonr(y_test, y_pred)\n",
    "plt.title(f\"r={r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:51<00:00, 194.06it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 15674.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (5124, 1) (5124, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1054.64it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "settings = dict(duration_of_trial=0., number_of_sources=1)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "X = np.stack([(x - np.mean(x)) / np.std(x) for x in X], axis=0)\n",
    "y = np.array([extent[0] for extent in sim.simulation_info.positions.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, None, 200)         12400     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, None, 200)         40200     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, None, 200)         40200     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, None, 3)           603       \n",
      "=================================================================\n",
      "Total params: 93,403\n",
      "Trainable params: 93,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_27_input'), name='dense_27_input', description=\"created by layer 'dense_27_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_27_input'), name='dense_27_input', description=\"created by layer 'dense_27_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "259/266 [============================>.] - ETA: 0s - loss: 976.8743WARNING:tensorflow:Model was constructed with shape (None, None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 61), dtype=tf.float32, name='dense_27_input'), name='dense_27_input', description=\"created by layer 'dense_27_input'\"), but it was called on an input with incompatible shape (None, 61).\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 969.7183 - val_loss: 692.4044\n",
      "Epoch 2/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 503.6608 - val_loss: 296.3875\n",
      "Epoch 3/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 147.0448 - val_loss: 77.3386\n",
      "Epoch 4/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 60.1079 - val_loss: 52.8858\n",
      "Epoch 5/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 43.2244 - val_loss: 45.9378\n",
      "Epoch 6/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 36.5802 - val_loss: 41.2299\n",
      "Epoch 7/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 33.3971 - val_loss: 38.8395\n",
      "Epoch 8/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 30.3243 - val_loss: 37.7036\n",
      "Epoch 9/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 27.9554 - val_loss: 36.6800\n",
      "Epoch 10/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 26.8237 - val_loss: 36.0849\n",
      "Epoch 11/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 25.5972 - val_loss: 34.2730\n",
      "Epoch 12/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 24.0130 - val_loss: 33.9246\n",
      "Epoch 13/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 23.6067 - val_loss: 34.6551\n",
      "Epoch 14/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 21.7809 - val_loss: 33.7791\n",
      "Epoch 15/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 21.3357 - val_loss: 33.4163\n",
      "Epoch 16/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 20.2408 - val_loss: 35.2071\n",
      "Epoch 17/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 19.7518 - val_loss: 33.8351\n",
      "Epoch 18/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 19.0819 - val_loss: 33.4153\n",
      "Epoch 19/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 18.5803 - val_loss: 32.5144\n",
      "Epoch 20/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 17.9454 - val_loss: 32.5008\n",
      "Epoch 21/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 17.2556 - val_loss: 31.7341\n",
      "Epoch 22/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 16.6049 - val_loss: 33.0225\n",
      "Epoch 23/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 16.2271 - val_loss: 32.0217\n",
      "Epoch 24/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 15.9358 - val_loss: 31.8842\n",
      "Epoch 25/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 15.4008 - val_loss: 31.4191\n",
      "Epoch 26/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 14.8487 - val_loss: 32.1704\n",
      "Epoch 27/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 14.5421 - val_loss: 32.4864\n",
      "Epoch 28/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 14.1018 - val_loss: 30.8837\n",
      "Epoch 29/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 13.5647 - val_loss: 34.2848\n",
      "Epoch 30/100\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 13.3258 - val_loss: 30.9139\n",
      "Epoch 31/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 13.0200 - val_loss: 31.0903\n",
      "Epoch 32/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 12.8300 - val_loss: 31.5745\n",
      "Epoch 33/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 12.3379 - val_loss: 30.8476\n",
      "Epoch 34/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 12.3526 - val_loss: 30.2837\n",
      "Epoch 35/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 11.8504 - val_loss: 30.6456\n",
      "Epoch 36/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 11.4397 - val_loss: 31.4395\n",
      "Epoch 37/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 11.1099 - val_loss: 30.3721\n",
      "Epoch 38/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 10.8356 - val_loss: 30.5192\n",
      "Epoch 39/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 10.6926 - val_loss: 31.2848\n",
      "Epoch 40/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 10.5936 - val_loss: 31.1163\n",
      "Epoch 41/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 10.3896 - val_loss: 30.2793\n",
      "Epoch 42/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 9.9230 - val_loss: 30.5006\n",
      "Epoch 43/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 9.7904 - val_loss: 30.1322\n",
      "Epoch 44/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 9.6927 - val_loss: 30.9696\n",
      "Epoch 45/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 9.4006 - val_loss: 30.7330\n",
      "Epoch 46/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 9.1010 - val_loss: 30.7602\n",
      "Epoch 47/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 8.9833 - val_loss: 30.4165\n",
      "Epoch 48/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 8.6529 - val_loss: 30.1897\n",
      "Epoch 49/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 8.6874 - val_loss: 30.7984\n",
      "Epoch 50/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 8.5129 - val_loss: 30.5438\n",
      "Epoch 51/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 8.5497 - val_loss: 30.5728\n",
      "Epoch 52/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.9103 - val_loss: 31.4474\n",
      "Epoch 53/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.8974 - val_loss: 30.3591\n",
      "Epoch 54/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.4936 - val_loss: 30.3311\n",
      "Epoch 55/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.6316 - val_loss: 31.1182\n",
      "Epoch 56/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.3228 - val_loss: 31.6826\n",
      "Epoch 57/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.3003 - val_loss: 31.1210\n",
      "Epoch 58/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 7.2508 - val_loss: 30.9801\n",
      "Epoch 59/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.9632 - val_loss: 30.6477\n",
      "Epoch 60/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.8206 - val_loss: 31.3453\n",
      "Epoch 61/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.7577 - val_loss: 30.9678\n",
      "Epoch 62/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.5678 - val_loss: 30.5064\n",
      "Epoch 63/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.4321 - val_loss: 30.4206\n",
      "Epoch 64/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.3145 - val_loss: 30.2834\n",
      "Epoch 65/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.2188 - val_loss: 30.6934\n",
      "Epoch 66/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.9284 - val_loss: 31.2277\n",
      "Epoch 67/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 6.0135 - val_loss: 30.8490\n",
      "Epoch 68/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.7290 - val_loss: 30.7342\n",
      "Epoch 69/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.8076 - val_loss: 30.9841\n",
      "Epoch 70/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.7622 - val_loss: 30.4893\n",
      "Epoch 71/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.7708 - val_loss: 30.3812\n",
      "Epoch 72/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.3910 - val_loss: 31.0873\n",
      "Epoch 73/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.2824 - val_loss: 31.2539\n",
      "Epoch 74/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.3069 - val_loss: 30.1769\n",
      "Epoch 75/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.1622 - val_loss: 31.4494\n",
      "Epoch 76/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 5.0549 - val_loss: 31.0579\n",
      "Epoch 77/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.9516 - val_loss: 31.2847\n",
      "Epoch 78/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.8752 - val_loss: 31.8573\n",
      "Epoch 79/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.8692 - val_loss: 31.3487\n",
      "Epoch 80/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.7094 - val_loss: 31.9703\n",
      "Epoch 81/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.7107 - val_loss: 30.7948\n",
      "Epoch 82/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.5776 - val_loss: 31.4701\n",
      "Epoch 83/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.3831 - val_loss: 31.4894\n",
      "Epoch 84/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.3665 - val_loss: 31.7872\n",
      "Epoch 85/100\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 4.3194 - val_loss: 31.5220\n",
      "Epoch 86/100\n",
      " 57/266 [=====>........................] - ETA: 0s - loss: 4.2530"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15560/1966097305.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 200\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "model.add(Dense(units=n_dense_units, activation=activation_function))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(3, activation='linear'))\n",
    "\n",
    "# Build model with input layer\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 16699.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (5124, 1) (5124, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1063.52it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "settings = dict(duration_of_trial=0., number_of_sources=1)\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "\n",
    "X_test = np.squeeze(np.stack([eeg.average().data for eeg in sim_test.eeg_data]))\n",
    "X_test = np.stack([(x - np.mean(x)) / np.std(x) for x in X_test], axis=0)\n",
    "y_test = np.array([extent[0] for extent in sim_test.simulation_info.positions.values])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "distances = np.sqrt(((y_pred - y_test)**2).sum(axis=1))\n",
    "plt.figure()\n",
    "plt.hist(distances)\n",
    "plt.xlabel(\"Euclidean distance to true maximum\")\n",
    "plt.title(f\"median={np.median(distances)}\")\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "brain = stc.plot(**plot_params)\n",
    "brain.add_foci(y_pred[0, :], coords_as_verts=False, hemi=\"lh\", color=\"red\")\n",
    "brain.add_foci(y_pred[0, :], coords_as_verts=False, hemi=\"rh\", color=\"red\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19.31613181,  2.04705525, 14.1597461 ,  5.27064398,  1.24725668,\n",
       "        4.36241445, 17.47596471,  6.62718545, 13.15538461,  7.84819766,\n",
       "        8.61954005,  6.39945473, 14.76710841,  3.26612865,  9.54420082,\n",
       "       10.87083289,  6.81587002,  6.41546564, 10.13584463, 10.49176123,\n",
       "        9.67520817,  2.31887653, 15.48955206,  3.6202502 ,  6.84626395,\n",
       "        1.67094146,  7.69778097, 22.35361877,  8.36259844,  3.94436435,\n",
       "        5.74545302,  4.08565251,  6.28159072,  1.52095045,  8.10723901,\n",
       "       30.65317859,  9.1726696 ,  9.52249766,  4.59374964,  8.80919645,\n",
       "        4.39782712,  4.68890662, 12.64326095,  1.84111413, 11.10927796,\n",
       "        4.1710168 ,  8.98015923,  7.91337976,  2.71961383, 19.25576127,\n",
       "       10.95768192, 19.59450877,  7.20493244,  4.57305047, 13.62665406,\n",
       "        4.09885352,  7.0607517 ,  5.96046843,  2.61638712, 15.2390202 ,\n",
       "       15.50284665,  5.4391461 ,  9.02056581,  5.72645869, 13.97868068,\n",
       "        5.93857676, 10.61861199,  4.86034232,  7.82606373,  2.76642445,\n",
       "        3.17647927,  5.55145565,  7.81981132, 10.64737952,  9.13750376,\n",
       "        7.14999571,  9.26159944,  5.19485218, 24.11574343,  3.86282422,\n",
       "        5.44866103, 23.68734435,  8.87276304,  4.81279258,  4.52937482,\n",
       "        8.18618838,  4.56698731, 17.08104034, 17.79274529, 11.16341932,\n",
       "       11.74374684,  6.58170509,  9.50132277,  3.55311607,  4.37185283,\n",
       "        4.08723686, 18.05004033,  4.44913975,  6.13931921,  6.92947484])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
