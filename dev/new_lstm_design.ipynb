{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.6s remaining:    2.7s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    1.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico2\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnanny import verbose\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mne\n",
    "def prep_data(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    for i, (x_sample, y_sample) in enumerate(zip(X, y)):\n",
    "        X[i] = np.stack([(x - np.mean(x)) / np.std(x) for x in x_sample.T], axis=0).T\n",
    "        y[i] = np.stack([ y / np.max(abs(y)) for y in y_sample.T], axis=0).T\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        loss1 = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        loss2 = sparsity(None, y_pred)\n",
    "        return loss1 + loss2 * 1e-3\n",
    "    return loss\n",
    "\n",
    "from esinet.evaluate import auc_metric, eval_auc, eval_nmse, eval_mean_localization_error\n",
    "\n",
    "def eval(y_true, y_hat):\n",
    "    n_samples = y_true.shape[0]\n",
    "    n_time = y_true.shape[1]\n",
    "    aucs = np.zeros((n_samples, n_time))\n",
    "    mles = np.zeros((n_samples, n_time))\n",
    "    nmses = np.zeros((n_samples, n_time))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_time):\n",
    "            aucs[i,j] = np.mean(eval_auc(y_true[i,j], y_hat[i,j], pos))\n",
    "            nmses[i,j] = eval_nmse(y_true[i,j], y_hat[i,j])\n",
    "            mles[i,j] = eval_mean_localization_error(y_true[i,j], y_hat[i,j], pos)\n",
    "\n",
    "    return aucs, nmses, mles\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.5, dtype=tf.float32)\n",
    "\n",
    "class Compressor:\n",
    "    ''' Compression using Graph Fourier Transform\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, fwd, k=600):\n",
    "        A = mne.spatial_src_adjacency(fwd[\"src\"], verbose=0).toarray()\n",
    "        D = np.diag(A.sum(axis=0))\n",
    "        L = D-A\n",
    "        U, s, V = np.linalg.svd(L)\n",
    "\n",
    "        self.U = U[:, -k:]\n",
    "        self.s = s[-k:]\n",
    "        self.V = V[:, -k:]\n",
    "        return self\n",
    "        \n",
    "    def encode(self, X):\n",
    "        ''' Encodes a true signal X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            True signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_comp : numpy.ndarray\n",
    "            Compressed signal\n",
    "        '''\n",
    "        X_comp = self.U.T @ X\n",
    "\n",
    "        return X_comp\n",
    "\n",
    "    def decode(self, X_comp):\n",
    "        ''' Decodes a compressed signal X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Compressed signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_unfold : numpy.ndarray\n",
    "            Decoded signal\n",
    "        '''\n",
    "        X_unfold = self.U @ X_comp\n",
    "        return X_unfold\n",
    "\n",
    "\n",
    "def generate_batches(batch_size, epochs=30, n_max=1000, settings=None):\n",
    "    if settings is None:\n",
    "        settings=dict(duration_of_trial=0.20, extents=(1,40), \n",
    "                      number_of_sources=(1,15), target_snr=1)\n",
    "\n",
    "    n_samples = int(n_max - (n_max % batch_size))\n",
    "    n_batches = int(n_samples / batch_size)\n",
    "    print(n_samples, n_batches)\n",
    "    while True:\n",
    "        sim = Simulation(fwd, info, settings=settings, verbose=0).simulate(n_samples=n_samples)\n",
    "        X, y = prep_data(sim)\n",
    "\n",
    "        # yield X, y\n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_batches-1):\n",
    "                X_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "        \n",
    "                yield X_batch, y_batch\n",
    "gen = generate_batches(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 199.98it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 87.01it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X, y = prep_data(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp = Compressor()\n",
    "# comp.fit(fwd)\n",
    "# y_comp = np.stack([comp.encode(yy.T).T for yy in y], axis=0)\n",
    "\n",
    "y_comp = y\n",
    "\n",
    "# %matplotlib qt\n",
    "# plt.figure()\n",
    "# plt.imshow(y[0], aspect=y[0].shape[1] / y[0].shape[0])\n",
    "# plt.figure()\n",
    "# plt.imshow(y_comp[0], aspect=y_comp[0].shape[1] / y_comp[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM SINGLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM DOUBLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "fc0 = TimeDistributed(Dense(n_channels, \n",
    "            activation=activation_function), \n",
    "            name='FC0')(inputs)\n",
    "fc0 = Dropout(dropout)(fc0)\n",
    "\n",
    "# Context\n",
    "lstm0 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM0')(fc0)\n",
    "cat = concatenate([lstm0, fc0])\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(cat)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "encoder_outputs, state_h, state_c = LSTM(n_lstm_units, return_state=True)(inputs)\n",
    "\n",
    "decoder_LSTM = LSTM(n_lstm_units, return_state=True, return_sequences=True)   \n",
    "decoder_outputs, _, _ = decoder_LSTM(inputs, initial_state=[state_h, state_c])\n",
    "outputs = TimeDistributed(Dense(n_dipoles, activation='linear'))(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Encoder-Decoder')\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.25, extents=(1,2), number_of_sources=6, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='LSTM1'))(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(lstm1)\n",
    "\n",
    "\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='LSTM_Old')\n",
    "\n",
    "\n",
    "model2.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model2.summary()\n",
    "model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "model3 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='FC')\n",
    "\n",
    "\n",
    "model3.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model3.summary()\n",
    "model3.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 100\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_dipoles, 1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "# cnn = Conv2D(n_filters, kernel_size, dilation_rate=dilation_rate, padding=\"valid\")(inputs)\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "\n",
    "cnn = Reshape((n_time, n_dense_units, 1))(fc1)\n",
    "cnn = Conv2D(n_filters, (3, 100), dilation_rate=dilation_rate, padding=\"same\", activation=activation_function)(cnn)\n",
    "cnn = Reshape((n_time, n_filters*n_dense_units))(cnn)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(300, \n",
    "            activation=activation_function),\n",
    "            name='FC2')(cnn)\n",
    "\n",
    "fc3 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC3')(fc2)\n",
    "\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "model4 = tf.keras.Model(inputs=inputs, outputs=fc3, name='FC-CNN')\n",
    "\n",
    "\n",
    "model4.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model4.summary()\n",
    "model4.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model4.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC - context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC-Context\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 20, 61)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1220)         0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context (Dense)                 (None, 50)           61050       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_5 (RepeatVector)  (None, 20, 50)       0           context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 20, 111)      0           Input[0][0]                      \n",
      "                                                                 repeat_vector_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FC1 (TimeDistributed)           (None, 20, 300)      33600       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "FC2 (TimeDistributed)           (None, 20, 324)      97524       FC1[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 192,174\n",
      "Trainable params: 192,174\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "992 31\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 349.01it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 26810.34it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 96.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 900/1000 [==========================>...] - ETA: 0s - loss: -0.1983Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 350.81it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 23616.87it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 98.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 28s 28ms/step - loss: -0.1940\n",
      "Epoch 2/300\n",
      " 796/1000 [======================>.......] - ETA: 1s - loss: -0.2441Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 355.13it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 27559.74it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 98.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 28s 28ms/step - loss: -0.2318\n",
      "Epoch 3/300\n",
      " 695/1000 [===================>..........] - ETA: 2s - loss: -0.2637Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 344.56it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 26096.18it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 94.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 29s 29ms/step - loss: -0.2439\n",
      "Epoch 4/300\n",
      " 600/1000 [=================>............] - ETA: 3s - loss: -0.2758Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:03<00:00, 326.04it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 21097.42it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 91.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 30s 30ms/step - loss: -0.2508\n",
      "Epoch 5/300\n",
      " 496/1000 [=============>................] - ETA: 4s - loss: -0.2863Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 350.57it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 23607.36it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 98.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 28s 28ms/step - loss: -0.2548\n",
      "Epoch 6/300\n",
      " 399/1000 [==========>...................] - ETA: 5s - loss: -0.2925Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:02<00:00, 350.60it/s]\n",
      "100%|██████████| 992/992 [00:00<00:00, 24801.06it/s]\n",
      "100%|██████████| 992/992 [00:10<00:00, 97.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 28s 28ms/step - loss: -0.2583\n",
      "Epoch 7/300\n",
      " 245/1000 [======>.......................] - ETA: 6s - loss: -0.2998"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14204/3672144059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# model5.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape, Dropout, concatenate, RepeatVector\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "n_embed = 50\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_embed)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "\n",
    "context = Flatten()(inputs)\n",
    "context = Dense(n_embed, activation=activation_function, name=\"context\")(context)\n",
    "# context = Reshape(new_shape)(context)\n",
    "context = RepeatVector(n_time)(context)\n",
    "\n",
    "context = concatenate([inputs, context])\n",
    "# context = Dropout(0.2)(context)\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(context)\n",
    "# cat = concatenate([fc1, context])\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "model5 = tf.keras.Model(inputs=inputs, outputs=fc2, name='FC-Context')\n",
    "# model5.add_loss(1e1*K.mean(K.abs(fc2)))\n",
    "\n",
    "model5.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model5.summary()\n",
    "# model5.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)\n",
    "model5.fit(generate_batches(batch_size), steps_per_epoch=1000, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 221.96it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x255c5f17460>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.27802494 0.31430394 0.44143655]\n",
      "Using control points [0.27802494 0.31430394 0.44143655]\n",
      "Using control points [7.03754526e-15 1.77587060e-12 8.64564183e-08]\n",
      "Using control points [0.1903404  0.24775854 0.35707219]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model5.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import sys; sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "solver = SolverChampagne()\n",
    "solver.make_inverse_operator(fwd)\n",
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, number_of_sources=(1,10), extents=(1,40))\n",
    "# settings = dict(duration_of_trial=0.25, number_of_sources=5, extents=(1,2))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "# models = [model, model2, model3]\n",
    "# aucs = []\n",
    "# mles = []\n",
    "# nmses = []\n",
    "# for net in models:\n",
    "#     y_hat = net.predict(X_test)\n",
    "#     auc, nmse, mle = eval(y_test, y_hat)\n",
    "#     aucs.append( auc )\n",
    "#     nmses.append( nmse )\n",
    "#     mles.append( mle )\n",
    "#     print(f\"{net.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "# y_hat = np.stack([solver.apply_inverse_operator(epochs.average()).data for epochs in tqdm(sim_test.eeg_data)], axis=0)\n",
    "# y_hat = np.swapaxes(y_hat, 1, 2)\n",
    "# auc, nmse, mle = eval(y_test, y_hat)\n",
    "# aucs.append( auc )\n",
    "# nmses.append( nmse )\n",
    "# mles.append( mle )\n",
    "\n",
    "# models.append(solver)\n",
    "# print(f\"{solver.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "idx = 0\n",
    "n = sim_test.simulation_info[\"number_of_sources\"].values[idx]\n",
    "print(f\"{n} sources\")\n",
    "\n",
    "# PLOTTING BRAINS\n",
    "stc = sim_test.source_data[idx]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "y_hat = model5.predict(X_test)[idx]\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=model5.name))\n",
    "r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "print(f\"{model5.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# y_hat = model2.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model2.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model2.name}: r={r:.2f}\")\n",
    "\n",
    "# y_hat = model3.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model3.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model3.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# evoked = sim_test.eeg_data[idx].average()\n",
    "# stc_hat = solver.apply_inverse_operator(evoked)\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "# y_hat = stc_hat.data\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{solver.name}: r={r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "names = [m.name for m in models]\n",
    "xticks = (np.arange(len(models)), names)\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(aucs,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.title(\"AUC\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(mles,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.title(\"MLE\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(nmses,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"NMSE\")\n",
    "plt.title(\"NMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
