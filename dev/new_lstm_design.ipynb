{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# from copy import deepcopy\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mne\n",
    "def prep_data(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    for i, (x_sample, y_sample) in enumerate(zip(X, y)):\n",
    "        X[i] = np.stack([(x - np.mean(x)) / np.std(x) for x in x_sample.T], axis=0).T\n",
    "        y[i] = np.stack([ y / np.max(abs(y)) for y in y_sample.T], axis=0).T\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        loss1 = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        loss2 = sparsity(None, y_pred)\n",
    "        return loss1 + loss2 * 1e-3\n",
    "    return loss\n",
    "\n",
    "from esinet.evaluate import auc_metric, eval_auc, eval_nmse, eval_mean_localization_error\n",
    "\n",
    "def eval(y_true, y_hat):\n",
    "    n_samples = y_true.shape[0]\n",
    "    n_time = y_true.shape[1]\n",
    "    aucs = np.zeros((n_samples, n_time))\n",
    "    mles = np.zeros((n_samples, n_time))\n",
    "    nmses = np.zeros((n_samples, n_time))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_time):\n",
    "            aucs[i,j] = np.mean(eval_auc(y_true[i,j], y_hat[i,j], pos))\n",
    "            nmses[i,j] = eval_nmse(y_true[i,j], y_hat[i,j])\n",
    "            mles[i,j] = eval_mean_localization_error(y_true[i,j], y_hat[i,j], pos)\n",
    "\n",
    "    return aucs, nmses, mles\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.5, dtype=tf.float32)\n",
    "\n",
    "class Compressor:\n",
    "    ''' Compression using Graph Fourier Transform\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, fwd, k=600):\n",
    "        A = mne.spatial_src_adjacency(fwd[\"src\"], verbose=0).toarray()\n",
    "        D = np.diag(A.sum(axis=0))\n",
    "        L = D-A\n",
    "        U, s, V = np.linalg.svd(L)\n",
    "\n",
    "        self.U = U[:, -k:]\n",
    "        self.s = s[-k:]\n",
    "        self.V = V[:, -k:]\n",
    "        return self\n",
    "        \n",
    "    def encode(self, X):\n",
    "        ''' Encodes a true signal X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            True signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_comp : numpy.ndarray\n",
    "            Compressed signal\n",
    "        '''\n",
    "        X_comp = self.U.T @ X\n",
    "\n",
    "        return X_comp\n",
    "\n",
    "    def decode(self, X_comp):\n",
    "        ''' Decodes a compressed signal X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Compressed signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_unfold : numpy.ndarray\n",
    "            Decoded signal\n",
    "        '''\n",
    "        X_unfold = self.U @ X_comp\n",
    "        return X_unfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:21<00:00, 235.06it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 15966.18it/s]\n",
      "100%|██████████| 5000/5000 [01:09<00:00, 71.58it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5000\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X, y = prep_data(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp = Compressor()\n",
    "# comp.fit(fwd)\n",
    "# y_comp = np.stack([comp.encode(yy.T).T for yy in y], axis=0)\n",
    "\n",
    "y_comp = y\n",
    "\n",
    "# %matplotlib qt\n",
    "# plt.figure()\n",
    "# plt.imshow(y[0], aspect=y[0].shape[1] / y[0].shape[0])\n",
    "# plt.figure()\n",
    "# plt.imshow(y_comp[0], aspect=y_comp[0].shape[1] / y_comp[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM SINGLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM DOUBLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "fc0 = TimeDistributed(Dense(n_channels, \n",
    "            activation=activation_function), \n",
    "            name='FC0')(inputs)\n",
    "fc0 = Dropout(dropout)(fc0)\n",
    "\n",
    "# Context\n",
    "lstm0 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM0')(fc0)\n",
    "cat = concatenate([lstm0, fc0])\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(cat)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y_comp.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "encoder_outputs, state_h, state_c = LSTM(n_lstm_units, return_state=True)(inputs)\n",
    "\n",
    "decoder_LSTM = LSTM(n_lstm_units, return_state=True, return_sequences=True)   \n",
    "decoder_outputs, _, _ = decoder_LSTM(inputs, initial_state=[state_h, state_c])\n",
    "outputs = TimeDistributed(Dense(n_dipoles, activation='linear'))(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Encoder-Decoder')\n",
    "model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.25, extents=(1,2), number_of_sources=6, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='LSTM1'))(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(lstm1)\n",
    "\n",
    "\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='LSTM_Old')\n",
    "\n",
    "\n",
    "model2.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "model2.summary()\n",
    "model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "model3 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='FC')\n",
    "\n",
    "\n",
    "model3.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model3.summary()\n",
    "model3.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 100\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_dipoles, 1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "# cnn = Conv2D(n_filters, kernel_size, dilation_rate=dilation_rate, padding=\"valid\")(inputs)\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "\n",
    "cnn = Reshape((n_time, n_dense_units, 1))(fc1)\n",
    "cnn = Conv2D(n_filters, (3, 100), dilation_rate=dilation_rate, padding=\"same\", activation=activation_function)(cnn)\n",
    "cnn = Reshape((n_time, n_filters*n_dense_units))(cnn)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(300, \n",
    "            activation=activation_function),\n",
    "            name='FC2')(cnn)\n",
    "\n",
    "fc3 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC3')(fc2)\n",
    "\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "model4 = tf.keras.Model(inputs=inputs, outputs=fc3, name='FC-CNN')\n",
    "\n",
    "\n",
    "model4.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model4.summary()\n",
    "model4.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model4.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC - context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC-Context\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 20, 61)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1220)         0           ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " context (Dense)                (None, 1)            1221        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 20, 1)       0           ['context[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 20, 62)       0           ['Input[0][0]',                  \n",
      "                                                                  'repeat_vector_1[0][0]']        \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, 20, 300)      18900       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, 20, 1284)     386484      ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.abs_1 (TFOpLambda)     (None, 20, 1284)     0           ['FC2[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  ()                  0           ['tf.math.abs_1[0][0]']          \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  ()                  0           ['tf.math.reduce_mean_1[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_loss_1 (AddLoss)           ()                   0           ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 406,605\n",
      "Trainable params: 406,605\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1976 - val_loss: -0.0962\n",
      "Epoch 2/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.1480 - val_loss: -0.1969\n",
      "Epoch 3/50\n",
      "133/133 [==============================] - 1s 9ms/step - loss: -0.2080 - val_loss: -0.2274\n",
      "Epoch 4/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.2349 - val_loss: -0.2542\n",
      "Epoch 5/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.2473 - val_loss: -0.2552\n",
      "Epoch 6/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.2569 - val_loss: -0.2684\n",
      "Epoch 7/50\n",
      "133/133 [==============================] - 1s 9ms/step - loss: -0.2645 - val_loss: -0.2692\n",
      "Epoch 8/50\n",
      "133/133 [==============================] - 1s 10ms/step - loss: -0.2693 - val_loss: -0.2797\n",
      "Epoch 9/50\n",
      "133/133 [==============================] - 1s 9ms/step - loss: -0.2752 - val_loss: -0.2869\n",
      "Epoch 10/50\n",
      "133/133 [==============================] - 1s 9ms/step - loss: -0.2803 - val_loss: -0.2915\n",
      "Epoch 11/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.2828 - val_loss: -0.2966\n",
      "Epoch 12/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.2869 - val_loss: -0.3009\n",
      "Epoch 13/50\n",
      "133/133 [==============================] - 1s 11ms/step - loss: -0.2899 - val_loss: -0.3015\n",
      "Epoch 14/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.2936 - val_loss: -0.3048\n",
      "Epoch 15/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.2971 - val_loss: -0.3050\n",
      "Epoch 16/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.2988 - val_loss: -0.3082\n",
      "Epoch 17/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3006 - val_loss: -0.3117\n",
      "Epoch 18/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3035 - val_loss: -0.3141\n",
      "Epoch 19/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3060 - val_loss: -0.3140\n",
      "Epoch 20/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3077 - val_loss: -0.3130\n",
      "Epoch 21/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3091 - val_loss: -0.3199\n",
      "Epoch 22/50\n",
      "133/133 [==============================] - 1s 11ms/step - loss: -0.3110 - val_loss: -0.3203\n",
      "Epoch 23/50\n",
      "133/133 [==============================] - 2s 14ms/step - loss: -0.3120 - val_loss: -0.3193\n",
      "Epoch 24/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3138 - val_loss: -0.3249\n",
      "Epoch 25/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3163 - val_loss: -0.3284\n",
      "Epoch 26/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3154 - val_loss: -0.3230\n",
      "Epoch 27/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3187 - val_loss: -0.3242\n",
      "Epoch 28/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3186 - val_loss: -0.3281\n",
      "Epoch 29/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3218 - val_loss: -0.3318\n",
      "Epoch 30/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3193 - val_loss: -0.3351\n",
      "Epoch 31/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3228 - val_loss: -0.3264\n",
      "Epoch 32/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3221 - val_loss: -0.3355\n",
      "Epoch 33/50\n",
      "133/133 [==============================] - 1s 9ms/step - loss: -0.3242 - val_loss: -0.3344\n",
      "Epoch 34/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3254 - val_loss: -0.3327\n",
      "Epoch 35/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3254 - val_loss: -0.3366\n",
      "Epoch 36/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3255 - val_loss: -0.3361\n",
      "Epoch 37/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3268 - val_loss: -0.3340\n",
      "Epoch 38/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3268 - val_loss: -0.3373\n",
      "Epoch 39/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3285 - val_loss: -0.3395\n",
      "Epoch 40/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3284 - val_loss: -0.3351\n",
      "Epoch 41/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3290 - val_loss: -0.3357\n",
      "Epoch 42/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3306 - val_loss: -0.3435\n",
      "Epoch 43/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3301 - val_loss: -0.3396\n",
      "Epoch 44/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3304 - val_loss: -0.3359\n",
      "Epoch 45/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3303 - val_loss: -0.3382\n",
      "Epoch 46/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3305 - val_loss: -0.3443\n",
      "Epoch 47/50\n",
      "133/133 [==============================] - 1s 8ms/step - loss: -0.3307 - val_loss: -0.3419\n",
      "Epoch 48/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3314 - val_loss: -0.3403\n",
      "Epoch 49/50\n",
      "133/133 [==============================] - 1s 6ms/step - loss: -0.3324 - val_loss: -0.3388\n",
      "Epoch 50/50\n",
      "133/133 [==============================] - 1s 7ms/step - loss: -0.3322 - val_loss: -0.3367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267ccfdc490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape, Dropout, concatenate, RepeatVector\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "n_embed = 1\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_embed)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "\n",
    "context = Flatten()(inputs)\n",
    "context = Dense(n_embed, activation=activation_function, name=\"context\")(context)\n",
    "# context = Reshape(new_shape)(context)\n",
    "context = RepeatVector(n_time)(context)\n",
    "\n",
    "context = concatenate([inputs, context])\n",
    "# context = Dropout(0.2)(context)\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(context)\n",
    "# cat = concatenate([fc1, context])\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "model5 = tf.keras.Model(inputs=inputs, outputs=fc2, name='FC-Context')\n",
    "model5.add_loss(1e1*K.mean(K.abs(fc2)))\n",
    "\n",
    "model5.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model5.summary()\n",
    "model5.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 69.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x267b751f310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.03703328 0.04606918 0.12230216]\n",
      "Using control points [0.02217556 0.02717292 0.07537197]\n",
      "Using control points [0.00000000e+00 0.00000000e+00 2.87812185e-08]\n",
      "Using control points [0.01065652 0.01170133 0.02240586]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model5.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import sys; sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "solver = SolverChampagne()\n",
    "solver.make_inverse_operator(fwd)\n",
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, number_of_sources=(1,10), extents=(1,40))\n",
    "# settings = dict(duration_of_trial=0.25, number_of_sources=5, extents=(1,2))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "# models = [model, model2, model3]\n",
    "# aucs = []\n",
    "# mles = []\n",
    "# nmses = []\n",
    "# for net in models:\n",
    "#     y_hat = net.predict(X_test)\n",
    "#     auc, nmse, mle = eval(y_test, y_hat)\n",
    "#     aucs.append( auc )\n",
    "#     nmses.append( nmse )\n",
    "#     mles.append( mle )\n",
    "#     print(f\"{net.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "# y_hat = np.stack([solver.apply_inverse_operator(epochs.average()).data for epochs in tqdm(sim_test.eeg_data)], axis=0)\n",
    "# y_hat = np.swapaxes(y_hat, 1, 2)\n",
    "# auc, nmse, mle = eval(y_test, y_hat)\n",
    "# aucs.append( auc )\n",
    "# nmses.append( nmse )\n",
    "# mles.append( mle )\n",
    "\n",
    "# models.append(solver)\n",
    "# print(f\"{solver.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "idx = 0\n",
    "n = sim_test.simulation_info[\"number_of_sources\"].values[idx]\n",
    "print(f\"{n} sources\")\n",
    "\n",
    "# PLOTTING BRAINS\n",
    "stc = sim_test.source_data[idx]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "y_hat = model5.predict(X_test)[idx]\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=model5.name))\n",
    "r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "print(f\"{model5.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# y_hat = model2.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model2.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model2.name}: r={r:.2f}\")\n",
    "\n",
    "# y_hat = model3.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model3.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model3.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# evoked = sim_test.eeg_data[idx].average()\n",
    "# stc_hat = solver.apply_inverse_operator(evoked)\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "# y_hat = stc_hat.data\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{solver.name}: r={r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "names = [m.name for m in models]\n",
    "xticks = (np.arange(len(models)), names)\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(aucs,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.title(\"AUC\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(mles,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.title(\"MLE\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(nmses,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"NMSE\")\n",
    "plt.title(\"NMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
