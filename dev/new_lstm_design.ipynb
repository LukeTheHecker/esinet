{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mne\n",
    "def prep_data(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    for i, (x_sample, y_sample) in enumerate(zip(X, y)):\n",
    "        # X[i] = np.stack([(x - np.mean(x)) / np.std(x) for x in x_sample.T], axis=0).T\n",
    "        # y[i] = np.stack([ y / np.max(abs(y)) for y in y_sample.T], axis=0).T\n",
    "\n",
    "        X[i] = np.stack([x - np.mean(x) for x in x_sample.T], axis=0).T\n",
    "        X[i] /= X[i].std()\n",
    "        y[i] /= np.max(abs(y[i]))\n",
    "\n",
    "        \n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        loss1 = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        loss2 = sparsity(None, y_pred)\n",
    "        return loss1 + loss2 * 1e-3\n",
    "    return loss\n",
    "\n",
    "from esinet.evaluate import auc_metric, eval_auc, eval_nmse, eval_mean_localization_error\n",
    "\n",
    "def eval(y_true, y_hat):\n",
    "    n_samples = y_true.shape[0]\n",
    "    n_time = y_true.shape[1]\n",
    "    aucs = np.zeros((n_samples, n_time))\n",
    "    mles = np.zeros((n_samples, n_time))\n",
    "    nmses = np.zeros((n_samples, n_time))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_time):\n",
    "            aucs[i,j] = np.mean(eval_auc(y_true[i,j], y_hat[i,j], pos))\n",
    "            nmses[i,j] = eval_nmse(y_true[i,j], y_hat[i,j])\n",
    "            mles[i,j] = eval_mean_localization_error(y_true[i,j], y_hat[i,j], pos)\n",
    "\n",
    "    return aucs, nmses, mles\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.5, dtype=tf.float32)\n",
    "\n",
    "class Compressor:\n",
    "    ''' Compression using Graph Fourier Transform\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, fwd, k=600):\n",
    "        A = mne.spatial_src_adjacency(fwd[\"src\"], verbose=0).toarray()\n",
    "        D = np.diag(A.sum(axis=0))\n",
    "        L = D-A\n",
    "        U, s, V = np.linalg.svd(L)\n",
    "\n",
    "        self.U = U[:, -k:]\n",
    "        self.s = s[-k:]\n",
    "        self.V = V[:, -k:]\n",
    "        return self\n",
    "        \n",
    "    def encode(self, X):\n",
    "        ''' Encodes a true signal X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            True signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_comp : numpy.ndarray\n",
    "            Compressed signal\n",
    "        '''\n",
    "        X_comp = self.U.T @ X\n",
    "\n",
    "        return X_comp\n",
    "\n",
    "    def decode(self, X_comp):\n",
    "        ''' Decodes a compressed signal X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Compressed signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_unfold : numpy.ndarray\n",
    "            Decoded signal\n",
    "        '''\n",
    "        X_unfold = self.U @ X_comp\n",
    "        return X_unfold\n",
    "\n",
    "\n",
    "def generate_batches(batch_size, epochs=30, n_max=1000, settings=None):\n",
    "    if settings is None:\n",
    "        settings=dict(duration_of_trial=0.20, extents=(1,40), \n",
    "                      number_of_sources=(1,15), target_snr=1)\n",
    "\n",
    "    n_samples = int(n_max - (n_max % batch_size))\n",
    "    n_batches = int(n_samples / batch_size)\n",
    "    print(n_samples, n_batches)\n",
    "    while True:\n",
    "        sim = Simulation(fwd, info, settings=settings, verbose=0).simulate(n_samples=n_samples)\n",
    "        X, y = prep_data(sim)\n",
    "\n",
    "        # yield X, y\n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_batches-1):\n",
    "                X_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "        \n",
    "                yield X_batch, y_batch\n",
    "gen = generate_batches(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on 1/f noise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:06<00:00, 75.28it/s] \n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2336.33it/s]\n",
      "100%|██████████| 5000/5000 [01:18<00:00, 63.60it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5000\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=(1, 15), method=\"noise\", beta_source=4)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X, y = prep_data(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp = Compressor()\n",
    "# comp.fit(fwd)\n",
    "# y_comp = np.stack([comp.encode(yy.T).T for yy in y], axis=0)\n",
    "\n",
    "# y_comp = y\n",
    "\n",
    "# %matplotlib qt\n",
    "# plt.figure()\n",
    "# plt.imshow(y[0], aspect=y[0].shape[1] / y[0].shape[0])\n",
    "# plt.figure()\n",
    "# plt.imshow(y_comp[0], aspect=y_comp[0].shape[1] / y_comp[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"huber\"\n",
    "# loss = tf.keras.losses.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM SINGLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, None, 300)    18600       ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, None, 300)    0           ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, None, 64)     64128       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, None, 1284)   386484      ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 1284)   83460       ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 1284)   0           ['FC2[0][0]',                    \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.abs_16 (TFOpLambda)    (None, None, 1284)   0           ['Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_16 (TFOpLa  ()                  0           ['tf.math.abs_16[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  ()                  0           ['tf.math.reduce_mean_16[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " add_loss_14 (AddLoss)          ()                   0           ['tf.math.multiply_15[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 552,672\n",
      "Trainable params: 552,672\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 5s 20ms/step - loss: 0.0310 - val_loss: 0.0265\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0260 - val_loss: 0.0251\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0249 - val_loss: 0.0244\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.0243 - val_loss: 0.0240\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.0239 - val_loss: 0.0236\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.0236 - val_loss: 0.0234\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0233 - val_loss: 0.0232\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.0231 - val_loss: 0.0230\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.0229 - val_loss: 0.0228\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0228 - val_loss: 0.0227\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0227 - val_loss: 0.0227\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0226 - val_loss: 0.0225\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.0225 - val_loss: 0.0224\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0224 - val_loss: 0.0224\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0223 - val_loss: 0.0223\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0222 - val_loss: 0.0221\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0222 - val_loss: 0.0222\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0221 - val_loss: 0.0221\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.0220 - val_loss: 0.0220\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0220 - val_loss: 0.0220\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0220 - val_loss: 0.0219\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0219 - val_loss: 0.0219\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.0219 - val_loss: 0.0219\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0219 - val_loss: 0.0219\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0217 - val_loss: 0.0218\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0217 - val_loss: 0.0218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb0d515850>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "from tensorflow.keras import backend as K\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "model.add_loss(K.mean(K.abs(mask))*1e-2)\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 168.02it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2182.21it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "error: 0.010041557252407074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1fb068ea280>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.32916539 0.38581702 0.54196449]\n",
      "Using control points [0.56020409 0.6503592  0.97683875]\n",
      "Using control points [4.31681776e-14 2.72470626e-13 1.12558118e-11]\n",
      "Using control points [0.07107591 0.0752193  0.1097129 ]\n",
      "Using control points [0.30229189 0.37536541 0.54561051]\n",
      "Using control points [0.64646505 0.78543492 1.11528787]\n",
      "Using control points [0.64646505 0.78543492 1.11528787]\n",
      "Using control points [0.68535402 0.84364348 1.24558203]\n",
      "Using control points [0.70009242 0.84312491 1.22084468]\n",
      "Using control points [0.79759048 0.95056859 1.38809427]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "# settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "settings_alt = dict(duration_of_trial=0.20, extents=(1, 40), number_of_sources=(1,15), target_snr=(1, 15))\n",
    "\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings_alt).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM DOUBLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " FC0 (TimeDistributed)          (None, None, 61)     3782        ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 61)     0           ['FC0[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM0 (Bidirectional)          (None, None, 64)     18240       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, 125)    0           ['LSTM0[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, None, 61)     7686        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, None, 61)     0           ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, None, 64)     18240       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, None, 1284)   79608       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 1284)   83460       ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 1284)   0           ['FC2[0][0]',                    \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.abs_2 (TFOpLambda)     (None, None, 1284)   0           ['Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  ()                  0           ['tf.math.abs_2[0][0]']          \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  ()                  0           ['tf.math.reduce_mean_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_loss_2 (AddLoss)           ()                   0           ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 211,016\n",
      "Trainable params: 211,016\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 9s 29ms/step - loss: 0.0067 - val_loss: 0.0032\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 3s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 3s 20ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 3s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa06b34190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "fc0 = TimeDistributed(Dense(n_channels, \n",
    "            activation=activation_function), \n",
    "            name='FC0')(inputs)\n",
    "fc0 = Dropout(dropout)(fc0)\n",
    "\n",
    "# Context\n",
    "lstm0 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM0')(fc0)\n",
    "cat = concatenate([lstm0, fc0])\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(cat)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "model.add_loss(K.mean(K.abs(mask))*1e-2)\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 261.28it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 18233.33it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 71.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "error: 0.002569999312981963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f7e3b47190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [3.02116368e-09 2.67798996e-08 3.51250970e-08]\n",
      "Using control points [8.25629933e-05 1.09109362e-04 6.67372005e-04]\n",
      "Using control points [3.02116368e-09 2.67798996e-08 3.51250970e-08]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder-Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        97280       ['Input[0][0]']                  \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 128),  97280       ['Input[0][0]',                  \n",
      "                                 (None, 128),                     'lstm[0][1]',                   \n",
      "                                 (None, 128)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 1284)  165636      ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 360,196\n",
      "Trainable params: 360,196\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 6s 24ms/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0024 - val_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa0f58f910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "encoder_outputs, state_h, state_c = LSTM(n_lstm_units, return_state=True)(inputs)\n",
    "\n",
    "decoder_LSTM = LSTM(n_lstm_units, return_state=True, return_sequences=True)   \n",
    "decoder_outputs, _, _ = decoder_LSTM(inputs, initial_state=[state_h, state_c])\n",
    "outputs = TimeDistributed(Dense(n_dipoles, activation='linear'))(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Encoder-Decoder')\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 263.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 20053.57it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "error: 0.002369122812524438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f9abcb6790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.03084668 0.03540283 0.06492488]\n",
      "Using control points [7.83109839e-15 2.70473606e-12 1.32622130e-08]\n",
      "Using control points [7.83109839e-15 2.70473606e-12 1.32622130e-08]\n",
      "Using control points [3.53347129e-15 1.22040444e-12 9.49015868e-09]\n",
      "Using control points [4.87417981e-16 1.68346370e-13 3.16518037e-08]\n",
      "Using control points [0.04085439 0.0593598  0.11993631]\n",
      "Using control points [1.15491897e-15 3.98890530e-13 3.31839426e-08]\n",
      "Using control points [0.04770753 0.06768148 0.13079534]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='LSTM1'))(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(lstm1)\n",
    "\n",
    "\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='LSTM_Old')\n",
    "\n",
    "\n",
    "model2.compile(loss=loss, optimizer=\"adam\")\n",
    "model2.summary()\n",
    "model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, None, 61)]        0         \n",
      "                                                                 \n",
      " FC1 (TimeDistributed)       (None, None, 300)         18600     \n",
      "                                                                 \n",
      " FC2 (TimeDistributed)       (None, None, 1284)        386484    \n",
      "                                                                 \n",
      " tf.math.abs_17 (TFOpLambda)  (None, None, 1284)       0         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_17 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.math.multiply_16 (TFOpLa  ()                       0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " add_loss_15 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 405,084\n",
      "Trainable params: 405,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 2s 9ms/step - loss: 0.0278 - val_loss: 0.0243\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0237\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0234 - val_loss: 0.0234\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0233\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0233\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f790ef6c40>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "model3 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='FC')\n",
    "model3.add_loss(K.mean(K.abs(direct_out))*1e-2)\n",
    "\n",
    "\n",
    "model3.compile(loss=loss, optimizer=\"adam\")\n",
    "\n",
    "model3.summary()\n",
    "model3.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 93.54it/s] \n",
      "100%|██████████| 200/200 [00:00<00:00, 2099.70it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "error: 0.010495411232113838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1fb056dac10>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings_alt = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=(1,15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings_alt).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model3.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 100\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_dipoles, 1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "# cnn = Conv2D(n_filters, kernel_size, dilation_rate=dilation_rate, padding=\"valid\")(inputs)\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "\n",
    "cnn = Reshape((n_time, n_dense_units, 1))(fc1)\n",
    "cnn = Conv2D(n_filters, (3, 100), dilation_rate=dilation_rate, padding=\"same\", activation=activation_function)(cnn)\n",
    "cnn = Reshape((n_time, n_filters*n_dense_units))(cnn)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(300, \n",
    "            activation=activation_function),\n",
    "            name='FC2')(cnn)\n",
    "\n",
    "fc3 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC3')(fc2)\n",
    "\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "model4 = tf.keras.Model(inputs=inputs, outputs=fc3, name='FC-CNN')\n",
    "\n",
    "\n",
    "model4.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model4.summary()\n",
    "model4.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model4.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC - context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC-Context\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 20, 61)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1220)         0           ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " context (Dense)                (None, 200)          244200      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 20, 200)     0           ['context[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 20, 261)      0           ['Input[0][0]',                  \n",
      "                                                                  'repeat_vector_2[0][0]']        \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, 20, 300)      78600       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, 20, 1284)     386484      ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.abs_7 (TFOpLambda)     (None, 20, 1284)     0           ['FC2[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_7 (TFOpLam  ()                  0           ['tf.math.abs_7[0][0]']          \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  ()                  0           ['tf.math.reduce_mean_7[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_loss_5 (AddLoss)           ()                   0           ['tf.math.multiply_6[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 709,284\n",
      "Trainable params: 709,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0146 - val_loss: 0.0047\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa0f553910>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape, Dropout, concatenate, RepeatVector\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "n_embed = 200\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_embed)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "\n",
    "context = Flatten()(inputs)\n",
    "context = Dense(n_embed, activation=activation_function, name=\"context\")(context)\n",
    "# context = Reshape(new_shape)(context)\n",
    "context = RepeatVector(n_time)(context)\n",
    "\n",
    "context = concatenate([inputs, context])\n",
    "# context = Dropout(0.2)(context)\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(context)\n",
    "# cat = concatenate([fc1, context])\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "model5 = tf.keras.Model(inputs=inputs, outputs=fc2, name='FC-Context')\n",
    "model5.add_loss(K.mean(K.abs(fc2)) * 1e-2)\n",
    "\n",
    "model5.compile(loss=loss, optimizer=\"adam\")\n",
    "\n",
    "model5.summary()\n",
    "model5.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)\n",
    "# model5.fit(generate_batches(batch_size), steps_per_epoch=1000, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 271.17it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 20053.09it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 58.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "error: 0.002958708442747593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f9f8e4a430>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.02553031 0.02805882 0.03966862]\n",
      "Using control points [0.00000000e+00 3.59008654e-09 1.90625137e-08]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model5.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model5.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import sys; sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "solver = SolverChampagne()\n",
    "solver.make_inverse_operator(fwd)\n",
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, number_of_sources=(1,10), extents=(1,40))\n",
    "# settings = dict(duration_of_trial=0.25, number_of_sources=5, extents=(1,2))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "# models = [model, model2, model3]\n",
    "# aucs = []\n",
    "# mles = []\n",
    "# nmses = []\n",
    "# for net in models:\n",
    "#     y_hat = net.predict(X_test)\n",
    "#     auc, nmse, mle = eval(y_test, y_hat)\n",
    "#     aucs.append( auc )\n",
    "#     nmses.append( nmse )\n",
    "#     mles.append( mle )\n",
    "#     print(f\"{net.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "# y_hat = np.stack([solver.apply_inverse_operator(epochs.average()).data for epochs in tqdm(sim_test.eeg_data)], axis=0)\n",
    "# y_hat = np.swapaxes(y_hat, 1, 2)\n",
    "# auc, nmse, mle = eval(y_test, y_hat)\n",
    "# aucs.append( auc )\n",
    "# nmses.append( nmse )\n",
    "# mles.append( mle )\n",
    "\n",
    "# models.append(solver)\n",
    "# print(f\"{solver.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "idx = 0\n",
    "n = sim_test.simulation_info[\"number_of_sources\"].values[idx]\n",
    "print(f\"{n} sources\")\n",
    "\n",
    "# PLOTTING BRAINS\n",
    "stc = sim_test.source_data[idx]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "y_hat = model5.predict(X_test)[idx]\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=model5.name))\n",
    "r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "print(f\"{model5.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# y_hat = model2.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model2.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model2.name}: r={r:.2f}\")\n",
    "\n",
    "# y_hat = model3.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model3.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model3.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# evoked = sim_test.eeg_data[idx].average()\n",
    "# stc_hat = solver.apply_inverse_operator(evoked)\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "# y_hat = stc_hat.data\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{solver.name}: r={r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "names = [m.name for m in models]\n",
    "xticks = (np.arange(len(models)), names)\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(aucs,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.title(\"AUC\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(mles,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.title(\"MLE\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(nmses,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"NMSE\")\n",
    "plt.title(\"NMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9587d79750f5d7fc5c0560e15a7a8a49dff11015373bda407c2fe4ab31d0fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
