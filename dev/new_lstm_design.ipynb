{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: How simulations define your predictions\n",
    "The inverse problem has no unique solution as it is ill-posed. In order to solve it we need to constraint the space of possible solutions. While inverse solutions like minimum-norm estimates have an explicit constraint of minimum-energy, the constraints with esinet are implicit and mostly shaped by the simulations.\n",
    "\n",
    "This tutorial aims the relation between simulation parameters and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet import util\n",
    "from esinet import Simulation\n",
    "from esinet import Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "plot_params = dict(surface='white', hemi='both', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Forward model\n",
    "First we create a template forward model which comes with the esinet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    3.2s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    3.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(sfreq=100)\n",
    "fwd = create_forward_model(sampling=\"ico3\", info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mne\n",
    "def prep_data(sim):\n",
    "    X = np.squeeze(np.stack([eeg.average().data for eeg in sim.eeg_data]))\n",
    "    y = np.squeeze(np.stack([src.data for src in sim.source_data]))\n",
    "    for i, (x_sample, y_sample) in enumerate(zip(X, y)):\n",
    "        # X[i] = np.stack([(x - np.mean(x)) / np.std(x) for x in x_sample.T], axis=0).T\n",
    "        # y[i] = np.stack([ y / np.max(abs(y)) for y in y_sample.T], axis=0).T\n",
    "\n",
    "        X[i] = np.stack([x - np.mean(x) for x in x_sample.T], axis=0).T\n",
    "        X[i] /= X[i].std()\n",
    "        y[i] /= np.max(abs(y[i]))\n",
    "\n",
    "        \n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        loss1 = tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        loss2 = sparsity(None, y_pred)\n",
    "        return loss1 + loss2 * 1e-3\n",
    "    return loss\n",
    "\n",
    "from esinet.evaluate import auc_metric, eval_auc, eval_nmse, eval_mean_localization_error\n",
    "\n",
    "def eval(y_true, y_hat):\n",
    "    n_samples = y_true.shape[0]\n",
    "    n_time = y_true.shape[1]\n",
    "    aucs = np.zeros((n_samples, n_time))\n",
    "    mles = np.zeros((n_samples, n_time))\n",
    "    nmses = np.zeros((n_samples, n_time))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_time):\n",
    "            aucs[i,j] = np.mean(eval_auc(y_true[i,j], y_hat[i,j], pos))\n",
    "            nmses[i,j] = eval_nmse(y_true[i,j], y_hat[i,j])\n",
    "            mles[i,j] = eval_mean_localization_error(y_true[i,j], y_hat[i,j], pos)\n",
    "\n",
    "    return aucs, nmses, mles\n",
    "\n",
    "def threshold_activation(x):\n",
    "    return tf.cast(x > 0.5, dtype=tf.float32)\n",
    "\n",
    "class Compressor:\n",
    "    ''' Compression using Graph Fourier Transform\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, fwd, k=600):\n",
    "        A = mne.spatial_src_adjacency(fwd[\"src\"], verbose=0).toarray()\n",
    "        D = np.diag(A.sum(axis=0))\n",
    "        L = D-A\n",
    "        U, s, V = np.linalg.svd(L)\n",
    "\n",
    "        self.U = U[:, -k:]\n",
    "        self.s = s[-k:]\n",
    "        self.V = V[:, -k:]\n",
    "        return self\n",
    "        \n",
    "    def encode(self, X):\n",
    "        ''' Encodes a true signal X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            True signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_comp : numpy.ndarray\n",
    "            Compressed signal\n",
    "        '''\n",
    "        X_comp = self.U.T @ X\n",
    "\n",
    "        return X_comp\n",
    "\n",
    "    def decode(self, X_comp):\n",
    "        ''' Decodes a compressed signal X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Compressed signal\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        X_unfold : numpy.ndarray\n",
    "            Decoded signal\n",
    "        '''\n",
    "        X_unfold = self.U @ X_comp\n",
    "        return X_unfold\n",
    "\n",
    "\n",
    "def generate_batches(batch_size, epochs=30, n_max=1000, settings=None):\n",
    "    if settings is None:\n",
    "        settings=dict(duration_of_trial=0.20, extents=(1,40), \n",
    "                      number_of_sources=(1,15), target_snr=1)\n",
    "\n",
    "    n_samples = int(n_max - (n_max % batch_size))\n",
    "    n_batches = int(n_samples / batch_size)\n",
    "    print(n_samples, n_batches)\n",
    "    while True:\n",
    "        sim = Simulation(fwd, info, settings=settings, verbose=0).simulate(n_samples=n_samples)\n",
    "        X, y = prep_data(sim)\n",
    "\n",
    "        # yield X, y\n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_batches-1):\n",
    "                X_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "        \n",
    "                yield X_batch, y_batch\n",
    "gen = generate_batches(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 305.42it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 16945.45it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 78.36it/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,25), number_of_sources=(1, 4), target_snr=1e99)\n",
    "sim = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X, y = prep_data(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp = Compressor()\n",
    "# comp.fit(fwd)\n",
    "# y_comp = np.stack([comp.encode(yy.T).T for yy in y], axis=0)\n",
    "\n",
    "# y_comp = y\n",
    "\n",
    "# %matplotlib qt\n",
    "# plt.figure()\n",
    "# plt.imshow(y[0], aspect=y[0].shape[1] / y[0].shape[0])\n",
    "# plt.figure()\n",
    "# plt.imshow(y_comp[0], aspect=y_comp[0].shape[1] / y_comp[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = \"huber\"\n",
    "loss = tf.keras.losses.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM SINGLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, None, 61)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FC1 (TimeDistributed)           (None, None, 300)    18600       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 300)    0           FC1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "LSTM1 (Bidirectional)           (None, None, 64)     64128       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FC2 (TimeDistributed)           (None, None, 1284)   386484      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mask (TimeDistributed)          (None, None, 1284)   83460       LSTM1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 1284)   0           FC2[0][0]                        \n",
      "                                                                 Mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs (TFOpLambda)        (None, None, 1284)   0           Mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda ()                   0           tf.math.abs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   ()                   0           tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf.math.multiply[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 552,672\n",
      "Trainable params: 552,672\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "27/27 [==============================] - 6s 109ms/step - loss: -0.0196 - val_loss: -0.0479\n",
      "Epoch 2/30\n",
      "27/27 [==============================] - 1s 55ms/step - loss: -0.0791 - val_loss: -0.0658\n",
      "Epoch 3/30\n",
      "27/27 [==============================] - 2s 62ms/step - loss: -0.1095 - val_loss: -0.0779\n",
      "Epoch 4/30\n",
      "27/27 [==============================] - 2s 61ms/step - loss: -0.1336 - val_loss: -0.0872\n",
      "Epoch 5/30\n",
      "27/27 [==============================] - 2s 60ms/step - loss: -0.1541 - val_loss: -0.0950\n",
      "Epoch 6/30\n",
      "27/27 [==============================] - 2s 58ms/step - loss: -0.1739 - val_loss: -0.0999\n",
      "Epoch 7/30\n",
      "27/27 [==============================] - 2s 67ms/step - loss: -0.1921 - val_loss: -0.1053\n",
      "Epoch 8/30\n",
      "27/27 [==============================] - 2s 64ms/step - loss: -0.2131 - val_loss: -0.1132\n",
      "Epoch 9/30\n",
      "27/27 [==============================] - 1s 50ms/step - loss: -0.2344 - val_loss: -0.1169\n",
      "Epoch 10/30\n",
      "27/27 [==============================] - 1s 49ms/step - loss: -0.2558 - val_loss: -0.1240\n",
      "Epoch 11/30\n",
      "27/27 [==============================] - 1s 53ms/step - loss: -0.2789 - val_loss: -0.1274\n",
      "Epoch 12/30\n",
      "27/27 [==============================] - 1s 52ms/step - loss: -0.3038 - val_loss: -0.1336\n",
      "Epoch 13/30\n",
      "27/27 [==============================] - 2s 60ms/step - loss: -0.3327 - val_loss: -0.1396\n",
      "Epoch 14/30\n",
      "27/27 [==============================] - 1s 54ms/step - loss: -0.3608 - val_loss: -0.1453\n",
      "Epoch 15/30\n",
      "27/27 [==============================] - 1s 50ms/step - loss: -0.3895 - val_loss: -0.1502\n",
      "Epoch 16/30\n",
      "27/27 [==============================] - 1s 52ms/step - loss: -0.4219 - val_loss: -0.1560\n",
      "Epoch 17/30\n",
      "27/27 [==============================] - 1s 50ms/step - loss: -0.4489 - val_loss: -0.1552\n",
      "Epoch 18/30\n",
      "27/27 [==============================] - 2s 63ms/step - loss: -0.4818 - val_loss: -0.1631\n",
      "Epoch 19/30\n",
      "27/27 [==============================] - 2s 57ms/step - loss: -0.5115 - val_loss: -0.1664\n",
      "Epoch 20/30\n",
      "27/27 [==============================] - 1s 48ms/step - loss: -0.5385 - val_loss: -0.1680\n",
      "Epoch 21/30\n",
      "27/27 [==============================] - 1s 50ms/step - loss: -0.5665 - val_loss: -0.1723\n",
      "Epoch 22/30\n",
      "27/27 [==============================] - 1s 53ms/step - loss: -0.5879 - val_loss: -0.1748\n",
      "Epoch 23/30\n",
      "27/27 [==============================] - 2s 62ms/step - loss: -0.6139 - val_loss: -0.1770\n",
      "Epoch 24/30\n",
      "27/27 [==============================] - 2s 58ms/step - loss: -0.6374 - val_loss: -0.1778\n",
      "Epoch 25/30\n",
      "27/27 [==============================] - 2s 58ms/step - loss: -0.6541 - val_loss: -0.1820\n",
      "Epoch 26/30\n",
      "27/27 [==============================] - ETA: 0s - loss: -0.67 - 2s 63ms/step - loss: -0.6715 - val_loss: -0.1832\n",
      "Epoch 27/30\n",
      "27/27 [==============================] - 2s 57ms/step - loss: -0.6876 - val_loss: -0.1822\n",
      "Epoch 28/30\n",
      "27/27 [==============================] - 2s 67ms/step - loss: -0.7059 - val_loss: -0.1853\n",
      "Epoch 29/30\n",
      "27/27 [==============================] - 2s 60ms/step - loss: -0.7183 - val_loss: -0.1855\n",
      "Epoch 30/30\n",
      "27/27 [==============================] - 2s 65ms/step - loss: -0.7305 - val_loss: -0.1869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c417c39520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "from tensorflow.keras import backend as K\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "model.add_loss(K.mean(K.abs(mask))*1e-2)\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 168.02it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2182.21it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "error: 0.010041557252407074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1fb068ea280>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.32916539 0.38581702 0.54196449]\n",
      "Using control points [0.56020409 0.6503592  0.97683875]\n",
      "Using control points [4.31681776e-14 2.72470626e-13 1.12558118e-11]\n",
      "Using control points [0.07107591 0.0752193  0.1097129 ]\n",
      "Using control points [0.30229189 0.37536541 0.54561051]\n",
      "Using control points [0.64646505 0.78543492 1.11528787]\n",
      "Using control points [0.64646505 0.78543492 1.11528787]\n",
      "Using control points [0.68535402 0.84364348 1.24558203]\n",
      "Using control points [0.70009242 0.84312491 1.22084468]\n",
      "Using control points [0.79759048 0.95056859 1.38809427]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "# settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "settings_alt = dict(duration_of_trial=0.20, extents=(1, 40), number_of_sources=(1,15), target_snr=(1, 15))\n",
    "\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings_alt).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM DOUBLE-PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Contextualizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " FC0 (TimeDistributed)          (None, None, 61)     3782        ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 61)     0           ['FC0[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM0 (Bidirectional)          (None, None, 64)     18240       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, 125)    0           ['LSTM0[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, None, 61)     7686        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, None, 61)     0           ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, None, 64)     18240       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, None, 1284)   79608       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 1284)   83460       ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 1284)   0           ['FC2[0][0]',                    \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.abs_2 (TFOpLambda)     (None, None, 1284)   0           ['Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  ()                  0           ['tf.math.abs_2[0][0]']          \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  ()                  0           ['tf.math.reduce_mean_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_loss_2 (AddLoss)           ()                   0           ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 211,016\n",
      "Trainable params: 211,016\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 9s 29ms/step - loss: 0.0067 - val_loss: 0.0032\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 3s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 3s 20ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 2s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 3s 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa06b34190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 32\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "\n",
    "fc0 = TimeDistributed(Dense(n_channels, \n",
    "            activation=activation_function), \n",
    "            name='FC0')(inputs)\n",
    "fc0 = Dropout(dropout)(fc0)\n",
    "\n",
    "# Context\n",
    "lstm0 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM0')(fc0)\n",
    "cat = concatenate([lstm0, fc0])\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(cat)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Masking\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, \n",
    "            input_shape=(None, n_dense_units), dropout=dropout), \n",
    "            name='LSTM1')(fc1)\n",
    "mask = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"sigmoid\"), \n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "multi = multiply([direct_out, mask], name=\"multiply\")\n",
    "model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')\n",
    "model.add_loss(K.mean(K.abs(mask))*1e-2)\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 261.28it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 18233.33it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 71.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "error: 0.002569999312981963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f7e3b47190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [3.02116368e-09 2.67798996e-08 3.51250970e-08]\n",
      "Using control points [8.25629933e-05 1.09109362e-04 6.67372005e-04]\n",
      "Using control points [3.02116368e-09 2.67798996e-08 3.51250970e-08]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder-Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 61)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        97280       ['Input[0][0]']                  \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 128),  97280       ['Input[0][0]',                  \n",
      "                                 (None, 128),                     'lstm[0][1]',                   \n",
      "                                 (None, 128)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 1284)  165636      ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 360,196\n",
      "Trainable params: 360,196\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 6s 24ms/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.0024 - val_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa0f58f910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, add, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmse_loss, nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, _ = leadfield.shape\n",
    "n_dipoles = y.shape[-1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "n_dense_units = 61\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "input_dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "encoder_outputs, state_h, state_c = LSTM(n_lstm_units, return_state=True)(inputs)\n",
    "\n",
    "decoder_LSTM = LSTM(n_lstm_units, return_state=True, return_sequences=True)   \n",
    "decoder_outputs, _, _ = decoder_LSTM(inputs, initial_state=[state_h, state_c])\n",
    "outputs = TimeDistributed(Dense(n_dipoles, activation='linear'))(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Encoder-Decoder')\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 263.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 20053.57it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "error: 0.002369122812524438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f9abcb6790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.03084668 0.03540283 0.06492488]\n",
      "Using control points [7.83109839e-15 2.70473606e-12 1.32622130e-08]\n",
      "Using control points [7.83109839e-15 2.70473606e-12 1.32622130e-08]\n",
      "Using control points [3.53347129e-15 1.22040444e-12 9.49015868e-09]\n",
      "Using control points [4.87417981e-16 1.68346370e-13 3.16518037e-08]\n",
      "Using control points [0.04085439 0.0593598  0.11993631]\n",
      "Using control points [1.15491897e-15 3.98890530e-13 3.31839426e-08]\n",
      "Using control points [0.04770753 0.06768148 0.13079534]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11524/623973408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m direct_out = TimeDistributed(Dense(n_dipoles, \n\u001b[0m\u001b[0;32m     27\u001b[0m             activation=\"linear\"),\n\u001b[0;32m     28\u001b[0m             name='FC2')(lstm1)\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    970\u001b[0m                                                 input_list)\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1105\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1106\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1108\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    876\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2623\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2625\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2626\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2627\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    177\u001b[0m         nest.map_structure(lambda x: x.ndims, input_shape))\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    180\u001b[0m           \u001b[1;34m'`TimeDistributed` Layer should be passed an `input_shape ` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m           'with at least 3 dimensions, received: ' + str(input_shape))\n",
      "\u001b[1;31mValueError\u001b[0m: `TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: (None, 128)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 128\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "# fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, name='LSTM1'))(fc1)\n",
    "# lstm1 = Bidirectional(GRU(n_lstm_units, return_sequences=True, return_state=True, name='LSTM1'))(fc1)[2]\n",
    "\n",
    "\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC2')(lstm1)\n",
    "\n",
    "\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='LSTM_Old')\n",
    "\n",
    "\n",
    "model2.compile(loss=loss, optimizer=\"adam\")\n",
    "model2.summary()\n",
    "model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, None, 61)]        0         \n",
      "                                                                 \n",
      " FC1 (TimeDistributed)       (None, None, 300)         18600     \n",
      "                                                                 \n",
      " FC2 (TimeDistributed)       (None, None, 1284)        386484    \n",
      "                                                                 \n",
      " tf.math.abs_17 (TFOpLambda)  (None, None, 1284)       0         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_17 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.math.multiply_16 (TFOpLa  ()                       0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " add_loss_15 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 405,084\n",
      "Trainable params: 405,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 2s 9ms/step - loss: 0.0278 - val_loss: 0.0243\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0237\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0234 - val_loss: 0.0234\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0233\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0233\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f790ef6c40>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, GRU, multiply, Activation\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.1\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_channels), name='Input')\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "model3 = tf.keras.Model(inputs=inputs, outputs=direct_out, name='FC')\n",
    "model3.add_loss(K.mean(K.abs(direct_out))*1e-2)\n",
    "\n",
    "\n",
    "model3.compile(loss=loss, optimizer=\"adam\")\n",
    "\n",
    "model3.summary()\n",
    "model3.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 93.54it/s] \n",
      "100%|██████████| 200/200 [00:00<00:00, 2099.70it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "error: 0.010495411232113838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1fb056dac10>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings_alt = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=(1,15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings_alt).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model3.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 100\n",
    "n_lstm_units = 30\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_dipoles, 1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "# cnn = Conv2D(n_filters, kernel_size, dilation_rate=dilation_rate, padding=\"valid\")(inputs)\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(inputs)\n",
    "\n",
    "cnn = Reshape((n_time, n_dense_units, 1))(fc1)\n",
    "cnn = Conv2D(n_filters, (3, 100), dilation_rate=dilation_rate, padding=\"same\", activation=activation_function)(cnn)\n",
    "cnn = Reshape((n_time, n_filters*n_dense_units))(cnn)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(300, \n",
    "            activation=activation_function),\n",
    "            name='FC2')(cnn)\n",
    "\n",
    "fc3 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"linear\"),\n",
    "            name='FC3')(fc2)\n",
    "\n",
    "# cnn = Reshape(new_shape)(cnn)\n",
    "\n",
    "\n",
    "model4 = tf.keras.Model(inputs=inputs, outputs=fc3, name='FC-CNN')\n",
    "\n",
    "\n",
    "model4.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=\"adam\")\n",
    "\n",
    "model4.summary()\n",
    "model4.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, extents=(1,40), number_of_sources=3, target_snr=(2, 15))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "y_hat = model4.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC - context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FC-Context\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 20, 61)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1220)         0           ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " context (Dense)                (None, 200)          244200      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 20, 200)     0           ['context[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 20, 261)      0           ['Input[0][0]',                  \n",
      "                                                                  'repeat_vector_2[0][0]']        \n",
      "                                                                                                  \n",
      " FC1 (TimeDistributed)          (None, 20, 300)      78600       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " FC2 (TimeDistributed)          (None, 20, 1284)     386484      ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.abs_7 (TFOpLambda)     (None, 20, 1284)     0           ['FC2[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_7 (TFOpLam  ()                  0           ['tf.math.abs_7[0][0]']          \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  ()                  0           ['tf.math.reduce_mean_7[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_loss_5 (AddLoss)           ()                   0           ['tf.math.multiply_6[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 709,284\n",
      "Trainable params: 709,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.0146 - val_loss: 0.0047\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa0f553910>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, Conv2D, Flatten, Reshape, Dropout, concatenate, RepeatVector\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from esinet.losses import nmae_loss\n",
    "\n",
    "leadfield, pos = util.unpack_fwd(fwd)[1:3]\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X.shape[1]\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 30\n",
    "n_embed = 200\n",
    "activation_function = \"tanh\"\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "dropout = 0.1\n",
    "n_filters = int(n_channels / 2)\n",
    "kernel_size = (1, n_dipoles)\n",
    "dilation_rate = (2, 1)\n",
    "new_shape = (n_time, n_embed)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X.shape[1], n_channels), name='Input')\n",
    "\n",
    "context = Flatten()(inputs)\n",
    "context = Dense(n_embed, activation=activation_function, name=\"context\")(context)\n",
    "# context = Reshape(new_shape)(context)\n",
    "context = RepeatVector(n_time)(context)\n",
    "\n",
    "context = concatenate([inputs, context])\n",
    "# context = Dropout(0.2)(context)\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function), \n",
    "            name='FC1')(context)\n",
    "# cat = concatenate([fc1, context])\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"tanh\"),\n",
    "            name='FC2')(fc1)\n",
    "\n",
    "\n",
    "\n",
    "model5 = tf.keras.Model(inputs=inputs, outputs=fc2, name='FC-Context')\n",
    "model5.add_loss(K.mean(K.abs(fc2)) * 1e-2)\n",
    "\n",
    "model5.compile(loss=loss, optimizer=\"adam\")\n",
    "\n",
    "model5.summary()\n",
    "model5.fit(X[:, :, :, np.newaxis], y[:, :, :, np.newaxis], epochs=epochs, batch_size=batch_size, validation_split=0.15)\n",
    "# model5.fit(generate_batches(batch_size), steps_per_epoch=1000, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 271.17it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 20053.09it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 58.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "error: 0.002958708442747593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne.viz._brain._brain.Brain at 0x1f9f8e4a430>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.02553031 0.02805882 0.03966862]\n",
      "Using control points [0.00000000e+00 3.59008654e-09 1.90625137e-08]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "settings = dict(duration_of_trial=0.20, extents=(1,40), number_of_sources=(1,15), target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "print(sim_test.simulation_info.number_of_sources.values[0])\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "error = model5.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"error: {error}\")\n",
    "y_hat = model5.predict(X_test)[0]\n",
    "\n",
    "stc = sim_test.source_data[0]\n",
    "stc.plot(**plot_params)\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "# stc_hat.data = comp.decode(y_hat.T)\n",
    "stc_hat.data = y_hat.T\n",
    "\n",
    "stc_hat.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import sys; sys.path.insert(0, '../../invert/')\n",
    "from invert.solvers.empirical_bayes import SolverChampagne\n",
    "\n",
    "solver = SolverChampagne()\n",
    "solver.make_inverse_operator(fwd)\n",
    "n_samples = 2\n",
    "settings = dict(duration_of_trial=0.2, number_of_sources=(1,10), extents=(1,40))\n",
    "# settings = dict(duration_of_trial=0.25, number_of_sources=5, extents=(1,2))\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings=settings).simulate(n_samples=n_samples)\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "\n",
    "# models = [model, model2, model3]\n",
    "# aucs = []\n",
    "# mles = []\n",
    "# nmses = []\n",
    "# for net in models:\n",
    "#     y_hat = net.predict(X_test)\n",
    "#     auc, nmse, mle = eval(y_test, y_hat)\n",
    "#     aucs.append( auc )\n",
    "#     nmses.append( nmse )\n",
    "#     mles.append( mle )\n",
    "#     print(f\"{net.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "# y_hat = np.stack([solver.apply_inverse_operator(epochs.average()).data for epochs in tqdm(sim_test.eeg_data)], axis=0)\n",
    "# y_hat = np.swapaxes(y_hat, 1, 2)\n",
    "# auc, nmse, mle = eval(y_test, y_hat)\n",
    "# aucs.append( auc )\n",
    "# nmses.append( nmse )\n",
    "# mles.append( mle )\n",
    "\n",
    "# models.append(solver)\n",
    "# print(f\"{solver.name}: \\n\\t{np.nanmedian(aucs[-1])} AUC \\n\\t{np.nanmedian(mles[-1])} mm \\n\\t{np.nanmedian(nmses[-1])} nMSE\")\n",
    "\n",
    "idx = 0\n",
    "n = sim_test.simulation_info[\"number_of_sources\"].values[idx]\n",
    "print(f\"{n} sources\")\n",
    "\n",
    "# PLOTTING BRAINS\n",
    "stc = sim_test.source_data[idx]\n",
    "stc.plot(**plot_params, brain_kwargs=dict(title=\"Ground Truth\"))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "y_hat = model5.predict(X_test)[idx]\n",
    "stc_hat.data = y_hat.T\n",
    "stc_hat.plot(**plot_params, brain_kwargs=dict(title=model5.name))\n",
    "r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "print(f\"{model5.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# y_hat = model2.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model2.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model2.name}: r={r:.2f}\")\n",
    "\n",
    "# y_hat = model3.predict(X_test)[idx]\n",
    "# stc_hat = stc.copy()\n",
    "# stc_hat.data = y_hat.T\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=model3.name))\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{model3.name}: r={r:.2f}\")\n",
    "\n",
    "\n",
    "# evoked = sim_test.eeg_data[idx].average()\n",
    "# stc_hat = solver.apply_inverse_operator(evoked)\n",
    "# stc_hat.plot(**plot_params, brain_kwargs=dict(title=solver.name))\n",
    "# y_hat = stc_hat.data\n",
    "# r = pearsonr(y_hat.flatten(), y_test[idx].flatten())[0]\n",
    "# print(f\"{solver.name}: r={r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "names = [m.name for m in models]\n",
    "xticks = (np.arange(len(models)), names)\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(aucs,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.title(\"AUC\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(mles,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.title(\"MLE\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=np.nanmean(nmses,axis=-1).T)\n",
    "plt.xticks(*xticks)\n",
    "plt.xlabel(\"NMSE\")\n",
    "plt.title(\"NMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('esienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a54b85cbc80ea8362b8e45e33618627fd9167210ff2c52e6dbeaf85afe35b874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
